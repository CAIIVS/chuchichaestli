{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>chuchichaestli</code>","text":"<p>Where you find all the state-of-the-art cooking utensils (salt, pepper, gradient descent... the usual).</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ul> <li>Install</li> <li>Usage</li> <li>API Reference</li> </ul> <p><code>chuchichaestli</code> is a collection of model architectures and other useful bits of code in use at the Centre for Artificial Intelligence's Intelligent Vision Systems group (CAIIVS) at the University of Applied Sciences Zurich (ZHAW). We use our models in research of various topics from medical image artifact removal to astrophysics emulation of galaxies.</p> <p>Why should I use <code>chuchichaestli</code>?</p> <p>Many machine learning projects introduce new code. While the code may function as intended, it is sometimes developed with a primary focus on rapid research publication, which can result in limited documentation and reduced ease of reuse. This project aims to address these challenges by providing well-structured implementations of state-of-the-art architectures and utility tools designed for reuse. The codebase of <code>chuchichaestli</code> is thoroughly \"battle-tested\" and documented to the best of our ability.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Modern Python Toolkit \ud83d\udc0d: Dive into a codebase purely in Python,   built with clarity and efficiency in mind.</li> <li>Easy Integration \ud83d\ude80: Designed for plug-and-play usability, so you   can quickly add chuchichaestli\u2019s utilities to your own projects and   experiments.</li> <li>State-of-the-Art Convenience \ud83c\udf20: Stop searching for scattered   snippets - get everything you need in one well-organized repository.</li> <li>Open Source Collaboration \ud83e\uddea: Join the community of scientists   and researchers. Share your own code and improvements - all   contributions welcome!</li> </ul>"},{"location":"install/","title":"Install","text":"<p>Tagged releases are available as PyPI packages. To install the latest package, run:</p> <pre><code>pip install chuchichaestli\n</code></pre> <p>For the bleeding-edge package directly from a git branch, use <pre><code>pip install git+https://github.com/CAIIVS/chuchichaestli.git@&lt;latest-branch&gt;\n</code></pre></p> <p>(and replace <code>&lt;latest-branch&gt;</code> with the actual branch name) or clone the repository and run the following command in the root directory of the repository:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"install/#uv","title":"uv","text":"<p><code>chuchichaestli</code> is developed using <code>uv</code> and thus provides a <code>uv.lock</code> file which should make installing the package easier, faster, and universal. In the project, run</p> <pre><code>uv sync [--all-groups]\n</code></pre> <p>To add <code>chuchichaestli</code> to your own project simply useful <pre><code>uv add chuchichaestli\n</code></pre> and it will appear as a dependency in your <code>pyproject.toml</code>.</p>"},{"location":"install/#nix","title":"nix","text":"<p>If reproducibility is of utmost importance, you might want to look into <code>nix</code>. <code>chuchichaestli</code> is packaged as nix package (see <code>default.nix</code>). To install it, you can add the following to your  nix-config and rebuild it</p> <pre><code>{pkgs, ...}: let\n    remote = builtins.fetchurl {\n        url = \"https://raw.githubusercontent.com/CAIIVS/chuchichaestli/refs/heads/main/default.nix\";\n        sha256 = \"0a838l8h2qv4c95zi68r1nr8ndmn8929f53js04g3h15ii3zbskb\";\n    };\n    chuchichaestli = pkgs.callPackage remote {\n        src = pkgs.fetchFromGitHub {\n        owner = \"CAIIVS\";\n        repo = \"chuchichaestli\";\n        rev = \"main\";\n        sha256 = \"10vyprf5736plprmww0xr20i1m83x2d43cnk7k1p0f4fnv6iraf2\";\n    };\nin {\n    environment.systemPackages = with pkgs; [\n        chuchichaestli\n    ];\n}\n</code></pre>"},{"location":"reference/","title":"API Reference","text":"<p>Here, the entire <code>chuchichaestli</code> API is referenced. Use the search functionality to look for specific references.</p>"},{"location":"reference/#chuchichaestli","title":"chuchichaestli","text":"<p>The chuchichaestli framework.</p> <p>Modules:</p> Name Description <code>data</code> <p>Data module of chuchichaestli.</p> <code>debug</code> <p>Debugging utilities module of chuchichaestli.</p> <code>diffusion</code> <p>Diffusion schedulers module of chuchichaestli.</p> <code>metrics</code> <p>Metrics module of chuchichaestli: various loss &amp; evaluation metric implementations.</p> <code>models</code> <p>Models module of chuchichaestli.</p> <code>utils</code> <p>Various utility functions for chuchichaestli.</p>"},{"location":"usage/","title":"Usage","text":"<p><code>chuchichaestli</code> provides various modules that are essential throughout the creation of neural network models, from training to evaluation. It is meant as a repository of building blocks with which you can build your own neural network models.</p> <p>Note</p> <p>The framework integrates into the PyTorch ecosystem and as any artificial  intelligence application is most efficiently used on GPU-based hardware.</p> <p>We recommend combining the package with a configuration framework such as hydra. It can easily configure and instantiate <code>chuchichaestli</code> modules such as data loaders, neural network models, loss functions, and evaluation metrics.</p>"},{"location":"usage/#datasets","title":"Datasets","text":"<p>The data module provides a HDF5Dataset which efficiently caches PyTorch tensors in shared memory. With sufficient RAM, subsequent training epochs can be considerably accelerated.</p>"},{"location":"usage/#example","title":"Example","text":"<p>Say, you have several HDF5 files with image datasets stored as <pre><code>data\n\u2514\u2500\u2500 images\n    \u251c\u2500\u2500 dodos\n    \u2502   \u2514\u2500\u2500 images.h5\n    \u251c\u2500\u2500 dragons\n    \u2502   \u2514\u2500\u2500 images.h5\n    \u2514\u2500\u2500 wolpertinger\n        \u2514\u2500\u2500 images.h5\n</code></pre></p> <p>then the following creates a dataset with 8 GB of memory allocation to cache image tensors read from the dataset</p> <pre><code>from chuchichaestli.data import HDF5Dataset\n\ndataset = HDF5Dataset(\"data/images/**/*.h5\", cache=\"8G\")\ndataset.info()\nsample_image = dataset[0]\n</code></pre>"},{"location":"usage/#models","title":"Models","text":"<p>The models module provides various neural network models ready to be instantiated such as UNet or built with the components implemented in diffusion, models.attention, models.adversarial, and more.</p> <p>These models are not pre-trained, meaning for proper functioning they have to be trained using appropriate data and objectives (loss functions).</p>"},{"location":"usage/#example_1","title":"Example","text":"<p>The U-Net architecture consists of an encoder-decoder structure with skip connections which ensure spatial information is passed through the network (even for higher compression levels). The building blocks of the U-Net can have various forms, but generally consist of convolutional layers. In this example, the encoder is purely convolutional, whereas the decoder includes a mixture of attention and (transposed) convolutional layers.</p> <pre><code>from chuchichaestli.models.unet import UNet\n\nmodel = UNet(\n    dimensions=2,     # spatial dimensions\n    in_channels=3,    # input image channels such as RGB\n    n_channels=64,    # channels of first hidden layer\n    out_channels=3,   # output image channels such as RGB\n    down_block_types=(\"DownBlock\")*4,      # simple residual blocks\n    up_block_types=(\"AttnUpBlock\")*4,      # residual blocks with attention heads in front\n    block_out_channel_mults=(1, 2, 2, 4),  # channel multipliers with each level\n    res_act=\"prelu\",  # parametric ReLU\n    res_dropout=0.4   # dropout for residual blocks\n    attn_n_heads=2    # number of attention heads per block,\n    skip_connection_action=\"concat\"        # skip connections are concatenated in decoder\n)\nprint(model)\n</code></pre>"},{"location":"usage/#metrics","title":"Metrics","text":"<p>The metrics module provides various metrics and losses to measure and compare image quality of fake and real samples. In contrast to many other image quality metric libraries, <code>chuchichaestli</code>'s only dependency for this module (besides <code>torch</code> itself) is <code>torchvision</code>. This makes <code>chuchichaestli</code> still very lightweight and avoids package conflicts during installs.</p>"},{"location":"usage/#example_2","title":"Example","text":"<p>This example demonstrates how to use a whole battery of metrics.  Each metric has a <code>.update</code> method which registers samples and adds them to the aggregate state. Typically, this method is used while iterating through the evaluation set to build aggregate statistics for the entire evaluation set. The <code>.compute</code> method computes the metric value for the current aggregate state. This method is typically used after iterating through an evaluation set to trigger the actual computation (reduction).</p> <pre><code>from chuchichaestli.metrics import MSE, PSNR, SSIM, FID\n\nbatch_size, num_channels, width, height = 4, 3, 512, 512\nsample_images = torch.rand(batch_size, num_channels, width, height)\n\nmetrics = [\n    MSE(),\n    PSNR(min_value=0, max_value=1), \n    SSIM(min_value=0, max_value=1, kernel_size=7, kernel_type=\"gaussian\"),\n    FID()\n]\n\nmodel.eval()\nwith torch.no_grad():\n    fake_images = model(sample_images)\n    evaluations = []\n    for metric in metrics:\n        metric.update(fake_images, sample_images)\n        val = metric.compute()\n        evaluations.append(val)\n        metric.reset()\nprint(evaluations)\n</code></pre>"},{"location":"reference/chuchichaestli/","title":"chuchichaestli","text":""},{"location":"reference/chuchichaestli/#chuchichaestli","title":"chuchichaestli","text":"<p>The chuchichaestli framework.</p> <p>Modules:</p> Name Description <code>data</code> <p>Data module of chuchichaestli.</p> <code>debug</code> <p>Debugging utilities module of chuchichaestli.</p> <code>diffusion</code> <p>Diffusion schedulers module of chuchichaestli.</p> <code>metrics</code> <p>Metrics module of chuchichaestli: various loss &amp; evaluation metric implementations.</p> <code>models</code> <p>Models module of chuchichaestli.</p> <code>utils</code> <p>Various utility functions for chuchichaestli.</p>"},{"location":"reference/chuchichaestli/data/","title":"chuchichaestli.data","text":""},{"location":"reference/chuchichaestli/data/#chuchichaestli.data","title":"chuchichaestli.data","text":"<p>Data module of chuchichaestli.</p> <p>Modules:</p> Name Description <code>cache</code> <p>Caching for tensors from PyTorch datasets.</p> <code>dataset</code> <p>PyTorch dataset classes.</p> <p>Classes:</p> Name Description <code>HDF5Dataset</code> <p>Dataset for loading HDF5 frames with optional shared memory (sto)caching.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset","title":"HDF5Dataset","text":"<pre><code>HDF5Dataset(\n    path: str | Path | list[str] | list[Path],\n    groups: str | tuple[str, ...] = \"*\",\n    sort_key: Callable | None = None,\n    collate: bool | Callable = False,\n    parallel: bool = False,\n    pair: bool = False,\n    squash: bool = True,\n    dtype: torch.dtype = torch.float32,\n    attr_groups: str | tuple[str, ...] | None = None,\n    attrs_retrieval: str | Callable = \"auto\",\n    return_as: str | dict | None = \"tuple\",\n    preload: bool = False,\n    cache: int | float | str | bool | None = \"4G\",\n    attrs_cache: int | float | str | bool | None = \"64M\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for loading HDF5 frames with optional shared memory (sto)caching.</p> <p>Constructs a <code>frame</code> of single or multiple HDF5 file objects. Can be selectively pointed to HDF5 Groups containing HDF5 Datasets via the <code>groups</code> keyword. If a frame has multiple datasets, the <code>collate</code> argument determines whether they are read in major-column or major-row ordering across files in a frame. A frame can be read in three different modes (always take effect after collate): - contiguous (parallel=False; sequentializes the datasets and returns single samples) - parallel (parallel=True; multiple samples, one for each dataset) - pair (pair=True; read samples pair-wise parallel) - custom (collate is callable; use your own ordering by passing a function to collate and/or attrs_retrieval) Note that caching only works for datasets with compatible shapes (i.e. image tensors need same dimensions).</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | list[str] | list[Path]</code> <p>Path to a file, files, or data directory which contains the HDF5 files. May contain wildcards <code>*</code> and <code>**</code>.</p> required <code>groups</code> <code>str | tuple[str, ...]</code> <p>Location(s) or filter pattern(s) for HDF5 groups containing HDF5 datasets (allows wildcards <code>*</code> and <code>**</code>). Note, if wildcard expressions have too many matches, the file tree search could take a long time, especially when there are many HDF5Groups in the file. Thus, it is recommended to not use too generic wildcard expressions. Alternatively, use <code>data_ignores</code> to tune the file tree search.</p> <code>'*'</code> <code>sort_key</code> <code>Callable | None</code> <p>Sorting key function for the frame tree of HDF5 groups.</p> <code>None</code> <code>collate</code> <code>bool | Callable</code> <p>If True, multiple HDF5 files are collated into a single dataset. This is equivalent to reading files in Fortran-like row-major ordering. If False, getitem tries to pull samples from each file. Can also be a function that creates a custom ordering with signature <code>func(hdf5_datasets: list[list]) -&gt; list[list]</code>.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>If True, HDF5 datasets are read in parallel, resulting in multiple samples being pulled, otherwise datasets are concatenated and sequentialized (note that contiguous reading (with squash=True) only works, if dataset dimensions are compatible).</p> <code>False</code> <code>pair</code> <code>bool</code> <p>If True, datasets are paired and read pairwise-parallel (automatically sets parallel=True).</p> <code>False</code> <code>squash</code> <code>bool</code> <p>Squash dimension along first axis via summation (default behaviour for sequential, i.e. parallel=False reads).</p> <code>True</code> <code>dtype</code> <code>torch.dtype</code> <p>Data tensor type; default: torch.float32.</p> <code>torch.float32</code> <code>attr_groups</code> <code>str | tuple[str, ...] | None</code> <p>Locations or filter pattern(s) for HDF5 groups containing HDF5 attributes (allows wildcards '' and '*'). If empty or None, no attributes will be filtered. Note: If wildcard expressions have too many matches, the file tree search could take a long time, especially when there are many HDF5Groups in the file. Thus, it is recommended to not use too generic wildcard expressions. Alternatively, use <code>attrs_ignores</code> to tune the file tree search.</p> <code>None</code> <code>attrs_retrieval</code> <code>str | Callable</code> <p>A retrieval strategy for attributes; can be a function one of ['auto' | None | Callable].</p> <code>'auto'</code> <code>return_as</code> <code>str | dict | None</code> <p>Return type of the dataset; one of ['tuple', 'dict', dict, None]. If a dictionary object is give, then the keys in the dictionary are used to map items onto it.</p> <code>'tuple'</code> <code>preload</code> <code>bool</code> <p>Preload and cache the dataset.</p> <code>False</code> <code>cache</code> <code>int | float | str | bool | None</code> <p>If not None or False, memory is allocated for stochastic caching of the tensors.</p> <code>'4G'</code> <code>attrs_cache</code> <code>int | float | str | bool | None</code> <p>If not None or False, memory is allocated for stochastic caching of ancillary data.</p> <code>'64M'</code> <code>kwargs</code> <p>Keyword arguments for <code>h5py.File</code>. See <code>https://docs.h5py.org/en/stable/high/file.html#h5py.File</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>attrs_getter</code> <p>Retrieve ancillary data item at <code>index</code>.</p> <code>cache_attrs</code> <p>Cache attrs (overwrite by default off).</p> <code>cache_item</code> <p>Cache item (overwrite by default off).</p> <code>close</code> <p>Exit the frame, i.e. close all open files and purge the cache.</p> <code>default_sort_key</code> <p>Default sort key for HDF5 groups in the frame tree.</p> <code>filter_tree</code> <p>Filter frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p> <code>get_cached_attrs</code> <p>Fetch attrs if in cache.</p> <code>get_cached_item</code> <p>Fetch item(s) if in cache.</p> <code>glob_path</code> <p>Glob path recursively for HDF5 files.</p> <code>info</code> <p>Print summary information about the frame.</p> <code>init_cache</code> <p>Initialize the cache.</p> <code>is_aligned</code> <p>Test if the model is aligned, i.e. same sample size for parallel reads.</p> <code>load_frame</code> <p>Load HDF5 file instances.</p> <code>load_frame_tree</code> <p>Load tree of HDF5 group for each frame (HDF5 file instance).</p> <code>make_index</code> <p>Index frame datasets, apply collation and parallelization settings.</p> <code>pin_attrs</code> <p>Find and pin attribute sets (metadata) in the frame.</p> <code>pin_data</code> <p>Find and pin datasets in the frame.</p> <code>purge_cache</code> <p>Purge the cache.</p> <p>Attributes:</p> Name Type Description <code>attr_groups</code> <p>HDF5 groups of the selected attribute sets in the frame.</p> <code>attrs_dims</code> <code>list[tuple] | None</code> <p>Output format of attributes.</p> <code>attrs_dtype</code> <code>list[torch.dtype]</code> <p>Data type of attributes if represented as HDF5 Dataset.</p> <code>attrs_is_data</code> <code>bool</code> <p>If True, attribute sets are represented by h5py.Dataset objects.</p> <code>attrs_is_dict</code> <code>bool</code> <p>If True, attribute sets are represented by dict or h5py.AttributeManager objects.</p> <code>cache_size</code> <code>nbytes</code> <p>Size of reserved cache.</p> <code>cached_attrs</code> <p>Number of cached samples from the frame.</p> <code>cached_bytes</code> <code>nbytes</code> <p>Size of used cache.</p> <code>cached_items</code> <p>Number of cached samples from the frame.</p> <code>data_groups</code> <p>HDF5 groups of the selected datasets in the frame.</p> <code>dims</code> <code>tuple | list[list[tuple]]</code> <p>Dataset dimensions (respecting collation and parallelization settings).</p> <code>frame</code> <code>list[H5PyFile]</code> <p>Lazy-loading list of HDF5 file instances.</p> <code>frame_tree</code> <code>list[list[str]]</code> <p>Fetch unfiltered frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p> <code>indexed_dims</code> <code>list[tuple]</code> <p>Dataset dimension of the indexed datasets.</p> <code>n_attrs</code> <p>Number of attribute sets in the frame.</p> <code>n_attrsets</code> <p>Number of attribute sets in the frame.</p> <code>n_datasets</code> <p>Number of datasets in the frame.</p> <code>n_files</code> <p>Number of files in the frame.</p> <code>n_samples</code> <p>Number of samples in the frame.</p> <code>raw_dims</code> <code>list[list[tuple]]</code> <p>Raw dataset dimensions (ignores collation and parallelization settings).</p> <code>sample_serial_size</code> <code>tuple[nbytes, nbytes]</code> <p>Size of a sample (tensor and metadata) in bytes (when serialized).</p> <code>serial_size</code> <code>tuple[nbytes, nbytes]</code> <p>Size of the entire dataset in bytes (when serialized).</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path | list[str] | list[Path],\n    groups: str | tuple[str, ...] = \"*\",\n    sort_key: Callable | None = None,\n    collate: bool | Callable = False,\n    parallel: bool = False,\n    pair: bool = False,\n    squash: bool = True,\n    dtype: torch.dtype = torch.float32,\n    attr_groups: str | tuple[str, ...] | None = None,\n    attrs_retrieval: str | Callable = \"auto\",\n    return_as: str | dict | None = \"tuple\",\n    preload: bool = False,\n    cache: int | float | str | bool | None = \"4G\",\n    attrs_cache: int | float | str | bool | None = \"64M\",\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        path: Path to a file, files, or data directory which contains the HDF5 files. May\n          contain wildcards `*` and `**`.\n        groups: Location(s) or filter pattern(s) for HDF5 groups containing HDF5 datasets\n          (allows wildcards `*` and `**`). Note, if wildcard expressions have too many\n          matches, the file tree search could take a long time, especially when there are\n          many HDF5Groups in the file. Thus, it is recommended to not use too generic\n          wildcard expressions. Alternatively, use `data_ignores` to tune the file tree search.\n        sort_key: Sorting key function for the frame tree of HDF5 groups.\n        collate: If True, multiple HDF5 files are collated into a single dataset. This is\n          equivalent to reading files in Fortran-like row-major ordering. If False,\n          __getitem__ tries to pull samples from each file. Can also be a function that\n          creates a custom ordering with signature\n          `func(hdf5_datasets: list[list]) -&gt; list[list]`.\n        parallel: If True, HDF5 datasets are read in parallel, resulting in multiple samples\n          being pulled, otherwise datasets are concatenated and sequentialized (note that\n          contiguous reading (with squash=True) only works, if dataset dimensions are\n          compatible).\n        pair: If True, datasets are paired and read pairwise-parallel\n          (automatically sets parallel=True).\n        squash: Squash dimension along first axis via summation (default behaviour\n          for sequential, i.e. parallel=False reads).\n        dtype: Data tensor type; default: torch.float32.\n        attr_groups: Locations or filter pattern(s) for HDF5 groups containing HDF5 attributes\n          (allows wildcards '*' and '**'). If empty or None, no attributes will be filtered.\n          Note: If wildcard expressions have too many matches, the file tree search could take a\n          long time, especially when there are many HDF5Groups in the file. Thus, it is\n          recommended to not use too generic wildcard expressions. Alternatively,\n          use `attrs_ignores` to tune the file tree search.\n        attrs_retrieval: A retrieval strategy for attributes; can be a function\n          one of ['auto' | None | Callable].\n        return_as: Return type of the dataset; one of ['tuple', 'dict', dict, None].\n          If a dictionary object is give, then the keys in the dictionary are used to\n          map items onto it.\n        preload: Preload and cache the dataset.\n        cache: If not None or False, memory is allocated for stochastic caching of the tensors.\n        attrs_cache: If not None or False, memory is allocated for stochastic caching\n          of ancillary data.\n        kwargs: Keyword arguments for `h5py.File`. See\n          `https://docs.h5py.org/en/stable/high/file.html#h5py.File`.\n    \"\"\"\n    self.frame_args = kwargs\n    self.frame_args.setdefault(\"libver\", \"latest\")\n    self.collate = collate\n    if callable(collate):\n        self.collate = True\n        self._collate_fn = staticmethod(collate)\n    else:\n        self._collate_fn = self._collate\n    self.pair = pair\n    self.parallel = parallel or self.pair\n    self.squash = squash\n    self.dtype = dtype\n    self.return_as = return_as\n    self.preload = preload\n\n    # build frame\n    self._frame: list[H5PyFile]\n    self._virt_frame: list[H5PyFile]\n    self._frame_tree: list[list[str]]\n    self.files = self.glob_path(path)\n    self.load_frame(**self.frame_args)\n\n    # locate HDF5 datasets in frame\n    self.frame_datasets: list[list[H5PyDataset]] = []\n    self.groups = (groups,) if not isinstance(groups, Iterable) else groups\n    self.sort_key = sort_key if sort_key is not None else self.default_sort_key\n    self.pin_data(self.groups)\n    self.make_index()\n\n    # ancillary data from HDF5 group attributes (or datasets) in frame\n    self.frame_attrs: list[list[H5PyAttrs | H5PyDataset]] = []\n    self.attr_groups_selected = attr_groups\n    self.attrs_retrieval = attrs_retrieval\n    if not isinstance(attr_groups, Iterable) and attr_groups is not None:\n        self.attr_groups_selected = (attr_groups,)\n    self.pin_attrs(self.attr_groups_selected)\n\n    # allocate memory for cache\n    self.cache: tuple[list[SharedArray] | None, list[SharedDictList] | None]\n    if isinstance(cache, bool) and cache:\n        cache = (\n            self.serial_size[0]\n            if self.serial_size[0] &lt; nbytes(\"4G\")\n            else nbytes(\"4G\")\n        )\n    if isinstance(attrs_cache, bool):\n        attrs_cache = self.serial_size[1]\n    self.init_cache(cache, attrs_cache)\n    if self.preload and self.cache[0] is not None:\n        for i in range(len(self)):\n            self[i]\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.attr_groups","title":"attr_groups  <code>property</code>","text":"<pre><code>attr_groups\n</code></pre> <p>HDF5 groups of the selected attribute sets in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.attrs_dims","title":"attrs_dims  <code>property</code>","text":"<pre><code>attrs_dims: list[tuple] | None\n</code></pre> <p>Output format of attributes.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.attrs_dtype","title":"attrs_dtype  <code>property</code>","text":"<pre><code>attrs_dtype: list[torch.dtype]\n</code></pre> <p>Data type of attributes if represented as HDF5 Dataset.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.attrs_is_data","title":"attrs_is_data  <code>property</code>","text":"<pre><code>attrs_is_data: bool\n</code></pre> <p>If True, attribute sets are represented by h5py.Dataset objects.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.attrs_is_dict","title":"attrs_is_dict  <code>property</code>","text":"<pre><code>attrs_is_dict: bool\n</code></pre> <p>If True, attribute sets are represented by dict or h5py.AttributeManager objects.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.cache_size","title":"cache_size  <code>property</code>","text":"<pre><code>cache_size: nbytes\n</code></pre> <p>Size of reserved cache.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.cached_attrs","title":"cached_attrs  <code>property</code>","text":"<pre><code>cached_attrs\n</code></pre> <p>Number of cached samples from the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.cached_bytes","title":"cached_bytes  <code>property</code>","text":"<pre><code>cached_bytes: nbytes\n</code></pre> <p>Size of used cache.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.cached_items","title":"cached_items  <code>property</code>","text":"<pre><code>cached_items\n</code></pre> <p>Number of cached samples from the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.data_groups","title":"data_groups  <code>property</code>","text":"<pre><code>data_groups\n</code></pre> <p>HDF5 groups of the selected datasets in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.dims","title":"dims  <code>property</code>","text":"<pre><code>dims: tuple | list[list[tuple]]\n</code></pre> <p>Dataset dimensions (respecting collation and parallelization settings).</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.frame","title":"frame  <code>property</code>","text":"<pre><code>frame: list[H5PyFile]\n</code></pre> <p>Lazy-loading list of HDF5 file instances.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.frame_tree","title":"frame_tree  <code>property</code>","text":"<pre><code>frame_tree: list[list[str]]\n</code></pre> <p>Fetch unfiltered frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.indexed_dims","title":"indexed_dims  <code>property</code>","text":"<pre><code>indexed_dims: list[tuple]\n</code></pre> <p>Dataset dimension of the indexed datasets.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.n_attrs","title":"n_attrs  <code>property</code>","text":"<pre><code>n_attrs\n</code></pre> <p>Number of attribute sets in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.n_attrsets","title":"n_attrsets  <code>property</code>","text":"<pre><code>n_attrsets\n</code></pre> <p>Number of attribute sets in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.n_datasets","title":"n_datasets  <code>property</code>","text":"<pre><code>n_datasets\n</code></pre> <p>Number of datasets in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.n_files","title":"n_files  <code>property</code>","text":"<pre><code>n_files\n</code></pre> <p>Number of files in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.n_samples","title":"n_samples  <code>property</code>","text":"<pre><code>n_samples\n</code></pre> <p>Number of samples in the frame.</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.raw_dims","title":"raw_dims  <code>property</code>","text":"<pre><code>raw_dims: list[list[tuple]]\n</code></pre> <p>Raw dataset dimensions (ignores collation and parallelization settings).</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.sample_serial_size","title":"sample_serial_size  <code>property</code>","text":"<pre><code>sample_serial_size: tuple[nbytes, nbytes]\n</code></pre> <p>Size of a sample (tensor and metadata) in bytes (when serialized).</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.serial_size","title":"serial_size  <code>property</code>","text":"<pre><code>serial_size: tuple[nbytes, nbytes]\n</code></pre> <p>Size of the entire dataset in bytes (when serialized).</p>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.attrs_getter","title":"attrs_getter","text":"<pre><code>attrs_getter(index: int) -&gt; list[dict | torch.Tensor]\n</code></pre> <p>Retrieve ancillary data item at <code>index</code>.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def attrs_getter(self, index: int) -&gt; list[dict | torch.Tensor]:\n    \"\"\"Retrieve ancillary data item at `index`.\"\"\"\n    if self.n_attrs &lt; 1:\n        return []\n    if self.attrs_retrieval == \"auto\":\n        if self.n_datasets == self.n_attrsets:\n            # data &lt;-&gt; attribute set mapping: 1-to-1\n            indexed_attrs = self._make_attrs_index(self.frame_attrs)\n        elif (\n            self.n_datasets &lt; self.n_attrsets and self.n_attrsets == self.n_samples\n        ):\n            # data &lt;-&gt; attribute set mapping: 1-to-all\n            indexed_attrs = self._make_attrs_index(self.frame_attrs)\n        elif self.n_datasets &lt; self.n_attrsets:\n            # data &lt;-&gt; attributes mapping: 1-to-many\n            indexed_attrs = self._make_many_attrs_index(self.frame_attrs)\n        elif self.n_datasets &gt; self.n_attrsets and self.n_attrsets == 1:\n            # data &lt;-&gt; attribute set mapping: all-to-1\n            if not hasattr(self, \"indexed_attrs\"):\n                self.indexed_attrs = [self.frame_attrs[0] * self.n_samples]\n            indexed_attrs = self.indexed_attrs\n        else:\n            # data &lt;-&gt; attribute set mapping: many-to-1\n            warnings.warn(\n                \"Automatic attribute retrieval not possible!\\n\"\n                f\"Found {self.n_datasets} datasets with {self.n_samples} samples in total,\\n\"\n                \"but only {self.n_attrsets} attribute sets.\\n\"\n                \"Try passing a attribute retrieval function\\n\"\n                \"   fn(index, frame_attrs) -&gt; list[H5PyAttrs | H5PyDataset]\\n\"\n                \"that selects the appropriate attributes for a given index.\"\n            )\n            indexed_attrs = []\n        if self.attrs_is_data:\n            return [\n                torch.from_numpy(a[index]).squeeze() for a in indexed_attrs if a\n            ]\n        else:\n            return [dict(a[index]) for a in indexed_attrs if a]\n    elif callable(self.attrs_retrieval):\n        return self.attrs_retrieval(index, self.frame_attrs)\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.cache_attrs","title":"cache_attrs","text":"<pre><code>cache_attrs(\n    index: int,\n    attrs: torch.Tensor | dict | list[torch.Tensor | dict],\n    output_index: int | None = None,\n    overwrite: bool = False,\n)\n</code></pre> <p>Cache attrs (overwrite by default off).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index where to cache the attribute.</p> required <code>attrs</code> <code>torch.Tensor | dict | list[torch.Tensor | dict]</code> <p>Attribute(s) to be cached.</p> required <code>output_index</code> <code>int | None</code> <p>Index of the list of caches, if output contains multiple attributes.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True and there already is a cached element at the given index, that attribute is overwritten.</p> <code>False</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def cache_attrs(\n    self,\n    index: int,\n    attrs: torch.Tensor | dict | list[torch.Tensor | dict],\n    output_index: int | None = None,\n    overwrite: bool = False,\n):\n    \"\"\"Cache attrs (overwrite by default off).\n\n    Args:\n        index: Index where to cache the attribute.\n        attrs: Attribute(s) to be cached.\n        output_index: Index of the list of caches, if output contains multiple attributes.\n        overwrite: If True and there already is a cached element at the given index,\n          that attribute is overwritten.\n    \"\"\"\n    if not hasattr(self, \"cache\") or self.cache[1] is None:\n        return\n    if output_index is None:\n        output_index = 0\n    if isinstance(attrs, torch.Tensor | dict):\n        previous = self.cache[1][output_index][index]\n        if previous is None or overwrite:\n            self.cache[1][output_index][index] = attrs\n    else:\n        for i, t in enumerate(attrs):\n            previous = self.cache[1][i][index]\n            if previous is None or overwrite:\n                self.cache[1][i][index] = t\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.cache_item","title":"cache_item","text":"<pre><code>cache_item(\n    index: int,\n    items: torch.Tensor | list[torch.Tensor],\n    output_index: int | None = None,\n    overwrite: bool = False,\n)\n</code></pre> <p>Cache item (overwrite by default off).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index where to cache the attribute.</p> required <code>items</code> <code>torch.Tensor | list[torch.Tensor]</code> <p>Item(s) to be cached.</p> required <code>output_index</code> <code>int | None</code> <p>Index of the list of caches, if output contains multiple items.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True and there already is a cached element at the given index, that item is overwritten.</p> <code>False</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def cache_item(\n    self,\n    index: int,\n    items: torch.Tensor | list[torch.Tensor],\n    output_index: int | None = None,\n    overwrite: bool = False,\n):\n    \"\"\"Cache item (overwrite by default off).\n\n    Args:\n        index: Index where to cache the attribute.\n        items: Item(s) to be cached.\n        output_index: Index of the list of caches, if output contains multiple items.\n        overwrite: If True and there already is a cached element at the given index,\n          that item is overwritten.\n    \"\"\"\n    if not hasattr(self, \"cache\") or self.cache[0] is None:\n        return\n    if output_index is None:\n        output_index = 0\n    if isinstance(items, torch.Tensor):\n        previous = self.cache[0][output_index][index]\n        if previous is None or overwrite:\n            self.cache[0][output_index][index] = items\n    else:\n        for i, t in enumerate(items):\n            previous = self.cache[0][i][index]\n            if previous is None or overwrite:\n                self.cache[0][i][index] = t\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Exit the frame, i.e. close all open files and purge the cache.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def close(self):\n    \"\"\"Exit the frame, i.e. close all open files and purge the cache.\"\"\"\n    self.purge_cache(reset=False)\n    for f in self.frame:\n        f.close()\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.default_sort_key","title":"default_sort_key  <code>staticmethod</code>","text":"<pre><code>default_sort_key(x: Any) -&gt; int\n</code></pre> <p>Default sort key for HDF5 groups in the frame tree.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>@staticmethod\ndef default_sort_key(x: Any) -&gt; int:\n    \"\"\"Default sort key for HDF5 groups in the frame tree.\"\"\"\n    return int(x.split(\"/\")[-1]) if x.split(\"/\")[-1].isdigit() else 0\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.filter_tree","title":"filter_tree","text":"<pre><code>filter_tree(\n    groups: tuple[str, ...],\n    omit_keys: tuple[str, ...] | None = None,\n    strip_root: bool = True,\n) -&gt; list[list[str]]\n</code></pre> <p>Filter frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>tuple[str, ...]</code> <p>Names of groups to check against the entire frame structure.</p> required <code>omit_keys</code> <code>tuple[str, ...] | None</code> <p>Keywords to ignore while filtering (for speed-up when <code>frame_tree</code> is massive).</p> <code>None</code> <code>strip_root</code> <code>bool</code> <p>If True, the groups starting with <code>/</code> are stripped.</p> <code>True</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def filter_tree(\n    self,\n    groups: tuple[str, ...],\n    omit_keys: tuple[str, ...] | None = None,\n    strip_root: bool = True,\n) -&gt; list[list[str]]:\n    \"\"\"Filter frame structure (HDF5 Groups) of each frame (HDF5 File instance).\n\n    Args:\n        groups: Names of groups to check against the entire frame structure.\n        omit_keys: Keywords to ignore while filtering\n          (for speed-up when `frame_tree` is massive).\n        strip_root: If True, the groups starting with `/` are stripped.\n    \"\"\"\n    filtered_groups: tuple[str, ...] = ()\n    for g in groups:\n        g = g[1:] if g.startswith(\"/\") else g\n        for i, t in enumerate(self.frame_tree):\n            matches = fnmatch.filter(t, g)\n            if omit_keys:\n                matches = [m for m in matches if not any(k in m for k in omit_keys)]\n            filtered_groups += tuple(matches)\n    return tuple(dict.fromkeys(filtered_groups).keys())\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.get_cached_attrs","title":"get_cached_attrs","text":"<pre><code>get_cached_attrs(\n    index: int,\n) -&gt; list[torch.Tensor | dict | None]\n</code></pre> <p>Fetch attrs if in cache.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def get_cached_attrs(self, index: int) -&gt; list[torch.Tensor | dict | None]:\n    \"\"\"Fetch attrs if in cache.\"\"\"\n    if hasattr(self, \"cache\") and self.cache[1] is not None:\n        return [c[index] for c in self.cache[1]]\n    return None\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.get_cached_item","title":"get_cached_item","text":"<pre><code>get_cached_item(index: int) -&gt; list[torch.Tensor | None]\n</code></pre> <p>Fetch item(s) if in cache.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def get_cached_item(self, index: int) -&gt; list[torch.Tensor | None]:\n    \"\"\"Fetch item(s) if in cache.\"\"\"\n    if hasattr(self, \"cache\") and self.cache[0] is not None:\n        return [c[index] for c in self.cache[0]]\n    return [None for _ in self.indexed_dims]\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.glob_path","title":"glob_path  <code>staticmethod</code>","text":"<pre><code>glob_path(\n    path: str | Path | list[str] | list[Path],\n) -&gt; list[Path]\n</code></pre> <p>Glob path recursively for HDF5 files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | list[str] | list[Path]</code> <p>Filename, path or list, can contain wildcards <code>*</code> or <code>**</code>.</p> required Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>@staticmethod\ndef glob_path(path: str | Path | list[str] | list[Path]) -&gt; list[Path]:\n    \"\"\"Glob path recursively for HDF5 files.\n\n    Args:\n        path: Filename, path or list, can contain wildcards `*` or `**`.\n    \"\"\"\n    files: list[Path] = []\n    root, file_key = HDF5Dataset._split_glob(path)\n    for p, k in zip(root, file_key):\n        if k is None:\n            files.append(Path(p))\n        else:\n            path_files = [\n                f\n                for f in Path(p).rglob(k)\n                if f.is_file() and f.suffix in H5_FILE_EXTENSIONS\n            ]\n            files += sorted(path_files)\n    files = [p for p in files if p.exists()]\n    return files\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.info","title":"info","text":"<pre><code>info(\n    print_: bool = True,\n    show_data_info: bool = True,\n    show_attrs_info: bool = True,\n) -&gt; str\n</code></pre> <p>Print summary information about the frame.</p> <p>Parameters:</p> Name Type Description Default <code>print_</code> <code>bool</code> <p>Print to stdout, otherwise only return summary string.</p> <code>True</code> <code>show_data_info</code> <code>bool</code> <p>Include detailed info about the datasets in the frame.</p> <code>True</code> <code>show_attrs_info</code> <code>bool</code> <p>Include detailed info about the attribute sets in the frame.</p> <code>True</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def info(\n    self,\n    print_: bool = True,\n    show_data_info: bool = True,\n    show_attrs_info: bool = True,\n) -&gt; str:\n    \"\"\"Print summary information about the frame.\n\n    Args:\n        print_: Print to stdout, otherwise only return summary string.\n        show_data_info: Include detailed info about the datasets in the frame.\n        show_attrs_info: Include detailed info about the attribute sets in the frame.\n    \"\"\"\n    dims = self.dims if isinstance(self.dims, tuple) else self.indexed_dims\n    summary = str(self)\n    summary += \"\\n\" + \"-\" * 50 + \"\\n\"\n    summary += f\"Serial size:             \\t ({self.serial_size[0].as_str()}, {self.serial_size[1].as_str()})\\n\"\n    summary += f\"Sample size:             \\t ({self.sample_serial_size[0].as_str()}, {self.sample_serial_size[1].as_str()})\\n\"\n    summary += f\"Cache size:              \\t {self.cached_bytes.as_str()} / {self.cache_size.as_str()}\\n\"\n    summary += f\"Number of files:         \\t {self.n_files}\\n\"\n    summary += f\"Selected groups:         \\t {self.groups}\\n\"\n    summary += f\"Number of datasets:      \\t {self.n_datasets}\\n\"\n    summary += f\"Number of samples:       \\t {len(self)}\\n\"\n    summary += f\"Selected attrs:          \\t {self.attr_groups_selected}\\n\"\n    summary += f\"Number of attribute sets:\\t {self.n_attrsets}\\n\"\n    summary += f\"Number of attrs:         \\t {self.n_attrs}\\n\"\n    if show_data_info:\n        data_info = [\n            [f\"H5PyDataset={di}, shape={si}\" for di, si in zip(d, s)]\n            for d, s in zip(self.data_groups, self.raw_dims)\n        ]\n        summary += f\"Data {dims}:\\n{pprint.pformat(data_info)}\\n\"\n    if show_attrs_info:\n        attrs_info = self.frame_attrs\n        summary += f\"Attrs:\\n{pprint.pformat(attrs_info)}\\n\"\n    if print_:\n        print(summary)\n    return summary\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.init_cache","title":"init_cache","text":"<pre><code>init_cache(\n    cache: int | float | str | None = \"4G\",\n    attrs_cache: int | float | str | None = \"64M\",\n)\n</code></pre> <p>Initialize the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>int | float | str | None</code> <p>Cache size for data items.</p> <code>'4G'</code> <code>attrs_cache</code> <code>int | float | str | None</code> <p>Cache size for attributes.</p> <code>'64M'</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def init_cache(\n    self,\n    cache: int | float | str | None = \"4G\",\n    attrs_cache: int | float | str | None = \"64M\",\n):\n    \"\"\"Initialize the cache.\n\n    Args:\n        cache: Cache size for data items.\n        attrs_cache: Cache size for attributes.\n    \"\"\"\n    if cache and cache is not None:\n        output_length = max(len(self.indexed_dims), 1)\n        cache = nbytes(cache) / output_length\n        self.cache = (\n            [\n                SharedArray(shape=dims, size=cache.as_bstr(), dtype=self.dtype)\n                for dims in self.indexed_dims\n            ],\n        )\n    else:\n        self.cache = (None,)\n    if self.n_attrs &gt; 0 and attrs_cache is not None:\n        output_length = max(len(self.indexed_dims), 1)\n        attrs_cache = nbytes(attrs_cache) / output_length\n        if self.attrs_is_data:\n            self.cache += (\n                [\n                    SharedArray(shape=dims, size=attrs_cache.as_bstr(), dtype=dtype)\n                    for dims, dtype in zip(self.attrs_dims, self.attrs_dtype)\n                ],\n            )\n        elif self.attrs_is_dict:\n            self.cache += (\n                [\n                    SharedDictList(\n                        n=dims[0],\n                        size=attrs_cache.as_bstr(),\n                        slot_size=self.sample_serial_size[1].as_bstr(),\n                        descr=f\"shm_list_{i}\",\n                    )\n                    for i, dims in enumerate(self.attrs_dims)\n                ],\n            )\n    else:\n        self.cache += (None,)\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.is_aligned","title":"is_aligned  <code>staticmethod</code>","text":"<pre><code>is_aligned(dims: tuple | list[list[tuple]]) -&gt; bool\n</code></pre> <p>Test if the model is aligned, i.e. same sample size for parallel reads.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>tuple | list[list[tuple]]</code> <p>Dimensions to be tested for alignment.</p> required Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>@staticmethod\ndef is_aligned(dims: tuple | list[list[tuple]]) -&gt; bool:\n    \"\"\"Test if the model is aligned, i.e. same sample size for parallel reads.\n\n    Args:\n        dims: Dimensions to be tested for alignment.\n    \"\"\"\n    if isinstance(dims, tuple):\n        return True\n    sizes = [[di[0] for di in d] for d in dims]\n    collated_sizes = HDF5Dataset._collate(sizes)\n    return all([len(set(cs)) == 1 for cs in collated_sizes])\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.load_frame","title":"load_frame","text":"<pre><code>load_frame(**kwargs) -&gt; list[H5PyFile] | None\n</code></pre> <p>Load HDF5 file instances.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments for <code>h5py.File</code>.</p> <code>{}</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def load_frame(self, **kwargs) -&gt; list[H5PyFile] | None:\n    \"\"\"Load HDF5 file instances.\n\n    Args:\n        kwargs: Keyword arguments for `h5py.File`.\n    \"\"\"\n    self._frame = [H5PyFile(f, \"r\", **kwargs) for f in self.files]\n    self._virt_frame = []\n    return self._frame\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.load_frame_tree","title":"load_frame_tree","text":"<pre><code>load_frame_tree(\n    sort_key: Callable | None = None,\n) -&gt; list[list[str]]\n</code></pre> <p>Load tree of HDF5 group for each frame (HDF5 file instance).</p> <p>Parameters:</p> Name Type Description Default <code>sort_key</code> <code>Callable | None</code> <p>Sorting key function for the list of HDF5 groups.</p> <code>None</code> Note <p>This method walks through the entire HDF5 group tree and could take some time.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def load_frame_tree(self, sort_key: Callable | None = None) -&gt; list[list[str]]:\n    \"\"\"Load tree of HDF5 group for each frame (HDF5 file instance).\n\n    Args:\n        sort_key: Sorting key function for the list of HDF5 groups.\n\n    Note:\n        This method walks through the entire HDF5 group tree and could take some time.\n    \"\"\"\n    tree: list[list[str]] = []\n    sort_key = sort_key if sort_key else self.sort_key\n    for i, f in enumerate(self.frame):\n        tree.append([])\n        f.visit(tree[i].append)\n    self._frame_tree = [sorted(t, key=sort_key) for t in tree]\n    return self._frame_tree\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.make_index","title":"make_index","text":"<pre><code>make_index() -&gt; list[H5PyDataset]\n</code></pre> <p>Index frame datasets, apply collation and parallelization settings.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def make_index(self) -&gt; list[H5PyDataset]:\n    \"\"\"Index frame datasets, apply collation and parallelization settings.\"\"\"\n    if not hasattr(self, \"indexed_datasets\"):\n        dims = self.dims\n        datasets = self.frame_datasets\n        if not datasets:\n            self.indexed_datasets = datasets\n            return None\n        if self.collate:\n            datasets = self._collate_fn(datasets)\n        if self.pair:\n            datasets = self._pair(datasets)\n        if self.parallel or self.pair:\n            datasets = self._parallelize(datasets)\n        elif self.squash and isinstance(dims, tuple):\n            datasets = self._contiguate(datasets)\n        dims = [dims] if isinstance(dims, tuple) else dims\n        datasets = [\n            self._stitch_datasets(ds, layout=sh) for ds, sh in zip(datasets, dims)\n        ]\n        self.indexed_datasets = datasets\n    return self.indexed_datasets\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.pin_attrs","title":"pin_attrs","text":"<pre><code>pin_attrs(\n    attr_groups: tuple[str, ...] | None = None,\n) -&gt; list[H5PyAttrs | H5PyDataset]\n</code></pre> <p>Find and pin attribute sets (metadata) in the frame.</p> <p>Parameters:</p> Name Type Description Default <code>attr_groups</code> <code>tuple[str, ...] | None</code> <p>Location(s) or filter pattern(s) for HDF5 groups containing HDF5 attributes (allows wildcards '' and '*'). Note, if <code>None</code>, no attributes are pinned.</p> <code>None</code> <p>Note, if no attribute sets are found, separate (other than group-selected) datasets are pinned.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def pin_attrs(\n    self, attr_groups: tuple[str, ...] | None = None\n) -&gt; list[H5PyAttrs | H5PyDataset]:\n    \"\"\"Find and pin attribute sets (metadata) in the frame.\n\n    Args:\n        attr_groups: Location(s) or filter pattern(s) for HDF5 groups\n          containing HDF5 attributes (allows wildcards '*' and '**'). Note,\n          if `None`, no attributes are pinned.\n\n    Note, if no attribute sets are found, separate (other than group-selected)\n    datasets are pinned.\n    \"\"\"\n    attrs: list[list[H5PyAttrs | H5PyDataset]] = []\n    if attr_groups is None or not attr_groups:\n        self.frame_attrs = []\n    if attr_groups and any(\"*\" in g for g in attr_groups):\n        attr_groups = self.filter_tree(attr_groups)\n    if attr_groups:\n        for i, f in enumerate(self.frame):\n            # look for HDF5 Attributes\n            attrs_list = [\n                f[g].attrs for g in attr_groups if g in f and f[g].attrs.keys()\n            ]\n            # if no HDF5 Attributes found, look for other HDF5 Datasets\n            if not attrs_list:\n                attrs_list = [\n                    f[g]\n                    for g in attr_groups\n                    if g in f and isinstance(f[g], H5PyDataset)\n                ]\n                # check if attrs are just data and filter the list\n                if len(self.frame_datasets) &gt; i:\n                    data_list = self.frame_datasets[i]\n                    for d in data_list:\n                        for i, a in enumerate(attrs_list):\n                            attr_is_data = (\n                                d.shape == a.shape\n                                and d.size == a.size\n                                and d.nbytes == a.nbytes\n                                and d.name == a.name\n                            )\n                            if attr_is_data:\n                                attrs_list.pop(i)\n            if attrs_list:\n                attrs.append(attrs_list)\n    self.frame_attrs = attrs\n    return self.frame_attrs\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.pin_data","title":"pin_data","text":"<pre><code>pin_data(\n    groups: tuple[str, ...] | None = (\"*\",),\n) -&gt; list[H5PyDataset]\n</code></pre> <p>Find and pin datasets in the frame.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>tuple[str, ...] | None</code> <p>Location(s) or filter pattern(s) for HDF5 groups containing HDF5 datasets (allows wildcards '' and '*'). Note, if <code>None</code>, the entire HDF5 tree will be checked, which could take a long time for HDF5 files that contain many groups.</p> <code>('*',)</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def pin_data(self, groups: tuple[str, ...] | None = (\"*\",)) -&gt; list[H5PyDataset]:\n    \"\"\"Find and pin datasets in the frame.\n\n    Args:\n        groups: Location(s) or filter pattern(s) for HDF5 groups containing\n          HDF5 datasets (allows wildcards '*' and '**'). Note, if `None`, the\n          entire HDF5 tree will be checked, which could take a long time for\n          HDF5 files that contain many groups.\n    \"\"\"\n    datasets: list[list[H5PyDataset]] = []\n    if groups is None:\n        groups = self.groups or (\"*\",)\n    if any(\"*\" in g for g in groups):\n        groups = self.filter_tree(groups, omit_keys=self.data_ignores)\n    for f in self.frame:\n        datasets.append(\n            [f[g] for g in groups if g in f and isinstance(f[g], H5PyDataset)]\n        )\n    self.frame_datasets = datasets\n    return self.frame_datasets\n</code></pre>"},{"location":"reference/chuchichaestli/data/#chuchichaestli.data.HDF5Dataset.purge_cache","title":"purge_cache","text":"<pre><code>purge_cache(reset: bool = False)\n</code></pre> <p>Purge the cache.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>If True, the cache instance attribute is reinitialized.</p> <code>False</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def purge_cache(self, reset: bool = False):\n    \"\"\"Purge the cache.\n\n    Args:\n        reset: If True, the cache instance attribute is reinitialized.\n    \"\"\"\n    cache_sizes = [None, None]\n    for i, c in enumerate(self.cache):\n        if c is not None:\n            cache_sizes[i] = 0\n            for ci in c:\n                cache_sizes[i] += ci.cache_size\n                ci.clear_allocation()\n    del self.cache\n    if reset:\n        self.init_cache(*cache_sizes)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/","title":"chuchichaestli.data.cache","text":""},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache","title":"chuchichaestli.data.cache","text":"<p>Caching for tensors from PyTorch datasets.</p> <p>Classes:</p> Name Description <code>SharedArray</code> <p>A shared memory array for use as tensor data cache in PyTorch datasets.</p> <code>SharedDict</code> <p>A shared memory dictionary for use as metadata cache in PyTorch datasets.</p> <code>SharedDictList</code> <p>A shared memory list of dictionaries for use as metadata cache in PyTorch datasets.</p> <code>nbytes</code> <p>A float class which accepts byte size strings, e.g. '4.2 GB'.</p> <p>Functions:</p> Name Description <code>get_max_ram</code> <p>Get the maximal size of total RAM.</p> <code>get_max_shm</code> <p>Get the maximal size of available shared memory.</p> <code>serial_byte_size</code> <p>Estimate the size of a serializable object (e.g. a dictionary).</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray","title":"SharedArray","text":"<pre><code>SharedArray(\n    shape: tuple[int, ...],\n    size: int | float | str = \"4.0G\",\n    dtype: torch.dtype = torch.float32,\n    use_lock: bool = True,\n    allow_overwrite: bool = True,\n    verbose: bool = False,\n)\n</code></pre> <p>A shared memory array for use as tensor data cache in PyTorch datasets.</p> <p>Uses a ctypes multiprocessing array on shared memory as basis. If the size of the dataset exceeds the size of the set cache limit, only the first N samples will be cached. For shuffled datasets, this is called 'stochastic caching'.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>Dataset N-d shape, e.g. (n_samples, channels, width, height, depth).</p> required <code>size</code> <code>int | float | str</code> <p>Maximum cache size in GiB (if int or float); default \"4.0 GiB\".</p> <code>'4.0G'</code> <code>dtype</code> <code>torch.dtype</code> <p>PyTorch dtype (default torch.float32). Must be type with corresponding ctype, i.e. bool, uint8, int8/16/32/64, or float32/64.</p> <code>torch.float32</code> <code>use_lock</code> <code>bool</code> <p>If True, applies a threading lock for multiprocessing.</p> <code>True</code> <code>allow_overwrite</code> <code>bool</code> <p>If True, cache slots can be overwritten.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print information to the stdout.</p> <code>False</code> <p>Methods:</p> Name Description <code>clear</code> <p>Clear the cache (optionally only at a specified index).</p> <code>clear_allocation</code> <p>Delete shared memory allocation.</p> <code>get_state</code> <p>Get the slot state at specified index.</p> <p>Attributes:</p> Name Type Description <code>array</code> <code>torch.Tensor</code> <p>Shared-memory tensor data array (cached samples).</p> <code>cached_bytes</code> <code>nbytes</code> <p>Bytes written to cache.</p> <code>cached_states</code> <code>int</code> <p>Written states in cache.</p> <code>states</code> <code>torch.Tensor</code> <p>Shared-memory tensor state array.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def __init__(\n    self,\n    shape: tuple[int, ...],\n    size: int | float | str = \"4.0G\",\n    dtype: torch.dtype = torch.float32,\n    use_lock: bool = True,\n    allow_overwrite: bool = True,\n    verbose: bool = False,\n):\n    \"\"\"Constructor.\n\n    Args:\n        shape: Dataset N-d shape, e.g. (n_samples, channels, width, height, depth).\n        size: Maximum cache size in GiB (if int or float); default \"4.0 GiB\".\n        dtype: PyTorch dtype (default torch.float32). Must be type with corresponding ctype,\n          i.e. bool, uint8, int8/16/32/64, or float32/64.\n        use_lock: If True, applies a threading lock for multiprocessing.\n        allow_overwrite: If True, cache slots can be overwritten.\n        verbose: Print information to the stdout.\n    \"\"\"\n    self.cache_size = (\n        nbytes(f\"{size}G\") if isinstance(size, int | float) else nbytes(size)\n    )\n    self.allow_overwrite = allow_overwrite\n    self.verbose = verbose\n\n    if dtype not in C_DTYPES:\n        raise ValueError(\n            f\"Unsupported dtype: {dtype}. Must be one of {C_DTYPES.keys()}\"\n        )\n    slot_size = int(np.prod(shape[1:]))\n    slot_bytes = nbytes(slot_size * nbytes(dtype.itemsize))\n    dataset_bytes = nbytes(shape[0] * slot_bytes)\n    cache_bytes = self.cache_size.to(\"B\")\n    states_bytes = nbytes(shape[0] * torch.uint8.itemsize)\n\n    total_bytes = dataset_bytes + states_bytes\n\n    if total_bytes &gt; cache_bytes:\n        n_slots = int((cache_bytes - states_bytes) / slot_bytes)\n        if self.verbose:\n            print(\n                f\"Requested memory ({total_bytes}) \"\n                f\"exceeds cache size ({self.cache_size}).\\n\"\n                f\"Allocating cache for {n_slots} / {shape[0]} data samples.\"\n            )\n    else:\n        n_slots = shape[0]\n        if self.verbose:\n            print(\n                f\"Data size ({total_bytes}) fits into \"\n                f\"requested cache size ({self.cache_size}).\\n\"\n                f\"Allocating cache for {n_slots} data samples.\"\n            )\n\n    mp_arr = mp.Array(C_DTYPES[dtype], n_slots * slot_size, lock=use_lock)  # type: ignore\n    shm_arr = np.ctypeslib.as_array(mp_arr.get_obj())\n    shm_arr = shm_arr.reshape((n_slots, *shape[1:]))\n    self._slots = torch.from_numpy(shm_arr)\n    self._slots *= 0\n\n    mp_states_arr = mp.Array(C_DTYPES[torch.uint8], shape[0])  # type: ignore\n    shm_states_arr = np.ctypeslib.as_array(mp_states_arr.get_obj())\n    self._shm_states = torch.from_numpy(shm_states_arr)\n    self._shm_states *= 0\n    self._shm_states[n_slots:] = SlotState.OOC.value\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.array","title":"array  <code>property</code>","text":"<pre><code>array: torch.Tensor\n</code></pre> <p>Shared-memory tensor data array (cached samples).</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.cached_bytes","title":"cached_bytes  <code>property</code>","text":"<pre><code>cached_bytes: nbytes\n</code></pre> <p>Bytes written to cache.</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.cached_states","title":"cached_states  <code>property</code>","text":"<pre><code>cached_states: int\n</code></pre> <p>Written states in cache.</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.states","title":"states  <code>property</code>","text":"<pre><code>states: torch.Tensor\n</code></pre> <p>Shared-memory tensor state array.</p> Note <ul> <li>states[index] == 0 means sample at index are not yet cached.</li> <li>states[index] == 1 means sample at index are cached.</li> <li>states[index] == 2 means sample at index cannot be cached (due to cache limit).</li> </ul>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.clear","title":"clear","text":"<pre><code>clear(index: int | None = None)\n</code></pre> <p>Clear the cache (optionally only at a specified index).</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def clear(self, index: int | None = None):\n    \"\"\"Clear the cache (optionally only at a specified index).\"\"\"\n    _, index = self.get_state(index)\n    if index is None:\n        self._slots *= 0\n        self._shm_states *= 0\n        self._shm_states[len(self) :] = SlotState.OOC.value\n    else:\n        self[index] = torch.zeros(self._slots.shape[1:])\n        self._shm_states[index] = torch.zeros(self._shm_states.shape[1:])\n        if len(self) &lt;= index:\n            self._shm_states[index] = SlotState.OOC.value\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.clear_allocation","title":"clear_allocation","text":"<pre><code>clear_allocation()\n</code></pre> <p>Delete shared memory allocation.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def clear_allocation(self):\n    \"\"\"Delete shared memory allocation.\"\"\"\n    del self._slots\n    del self._shm_states\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedArray.get_state","title":"get_state","text":"<pre><code>get_state(\n    index: int | None,\n) -&gt; tuple[SlotState, int | None]\n</code></pre> <p>Get the slot state at specified index.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get_state(self, index: int | None) -&gt; tuple[SlotState, int | None]:\n    \"\"\"Get the slot state at specified index.\"\"\"\n    if index is None:\n        return SlotState.INVALID, None\n    if index &lt; 0:\n        index = len(self.states) + index\n    if index &lt; 0 or index &gt;= len(self.states):\n        raise IndexError(\n            f\"Index {index} out of range for dataset {list(self.states.shape)}\"\n        )\n    return SlotState(self.states[index].item()), index\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict","title":"SharedDict","text":"<pre><code>SharedDict(\n    descr: str = \"shm_dict\",\n    size: int | float | str = \"16M\",\n    serializer: DictSerializer = PickleSerializer(),\n    use_lock: bool = True,\n    allow_overwrite: bool = True,\n    verbose: bool = False,\n    **kwargs,\n)\n</code></pre> <p>A shared memory dictionary for use as metadata cache in PyTorch datasets.</p> Serialization of PyTorch tensors is in principle possible, but it is recommended <p>to use the SharedArray class for tensor types instead.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>descr</code> <code>str</code> <p>Descriptor ID for shared memory access.</p> <code>'shm_dict'</code> <code>size</code> <code>int | float | str</code> <p>Maximum cache size in MiB (if int or float); default \"16.0 GiB\".</p> <code>'16M'</code> <code>serializer</code> <code>DictSerializer</code> <p>Serializer for the encoding of the dictionary data.</p> <code>PickleSerializer()</code> <code>use_lock</code> <code>bool</code> <p>If True, applies a threading lock for multiprocessing.</p> <code>True</code> <code>allow_overwrite</code> <code>bool</code> <p>If True, cache slots can be overwritten.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print information to the stdout.</p> <code>False</code> <code>kwargs</code> <p>Key-value dictionary pairs to load into memory.</p> <code>{}</code> If the dictionary is supposed to contain the keys <p>['descr', 'size', 'sample_size', 'allow_overwrite', 'serlializer', 'verbose']</p> <p>Methods:</p> Name Description <code>clear</code> <p>Clear the data in the shared memory buffer.</p> <code>clear_allocation</code> <p>Delete shared memory allocation.</p> <code>get</code> <p>Getter (value by key) for dictionary in shared memory.</p> <code>get_allocation</code> <p>Get a shared memory allocation.</p> <code>items</code> <p>Getter for items of dictionary in shared memory.</p> <code>keys</code> <p>Getter for keys of dictionary in shared memory.</p> <code>open_buffer</code> <p>Buffer context manager.</p> <code>pop</code> <p>Pop the dictionary in shared memory.</p> <code>read_buffer</code> <p>Read the shared memory buffer.</p> <code>setdefault</code> <p>Setdefault the dictionary in shared memory.</p> <code>update</code> <p>Update the dictionary in shared memory.</p> <code>values</code> <p>Getter for values of dictionary in shared memory.</p> <code>write_buffer</code> <p>Write data to the shared memory buffer.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def __init__(\n    self,\n    descr: str = \"shm_dict\",\n    size: int | float | str = \"16M\",\n    serializer: DictSerializer = PickleSerializer(),\n    use_lock: bool = True,\n    allow_overwrite: bool = True,\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        descr: Descriptor ID for shared memory access.\n        size: Maximum cache size in MiB (if int or float); default \"16.0 GiB\".\n        serializer: Serializer for the encoding of the dictionary data.\n        use_lock: If True, applies a threading lock for multiprocessing.\n        allow_overwrite: If True, cache slots can be overwritten.\n        verbose: Print information to the stdout.\n        kwargs: Key-value dictionary pairs to load into memory.\n\n    Note: If the dictionary is supposed to contain the keys\n      ['descr', 'size', 'sample_size', 'allow_overwrite', 'serlializer', 'verbose']\n    \"\"\"\n    super().__init__()\n    self.descr = descr\n    self.allow_overwrite = True\n    self.cache_size = (\n        nbytes(f\"{size}M\") if isinstance(size, int | float) else nbytes(size)\n    )\n    if self.cache_size &lt;= 5:\n        raise ValueError(\"Chosen cache size is too small!\")\n    self.serializer = serializer\n    self._lock: mp.synchronize.Lock | DummyLock\n    if use_lock:\n        self._lock = Lock()\n    else:\n        self._lock = DummyLock()\n    self.verbose = verbose\n    self.shm = self.get_allocation()\n    self.clear()\n    self.update(kwargs)\n    self.allow_overwrite = allow_overwrite\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear the data in the shared memory buffer.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>@lock\ndef clear(self):\n    \"\"\"Clear the data in the shared memory buffer.\"\"\"\n    self.write_buffer({})\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.clear_allocation","title":"clear_allocation","text":"<pre><code>clear_allocation()\n</code></pre> <p>Delete shared memory allocation.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def clear_allocation(\n    self,\n):\n    \"\"\"Delete shared memory allocation.\"\"\"\n    if hasattr(self, \"shm\"):\n        try:\n            self.shm.close()\n            self.shm.unlink()\n        except FileNotFoundError:\n            self.shm.close()\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.get","title":"get","text":"<pre><code>get(key: str | int, default: Any | None = None) -&gt; Any\n</code></pre> <p>Getter (value by key) for dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get(self, key: str | int, default: Any | None = None) -&gt; Any:\n    \"\"\"Getter (value by key) for dictionary in shared memory.\"\"\"\n    return self.read_buffer().get(key, default)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.get_allocation","title":"get_allocation","text":"<pre><code>get_allocation(\n    name: str | None = None,\n    size: int | float | str | None = None,\n    **kwargs,\n) -&gt; SharedMemory\n</code></pre> <p>Get a shared memory allocation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Descriptor ID for shared memory access.</p> <code>None</code> <code>size</code> <code>int | float | str | None</code> <p>Number of samples in the dataset.</p> <code>None</code> <code>kwargs</code> <p>Additional keywords for compatibility.</p> <code>{}</code> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get_allocation(\n    self, name: str | None = None, size: int | float | str | None = None, **kwargs\n) -&gt; SharedMemory:\n    \"\"\"Get a shared memory allocation.\n\n    Args:\n        name: Descriptor ID for shared memory access.\n        size: Number of samples in the dataset.\n        kwargs: Additional keywords for compatibility.\n    \"\"\"\n    if name is None:\n        name = self.descr\n    if size is None or size == 0:\n        size = self.cache_size\n    try:\n        shm = SharedMemory(name=name)\n    except FileNotFoundError:\n        shm = SharedMemory(name=name, create=True, size=int(size))\n    return shm\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.items","title":"items","text":"<pre><code>items() -&gt; ItemsView\n</code></pre> <p>Getter for items of dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def items(self) -&gt; ItemsView:\n    \"\"\"Getter for items of dictionary in shared memory.\"\"\"\n    return self.read_buffer().items()\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.keys","title":"keys","text":"<pre><code>keys() -&gt; KeysView[str | int]\n</code></pre> <p>Getter for keys of dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def keys(self) -&gt; KeysView[str | int]:\n    \"\"\"Getter for keys of dictionary in shared memory.\"\"\"\n    return self.read_buffer().keys()\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.open_buffer","title":"open_buffer","text":"<pre><code>open_buffer() -&gt; Generator\n</code></pre> <p>Buffer context manager.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>@contextmanager\n@lock\ndef open_buffer(self) -&gt; Generator:\n    \"\"\"Buffer context manager.\"\"\"\n    dct = self.read_buffer()\n    yield dct\n    self.write_buffer(dct)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.pop","title":"pop","text":"<pre><code>pop(key: str | int, default: Any | None = None)\n</code></pre> <p>Pop the dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def pop(self, key: str | int, default: Any | None = None):\n    \"\"\"Pop the dictionary in shared memory.\"\"\"\n    with self.open_buffer() as dct:\n        if default is None or default is object():\n            return dct.pop(key)\n        return dct.pop(key, default)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.read_buffer","title":"read_buffer","text":"<pre><code>read_buffer() -&gt; dict\n</code></pre> <p>Read the shared memory buffer.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def read_buffer(self) -&gt; dict:\n    \"\"\"Read the shared memory buffer.\"\"\"\n    return self.serializer.loads(self.shm.buf.tobytes())\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.setdefault","title":"setdefault","text":"<pre><code>setdefault(\n    key: str | int, default: Any | None = None\n) -&gt; dict\n</code></pre> <p>Setdefault the dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def setdefault(self, key: str | int, default: Any | None = None) -&gt; dict:\n    \"\"\"Setdefault the dictionary in shared memory.\"\"\"\n    with self.open_buffer() as dct:\n        return dct.setdefault(key, default)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.update","title":"update","text":"<pre><code>update(other=(), /, **kwargs)\n</code></pre> <p>Update the dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def update(self, other=(), /, **kwargs):\n    \"\"\"Update the dictionary in shared memory.\"\"\"\n    with self.open_buffer() as dct:\n        dct.update(other, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.values","title":"values","text":"<pre><code>values() -&gt; ValuesView\n</code></pre> <p>Getter for values of dictionary in shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def values(self) -&gt; ValuesView:\n    \"\"\"Getter for values of dictionary in shared memory.\"\"\"\n    return self.read_buffer().values()\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDict.write_buffer","title":"write_buffer","text":"<pre><code>write_buffer(data: dict) -&gt; bytes | None\n</code></pre> <p>Write data to the shared memory buffer.</p> <p>Returns:</p> Type Description <code>bytes | None</code> <p>Written data if it was successfully written to the buffer, None otherwise.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def write_buffer(self, data: dict) -&gt; bytes | None:\n    \"\"\"Write data to the shared memory buffer.\n\n    Returns:\n        Written data if it was successfully written to the buffer, None otherwise.\n    \"\"\"\n    byte_data = self.serializer.dumps(data)\n    if not hasattr(self, \"shm\"):\n        self.get_allocation()\n    if len(byte_data) &lt; self.cache_size:\n        self.shm.buf[: len(byte_data)] = byte_data\n    else:\n        return None\n    return byte_data\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList","title":"SharedDictList","text":"<pre><code>SharedDictList(\n    n: int,\n    *sequence: list[\n        int | float | bool | str | bytes | dict | None\n    ],\n    descr: str = \"shm_list\",\n    slot_size: int | float | str = \"650b\",\n    size: int | float | str = \"64M\",\n    serializer: DictSerializer = PickleSerializer(),\n    use_lock: bool = True,\n    allow_overwrite: bool = True,\n    verbose: bool = False,\n)\n</code></pre> <p>A shared memory list of dictionaries for use as metadata cache in PyTorch datasets.</p> Serialization of PyTorch tensors is in principle possible, but it is recommended <p>to use the SharedArray class for tensor types instead.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples in the list (can be larger than the number of memory slots).</p> required <code>sequence</code> <code>list[int | float | bool | str | bytes | dict | None]</code> <p>Sequence of built-in types.</p> <code>()</code> <code>descr</code> <code>str</code> <p>Descriptor ID for shared memory access.</p> <code>'shm_list'</code> <code>slot_size</code> <code>int | float | str</code> <p>Size of a single list entry (should be big enough for even the biggest entry, otherwise a maximum size is estimated).</p> <code>'650b'</code> <code>size</code> <code>int | float | str</code> <p>Maximum cache size in MiB (if int or float); default \"16.0 GiB\".</p> <code>'64M'</code> <code>serializer</code> <code>DictSerializer</code> <p>Serializer for the encoding of the dictionary data.</p> <code>PickleSerializer()</code> <code>use_lock</code> <code>bool</code> <p>If True, applies a threading lock for multiprocessing.</p> <code>True</code> <code>allow_overwrite</code> <code>bool</code> <p>If True, cache slots can be overwritten.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print information to the stdout.</p> <code>False</code> If the dictionary is supposed to contain the keys <p>['descr', 'size', 'sample_size', 'allow_overwrite', 'serlializer', 'verbose']</p> <p>Methods:</p> Name Description <code>clear_allocation</code> <p>Delete shared memory allocation.</p> <code>get_allocation</code> <p>Get a shared memory allocation.</p> <code>get_state</code> <p>Get the slot state at specified index.</p> <p>Attributes:</p> Name Type Description <code>cached_bytes</code> <code>nbytes</code> <p>Bytes written to cache.</p> <code>cached_states</code> <code>int</code> <p>Written states in cache.</p> <code>slots</code> <code>Any</code> <p>Shared-memory data slots (cached sample data).</p> <code>states</code> <code>torch.Tensor</code> <p>Shared-memory tensor state array.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def __init__(\n    self,\n    n: int,\n    *sequence: list[int | float | bool | str | bytes | dict | None],\n    descr: str = \"shm_list\",\n    slot_size: int | float | str = \"650b\",\n    size: int | float | str = \"64M\",\n    serializer: DictSerializer = PickleSerializer(),\n    use_lock: bool = True,\n    allow_overwrite: bool = True,\n    verbose: bool = False,\n):\n    \"\"\"Constructor.\n\n    Args:\n        n: Number of samples in the list (can be larger than the number of memory slots).\n        sequence: Sequence of built-in types.\n        descr: Descriptor ID for shared memory access.\n        slot_size: Size of a single list entry (should be big enough for even the\n          biggest entry, otherwise a maximum size is estimated).\n        size: Maximum cache size in MiB (if int or float); default \"16.0 GiB\".\n        serializer: Serializer for the encoding of the dictionary data.\n        use_lock: If True, applies a threading lock for multiprocessing.\n        allow_overwrite: If True, cache slots can be overwritten.\n        verbose: Print information to the stdout.\n\n    Note: If the dictionary is supposed to contain the keys\n      ['descr', 'size', 'sample_size', 'allow_overwrite', 'serlializer', 'verbose']\n    \"\"\"\n    super().__init__()\n\n    self.descr = descr\n    self.serializer = serializer\n    self._lock: mp.synchronize.Lock | DummyLock\n    if use_lock:\n        self._lock = Lock()\n    else:\n        self._lock = DummyLock()\n    self.allow_overwrite = True\n    self.verbose = verbose\n    self.cache_size = (\n        nbytes(f\"{size}M\") if isinstance(size, int | float) else nbytes(size)\n    )\n    if sequence:\n        slot_bytes = max(\n            [nbytes(slot_size)] + [serial_byte_size(v) for v in sequence]\n        )\n    else:\n        slot_bytes = nbytes(slot_size)\n    n_slots_bytes = n * slot_bytes\n    cache_bytes = self.cache_size.to(\"B\")\n    states_bytes = nbytes(n * torch.uint8.itemsize)\n\n    total_bytes = n_slots_bytes + states_bytes\n\n    if total_bytes &gt; cache_bytes:\n        n_slots = int((cache_bytes - states_bytes) / slot_bytes)\n        if self.verbose:\n            print(\n                f\"Requested memory ({total_bytes}) \"\n                f\"exceeds cache size ({self.cache_size}).\\n\"\n                f\"Allocating cache for {n_slots} / {n} slots.\"\n            )\n    else:\n        n_slots = n\n        if self.verbose:\n            print(\n                f\"Data size ({total_bytes}) fits into \"\n                f\"requested cache size ({self.cache_size}).\\n\"\n                f\"Allocating cache for {n_slots} data samples.\"\n            )\n\n    self._slots, self._shm_states = self.get_allocation(\n        name=self.descr, n_slots=n_slots, slot_size=int(slot_bytes), size=n\n    )\n    for i, d in enumerate(sequence):\n        self[i] = d\n    self.allow_overwrite = allow_overwrite\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.cached_bytes","title":"cached_bytes  <code>property</code>","text":"<pre><code>cached_bytes: nbytes\n</code></pre> <p>Bytes written to cache.</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.cached_states","title":"cached_states  <code>property</code>","text":"<pre><code>cached_states: int\n</code></pre> <p>Written states in cache.</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.slots","title":"slots  <code>property</code>","text":"<pre><code>slots: Any\n</code></pre> <p>Shared-memory data slots (cached sample data).</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.states","title":"states  <code>property</code>","text":"<pre><code>states: torch.Tensor\n</code></pre> <p>Shared-memory tensor state array.</p> Note <ul> <li>states[index] == 0 means sample at index are not yet cached.</li> <li>states[index] == 1 means sample at index are cached.</li> <li>states[index] == 2 means sample at index cannot be cached (due to cache limit).</li> </ul>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.clear_allocation","title":"clear_allocation","text":"<pre><code>clear_allocation()\n</code></pre> <p>Delete shared memory allocation.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def clear_allocation(\n    self,\n):\n    \"\"\"Delete shared memory allocation.\"\"\"\n    if hasattr(self, \"_slots\"):\n        try:\n            self._slots.shm.close()\n            self._slots.shm.unlink()\n        except FileNotFoundError:\n            self._slots.shm.close()\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.get_allocation","title":"get_allocation","text":"<pre><code>get_allocation(\n    name: str | None = None,\n    n_slots: int | None = None,\n    slot_size: int | None = None,\n    size: int | None = None,\n    **kwargs,\n) -&gt; tuple[ShareableList, torch.Tensor | None]\n</code></pre> <p>Get a shared memory allocation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Descriptor ID for shared memory access.</p> <code>None</code> <code>n_slots</code> <code>int | None</code> <p>Number of slots the shared list should have.</p> <code>None</code> <code>slot_size</code> <code>int | None</code> <p>Number of bytes each element should allocate in shared memory.</p> <code>None</code> <code>size</code> <code>int | None</code> <p>Number of samples in the dataset.</p> <code>None</code> <code>kwargs</code> <p>Additional keywords for compatibility.</p> <code>{}</code> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get_allocation(\n    self,\n    name: str | None = None,\n    n_slots: int | None = None,\n    slot_size: int | None = None,\n    size: int | None = None,\n    **kwargs,\n) -&gt; tuple[ShareableList, torch.Tensor | None]:\n    \"\"\"Get a shared memory allocation.\n\n    Args:\n        name: Descriptor ID for shared memory access.\n        n_slots: Number of slots the shared list should have.\n        slot_size: Number of bytes each element should allocate in shared memory.\n        size: Number of samples in the dataset.\n        kwargs: Additional keywords for compatibility.\n    \"\"\"\n    if name is None:\n        name = self.descr\n    try:\n        shm_list = ShareableList(name=name)\n    except FileNotFoundError:\n        shm_list = ShareableList([np.random.bytes(slot_size)] * n_slots, name=name)\n    if not hasattr(self, \"_shm_states\"):\n        mp_states_arr = mp.Array(C_DTYPES[torch.uint8], size)  # type: ignore\n        shm_states_arr = np.ctypeslib.as_array(mp_states_arr.get_obj())\n        self._shm_states = torch.from_numpy(shm_states_arr)\n        self._shm_states *= 0\n        self._shm_states[n_slots:] = SlotState.OOC.value\n        _shm_states = self._shm_states\n    else:\n        _shm_states = None\n    return shm_list, _shm_states\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.SharedDictList.get_state","title":"get_state","text":"<pre><code>get_state(\n    index: int | None,\n) -&gt; tuple[SlotState, int | None]\n</code></pre> <p>Get the slot state at specified index.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get_state(self, index: int | None) -&gt; tuple[SlotState, int | None]:\n    \"\"\"Get the slot state at specified index.\"\"\"\n    if index is None:\n        return SlotState.INVALID, None\n    if index &lt; 0:\n        index = len(self.states) + index\n    if index &lt; 0 or index &gt;= len(self.states):\n        raise IndexError(\n            f\"Index {index} out of range for dataset {list(self.states.shape)}\"\n        )\n    return SlotState(self.states[index].item()), index\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.nbytes","title":"nbytes","text":"<p>               Bases: <code>float</code></p> <p>A float class which accepts byte size strings, e.g. '4.2 GB'.</p> <p>Methods:</p> Name Description <code>as_bstr</code> <p>Parse to string in binary units.</p> <code>as_str</code> <p>Parse to string in decimal units.</p> <code>to</code> <p>Convert to unit.</p>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.nbytes.as_bstr","title":"as_bstr","text":"<pre><code>as_bstr() -&gt; str\n</code></pre> <p>Parse to string in binary units.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def as_bstr(self) -&gt; str:\n    \"\"\"Parse to string in binary units.\"\"\"\n    for k in self.units:\n        if not k.endswith(\"B\") and 1 &lt;= int(self) // self.units[k] &lt; 999:\n            return f\"{self / self.units[k]:.2f}{k}\"\n    return \"0B\"\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.nbytes.as_str","title":"as_str","text":"<pre><code>as_str() -&gt; str\n</code></pre> <p>Parse to string in decimal units.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def as_str(self) -&gt; str:\n    \"\"\"Parse to string in decimal units.\"\"\"\n    for k in self.units:\n        if not k.endswith(\"B\"):\n            continue\n        if 1 &lt;= int(self) // self.units[k] &lt; 999:\n            return f\"{self / self.units[k]:.2f}{k}\"\n    return \"0B\"\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.nbytes.to","title":"to","text":"<pre><code>to(unit: str) -&gt; str\n</code></pre> <p>Convert to unit.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def to(self, unit: str) -&gt; str:\n    \"\"\"Convert to unit.\"\"\"\n    if unit in self.units:\n        return self.__class__(self / self.units[unit])\n    else:\n        raise ValueError(f\"Unknown unit, choose from {list(self.units.keys())}.\")\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.get_max_ram","title":"get_max_ram","text":"<pre><code>get_max_ram() -&gt; nbytes\n</code></pre> <p>Get the maximal size of total RAM.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get_max_ram() -&gt; nbytes:\n    \"\"\"Get the maximal size of total RAM.\"\"\"\n    return nbytes(psutil.virtual_memory().total)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.get_max_shm","title":"get_max_shm","text":"<pre><code>get_max_shm() -&gt; nbytes\n</code></pre> <p>Get the maximal size of available shared memory.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def get_max_shm() -&gt; nbytes:\n    \"\"\"Get the maximal size of available shared memory.\"\"\"\n    return nbytes(psutil.Process().memory_info().rss)\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.lock","title":"lock","text":"<pre><code>lock(fn)\n</code></pre> <p>Multiprocessing lock decorator.</p> <p>Note: This requires the instance to have a _lock attribute.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def lock(fn):\n    \"\"\"Multiprocessing lock decorator.\n\n    Note: This requires the instance to have a _lock attribute.\n    \"\"\"\n\n    @wraps(fn)\n    def wrapper(self, *args, **kwargs):\n        self._lock.acquire()\n        try:\n            return fn(self, *args, **kwargs)\n        finally:\n            self._lock.release()\n\n    return wrapper\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.npy_to_torch_dtype","title":"npy_to_torch_dtype","text":"<pre><code>npy_to_torch_dtype(dtype: str | np.dtype) -&gt; torch.dtype\n</code></pre> <p>Converts numpy to torch data types.</p> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def npy_to_torch_dtype(dtype: str | np.dtype) -&gt; torch.dtype:\n    \"\"\"Converts numpy to torch data types.\"\"\"\n    dtype = str(dtype)\n    return (\n        NPY_TO_TORCH_DTYPES[str(dtype)] if dtype in NPY_TO_TORCH_DTYPES.keys() else None\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/data/cache/#chuchichaestli.data.cache.serial_byte_size","title":"serial_byte_size","text":"<pre><code>serial_byte_size(\n    dct: Any,\n    serializer: type[DictSerializer] = PickleSerializer,\n) -&gt; nbytes\n</code></pre> <p>Estimate the size of a serializable object (e.g. a dictionary).</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>Any</code> <p>Any (through pickle) serializable data (e.g. a dictionary)</p> required <code>serializer</code> <code>type[DictSerializer]</code> <p>Callable for the encoding of the input data (e.g. dictionary).</p> <code>PickleSerializer</code> Source code in <code>src/chuchichaestli/data/cache.py</code> <pre><code>def serial_byte_size(\n    dct: Any,\n    serializer: type[DictSerializer] = PickleSerializer,\n) -&gt; nbytes:\n    \"\"\"Estimate the size of a serializable object (e.g. a dictionary).\n\n    Args:\n      dct: Any (through pickle) serializable data (e.g. a dictionary)\n      serializer: Callable for the encoding of the input data (e.g. dictionary).\n    \"\"\"\n    enc_bstr = serializer().dumps(dct)\n    return nbytes(len(enc_bstr))\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/","title":"chuchichaestli.data.dataset","text":""},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset","title":"chuchichaestli.data.dataset","text":"<p>PyTorch dataset classes.</p> <p>Classes:</p> Name Description <code>HDF5Dataset</code> <p>Dataset for loading HDF5 frames with optional shared memory (sto)caching.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset","title":"HDF5Dataset","text":"<pre><code>HDF5Dataset(\n    path: str | Path | list[str] | list[Path],\n    groups: str | tuple[str, ...] = \"*\",\n    sort_key: Callable | None = None,\n    collate: bool | Callable = False,\n    parallel: bool = False,\n    pair: bool = False,\n    squash: bool = True,\n    dtype: torch.dtype = torch.float32,\n    attr_groups: str | tuple[str, ...] | None = None,\n    attrs_retrieval: str | Callable = \"auto\",\n    return_as: str | dict | None = \"tuple\",\n    preload: bool = False,\n    cache: int | float | str | bool | None = \"4G\",\n    attrs_cache: int | float | str | bool | None = \"64M\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for loading HDF5 frames with optional shared memory (sto)caching.</p> <p>Constructs a <code>frame</code> of single or multiple HDF5 file objects. Can be selectively pointed to HDF5 Groups containing HDF5 Datasets via the <code>groups</code> keyword. If a frame has multiple datasets, the <code>collate</code> argument determines whether they are read in major-column or major-row ordering across files in a frame. A frame can be read in three different modes (always take effect after collate): - contiguous (parallel=False; sequentializes the datasets and returns single samples) - parallel (parallel=True; multiple samples, one for each dataset) - pair (pair=True; read samples pair-wise parallel) - custom (collate is callable; use your own ordering by passing a function to collate and/or attrs_retrieval) Note that caching only works for datasets with compatible shapes (i.e. image tensors need same dimensions).</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | list[str] | list[Path]</code> <p>Path to a file, files, or data directory which contains the HDF5 files. May contain wildcards <code>*</code> and <code>**</code>.</p> required <code>groups</code> <code>str | tuple[str, ...]</code> <p>Location(s) or filter pattern(s) for HDF5 groups containing HDF5 datasets (allows wildcards <code>*</code> and <code>**</code>). Note, if wildcard expressions have too many matches, the file tree search could take a long time, especially when there are many HDF5Groups in the file. Thus, it is recommended to not use too generic wildcard expressions. Alternatively, use <code>data_ignores</code> to tune the file tree search.</p> <code>'*'</code> <code>sort_key</code> <code>Callable | None</code> <p>Sorting key function for the frame tree of HDF5 groups.</p> <code>None</code> <code>collate</code> <code>bool | Callable</code> <p>If True, multiple HDF5 files are collated into a single dataset. This is equivalent to reading files in Fortran-like row-major ordering. If False, getitem tries to pull samples from each file. Can also be a function that creates a custom ordering with signature <code>func(hdf5_datasets: list[list]) -&gt; list[list]</code>.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>If True, HDF5 datasets are read in parallel, resulting in multiple samples being pulled, otherwise datasets are concatenated and sequentialized (note that contiguous reading (with squash=True) only works, if dataset dimensions are compatible).</p> <code>False</code> <code>pair</code> <code>bool</code> <p>If True, datasets are paired and read pairwise-parallel (automatically sets parallel=True).</p> <code>False</code> <code>squash</code> <code>bool</code> <p>Squash dimension along first axis via summation (default behaviour for sequential, i.e. parallel=False reads).</p> <code>True</code> <code>dtype</code> <code>torch.dtype</code> <p>Data tensor type; default: torch.float32.</p> <code>torch.float32</code> <code>attr_groups</code> <code>str | tuple[str, ...] | None</code> <p>Locations or filter pattern(s) for HDF5 groups containing HDF5 attributes (allows wildcards '' and '*'). If empty or None, no attributes will be filtered. Note: If wildcard expressions have too many matches, the file tree search could take a long time, especially when there are many HDF5Groups in the file. Thus, it is recommended to not use too generic wildcard expressions. Alternatively, use <code>attrs_ignores</code> to tune the file tree search.</p> <code>None</code> <code>attrs_retrieval</code> <code>str | Callable</code> <p>A retrieval strategy for attributes; can be a function one of ['auto' | None | Callable].</p> <code>'auto'</code> <code>return_as</code> <code>str | dict | None</code> <p>Return type of the dataset; one of ['tuple', 'dict', dict, None]. If a dictionary object is give, then the keys in the dictionary are used to map items onto it.</p> <code>'tuple'</code> <code>preload</code> <code>bool</code> <p>Preload and cache the dataset.</p> <code>False</code> <code>cache</code> <code>int | float | str | bool | None</code> <p>If not None or False, memory is allocated for stochastic caching of the tensors.</p> <code>'4G'</code> <code>attrs_cache</code> <code>int | float | str | bool | None</code> <p>If not None or False, memory is allocated for stochastic caching of ancillary data.</p> <code>'64M'</code> <code>kwargs</code> <p>Keyword arguments for <code>h5py.File</code>. See <code>https://docs.h5py.org/en/stable/high/file.html#h5py.File</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>attrs_getter</code> <p>Retrieve ancillary data item at <code>index</code>.</p> <code>cache_attrs</code> <p>Cache attrs (overwrite by default off).</p> <code>cache_item</code> <p>Cache item (overwrite by default off).</p> <code>close</code> <p>Exit the frame, i.e. close all open files and purge the cache.</p> <code>default_sort_key</code> <p>Default sort key for HDF5 groups in the frame tree.</p> <code>filter_tree</code> <p>Filter frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p> <code>get_cached_attrs</code> <p>Fetch attrs if in cache.</p> <code>get_cached_item</code> <p>Fetch item(s) if in cache.</p> <code>glob_path</code> <p>Glob path recursively for HDF5 files.</p> <code>info</code> <p>Print summary information about the frame.</p> <code>init_cache</code> <p>Initialize the cache.</p> <code>is_aligned</code> <p>Test if the model is aligned, i.e. same sample size for parallel reads.</p> <code>load_frame</code> <p>Load HDF5 file instances.</p> <code>load_frame_tree</code> <p>Load tree of HDF5 group for each frame (HDF5 file instance).</p> <code>make_index</code> <p>Index frame datasets, apply collation and parallelization settings.</p> <code>pin_attrs</code> <p>Find and pin attribute sets (metadata) in the frame.</p> <code>pin_data</code> <p>Find and pin datasets in the frame.</p> <code>purge_cache</code> <p>Purge the cache.</p> <p>Attributes:</p> Name Type Description <code>attr_groups</code> <p>HDF5 groups of the selected attribute sets in the frame.</p> <code>attrs_dims</code> <code>list[tuple] | None</code> <p>Output format of attributes.</p> <code>attrs_dtype</code> <code>list[torch.dtype]</code> <p>Data type of attributes if represented as HDF5 Dataset.</p> <code>attrs_is_data</code> <code>bool</code> <p>If True, attribute sets are represented by h5py.Dataset objects.</p> <code>attrs_is_dict</code> <code>bool</code> <p>If True, attribute sets are represented by dict or h5py.AttributeManager objects.</p> <code>cache_size</code> <code>nbytes</code> <p>Size of reserved cache.</p> <code>cached_attrs</code> <p>Number of cached samples from the frame.</p> <code>cached_bytes</code> <code>nbytes</code> <p>Size of used cache.</p> <code>cached_items</code> <p>Number of cached samples from the frame.</p> <code>data_groups</code> <p>HDF5 groups of the selected datasets in the frame.</p> <code>dims</code> <code>tuple | list[list[tuple]]</code> <p>Dataset dimensions (respecting collation and parallelization settings).</p> <code>frame</code> <code>list[H5PyFile]</code> <p>Lazy-loading list of HDF5 file instances.</p> <code>frame_tree</code> <code>list[list[str]]</code> <p>Fetch unfiltered frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p> <code>indexed_dims</code> <code>list[tuple]</code> <p>Dataset dimension of the indexed datasets.</p> <code>n_attrs</code> <p>Number of attribute sets in the frame.</p> <code>n_attrsets</code> <p>Number of attribute sets in the frame.</p> <code>n_datasets</code> <p>Number of datasets in the frame.</p> <code>n_files</code> <p>Number of files in the frame.</p> <code>n_samples</code> <p>Number of samples in the frame.</p> <code>raw_dims</code> <code>list[list[tuple]]</code> <p>Raw dataset dimensions (ignores collation and parallelization settings).</p> <code>sample_serial_size</code> <code>tuple[nbytes, nbytes]</code> <p>Size of a sample (tensor and metadata) in bytes (when serialized).</p> <code>serial_size</code> <code>tuple[nbytes, nbytes]</code> <p>Size of the entire dataset in bytes (when serialized).</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path | list[str] | list[Path],\n    groups: str | tuple[str, ...] = \"*\",\n    sort_key: Callable | None = None,\n    collate: bool | Callable = False,\n    parallel: bool = False,\n    pair: bool = False,\n    squash: bool = True,\n    dtype: torch.dtype = torch.float32,\n    attr_groups: str | tuple[str, ...] | None = None,\n    attrs_retrieval: str | Callable = \"auto\",\n    return_as: str | dict | None = \"tuple\",\n    preload: bool = False,\n    cache: int | float | str | bool | None = \"4G\",\n    attrs_cache: int | float | str | bool | None = \"64M\",\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        path: Path to a file, files, or data directory which contains the HDF5 files. May\n          contain wildcards `*` and `**`.\n        groups: Location(s) or filter pattern(s) for HDF5 groups containing HDF5 datasets\n          (allows wildcards `*` and `**`). Note, if wildcard expressions have too many\n          matches, the file tree search could take a long time, especially when there are\n          many HDF5Groups in the file. Thus, it is recommended to not use too generic\n          wildcard expressions. Alternatively, use `data_ignores` to tune the file tree search.\n        sort_key: Sorting key function for the frame tree of HDF5 groups.\n        collate: If True, multiple HDF5 files are collated into a single dataset. This is\n          equivalent to reading files in Fortran-like row-major ordering. If False,\n          __getitem__ tries to pull samples from each file. Can also be a function that\n          creates a custom ordering with signature\n          `func(hdf5_datasets: list[list]) -&gt; list[list]`.\n        parallel: If True, HDF5 datasets are read in parallel, resulting in multiple samples\n          being pulled, otherwise datasets are concatenated and sequentialized (note that\n          contiguous reading (with squash=True) only works, if dataset dimensions are\n          compatible).\n        pair: If True, datasets are paired and read pairwise-parallel\n          (automatically sets parallel=True).\n        squash: Squash dimension along first axis via summation (default behaviour\n          for sequential, i.e. parallel=False reads).\n        dtype: Data tensor type; default: torch.float32.\n        attr_groups: Locations or filter pattern(s) for HDF5 groups containing HDF5 attributes\n          (allows wildcards '*' and '**'). If empty or None, no attributes will be filtered.\n          Note: If wildcard expressions have too many matches, the file tree search could take a\n          long time, especially when there are many HDF5Groups in the file. Thus, it is\n          recommended to not use too generic wildcard expressions. Alternatively,\n          use `attrs_ignores` to tune the file tree search.\n        attrs_retrieval: A retrieval strategy for attributes; can be a function\n          one of ['auto' | None | Callable].\n        return_as: Return type of the dataset; one of ['tuple', 'dict', dict, None].\n          If a dictionary object is give, then the keys in the dictionary are used to\n          map items onto it.\n        preload: Preload and cache the dataset.\n        cache: If not None or False, memory is allocated for stochastic caching of the tensors.\n        attrs_cache: If not None or False, memory is allocated for stochastic caching\n          of ancillary data.\n        kwargs: Keyword arguments for `h5py.File`. See\n          `https://docs.h5py.org/en/stable/high/file.html#h5py.File`.\n    \"\"\"\n    self.frame_args = kwargs\n    self.frame_args.setdefault(\"libver\", \"latest\")\n    self.collate = collate\n    if callable(collate):\n        self.collate = True\n        self._collate_fn = staticmethod(collate)\n    else:\n        self._collate_fn = self._collate\n    self.pair = pair\n    self.parallel = parallel or self.pair\n    self.squash = squash\n    self.dtype = dtype\n    self.return_as = return_as\n    self.preload = preload\n\n    # build frame\n    self._frame: list[H5PyFile]\n    self._virt_frame: list[H5PyFile]\n    self._frame_tree: list[list[str]]\n    self.files = self.glob_path(path)\n    self.load_frame(**self.frame_args)\n\n    # locate HDF5 datasets in frame\n    self.frame_datasets: list[list[H5PyDataset]] = []\n    self.groups = (groups,) if not isinstance(groups, Iterable) else groups\n    self.sort_key = sort_key if sort_key is not None else self.default_sort_key\n    self.pin_data(self.groups)\n    self.make_index()\n\n    # ancillary data from HDF5 group attributes (or datasets) in frame\n    self.frame_attrs: list[list[H5PyAttrs | H5PyDataset]] = []\n    self.attr_groups_selected = attr_groups\n    self.attrs_retrieval = attrs_retrieval\n    if not isinstance(attr_groups, Iterable) and attr_groups is not None:\n        self.attr_groups_selected = (attr_groups,)\n    self.pin_attrs(self.attr_groups_selected)\n\n    # allocate memory for cache\n    self.cache: tuple[list[SharedArray] | None, list[SharedDictList] | None]\n    if isinstance(cache, bool) and cache:\n        cache = (\n            self.serial_size[0]\n            if self.serial_size[0] &lt; nbytes(\"4G\")\n            else nbytes(\"4G\")\n        )\n    if isinstance(attrs_cache, bool):\n        attrs_cache = self.serial_size[1]\n    self.init_cache(cache, attrs_cache)\n    if self.preload and self.cache[0] is not None:\n        for i in range(len(self)):\n            self[i]\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.attr_groups","title":"attr_groups  <code>property</code>","text":"<pre><code>attr_groups\n</code></pre> <p>HDF5 groups of the selected attribute sets in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.attrs_dims","title":"attrs_dims  <code>property</code>","text":"<pre><code>attrs_dims: list[tuple] | None\n</code></pre> <p>Output format of attributes.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.attrs_dtype","title":"attrs_dtype  <code>property</code>","text":"<pre><code>attrs_dtype: list[torch.dtype]\n</code></pre> <p>Data type of attributes if represented as HDF5 Dataset.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.attrs_is_data","title":"attrs_is_data  <code>property</code>","text":"<pre><code>attrs_is_data: bool\n</code></pre> <p>If True, attribute sets are represented by h5py.Dataset objects.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.attrs_is_dict","title":"attrs_is_dict  <code>property</code>","text":"<pre><code>attrs_is_dict: bool\n</code></pre> <p>If True, attribute sets are represented by dict or h5py.AttributeManager objects.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.cache_size","title":"cache_size  <code>property</code>","text":"<pre><code>cache_size: nbytes\n</code></pre> <p>Size of reserved cache.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.cached_attrs","title":"cached_attrs  <code>property</code>","text":"<pre><code>cached_attrs\n</code></pre> <p>Number of cached samples from the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.cached_bytes","title":"cached_bytes  <code>property</code>","text":"<pre><code>cached_bytes: nbytes\n</code></pre> <p>Size of used cache.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.cached_items","title":"cached_items  <code>property</code>","text":"<pre><code>cached_items\n</code></pre> <p>Number of cached samples from the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.data_groups","title":"data_groups  <code>property</code>","text":"<pre><code>data_groups\n</code></pre> <p>HDF5 groups of the selected datasets in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.dims","title":"dims  <code>property</code>","text":"<pre><code>dims: tuple | list[list[tuple]]\n</code></pre> <p>Dataset dimensions (respecting collation and parallelization settings).</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.frame","title":"frame  <code>property</code>","text":"<pre><code>frame: list[H5PyFile]\n</code></pre> <p>Lazy-loading list of HDF5 file instances.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.frame_tree","title":"frame_tree  <code>property</code>","text":"<pre><code>frame_tree: list[list[str]]\n</code></pre> <p>Fetch unfiltered frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.indexed_dims","title":"indexed_dims  <code>property</code>","text":"<pre><code>indexed_dims: list[tuple]\n</code></pre> <p>Dataset dimension of the indexed datasets.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.n_attrs","title":"n_attrs  <code>property</code>","text":"<pre><code>n_attrs\n</code></pre> <p>Number of attribute sets in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.n_attrsets","title":"n_attrsets  <code>property</code>","text":"<pre><code>n_attrsets\n</code></pre> <p>Number of attribute sets in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.n_datasets","title":"n_datasets  <code>property</code>","text":"<pre><code>n_datasets\n</code></pre> <p>Number of datasets in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.n_files","title":"n_files  <code>property</code>","text":"<pre><code>n_files\n</code></pre> <p>Number of files in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.n_samples","title":"n_samples  <code>property</code>","text":"<pre><code>n_samples\n</code></pre> <p>Number of samples in the frame.</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.raw_dims","title":"raw_dims  <code>property</code>","text":"<pre><code>raw_dims: list[list[tuple]]\n</code></pre> <p>Raw dataset dimensions (ignores collation and parallelization settings).</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.sample_serial_size","title":"sample_serial_size  <code>property</code>","text":"<pre><code>sample_serial_size: tuple[nbytes, nbytes]\n</code></pre> <p>Size of a sample (tensor and metadata) in bytes (when serialized).</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.serial_size","title":"serial_size  <code>property</code>","text":"<pre><code>serial_size: tuple[nbytes, nbytes]\n</code></pre> <p>Size of the entire dataset in bytes (when serialized).</p>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.attrs_getter","title":"attrs_getter","text":"<pre><code>attrs_getter(index: int) -&gt; list[dict | torch.Tensor]\n</code></pre> <p>Retrieve ancillary data item at <code>index</code>.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def attrs_getter(self, index: int) -&gt; list[dict | torch.Tensor]:\n    \"\"\"Retrieve ancillary data item at `index`.\"\"\"\n    if self.n_attrs &lt; 1:\n        return []\n    if self.attrs_retrieval == \"auto\":\n        if self.n_datasets == self.n_attrsets:\n            # data &lt;-&gt; attribute set mapping: 1-to-1\n            indexed_attrs = self._make_attrs_index(self.frame_attrs)\n        elif (\n            self.n_datasets &lt; self.n_attrsets and self.n_attrsets == self.n_samples\n        ):\n            # data &lt;-&gt; attribute set mapping: 1-to-all\n            indexed_attrs = self._make_attrs_index(self.frame_attrs)\n        elif self.n_datasets &lt; self.n_attrsets:\n            # data &lt;-&gt; attributes mapping: 1-to-many\n            indexed_attrs = self._make_many_attrs_index(self.frame_attrs)\n        elif self.n_datasets &gt; self.n_attrsets and self.n_attrsets == 1:\n            # data &lt;-&gt; attribute set mapping: all-to-1\n            if not hasattr(self, \"indexed_attrs\"):\n                self.indexed_attrs = [self.frame_attrs[0] * self.n_samples]\n            indexed_attrs = self.indexed_attrs\n        else:\n            # data &lt;-&gt; attribute set mapping: many-to-1\n            warnings.warn(\n                \"Automatic attribute retrieval not possible!\\n\"\n                f\"Found {self.n_datasets} datasets with {self.n_samples} samples in total,\\n\"\n                \"but only {self.n_attrsets} attribute sets.\\n\"\n                \"Try passing a attribute retrieval function\\n\"\n                \"   fn(index, frame_attrs) -&gt; list[H5PyAttrs | H5PyDataset]\\n\"\n                \"that selects the appropriate attributes for a given index.\"\n            )\n            indexed_attrs = []\n        if self.attrs_is_data:\n            return [\n                torch.from_numpy(a[index]).squeeze() for a in indexed_attrs if a\n            ]\n        else:\n            return [dict(a[index]) for a in indexed_attrs if a]\n    elif callable(self.attrs_retrieval):\n        return self.attrs_retrieval(index, self.frame_attrs)\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.cache_attrs","title":"cache_attrs","text":"<pre><code>cache_attrs(\n    index: int,\n    attrs: torch.Tensor | dict | list[torch.Tensor | dict],\n    output_index: int | None = None,\n    overwrite: bool = False,\n)\n</code></pre> <p>Cache attrs (overwrite by default off).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index where to cache the attribute.</p> required <code>attrs</code> <code>torch.Tensor | dict | list[torch.Tensor | dict]</code> <p>Attribute(s) to be cached.</p> required <code>output_index</code> <code>int | None</code> <p>Index of the list of caches, if output contains multiple attributes.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True and there already is a cached element at the given index, that attribute is overwritten.</p> <code>False</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def cache_attrs(\n    self,\n    index: int,\n    attrs: torch.Tensor | dict | list[torch.Tensor | dict],\n    output_index: int | None = None,\n    overwrite: bool = False,\n):\n    \"\"\"Cache attrs (overwrite by default off).\n\n    Args:\n        index: Index where to cache the attribute.\n        attrs: Attribute(s) to be cached.\n        output_index: Index of the list of caches, if output contains multiple attributes.\n        overwrite: If True and there already is a cached element at the given index,\n          that attribute is overwritten.\n    \"\"\"\n    if not hasattr(self, \"cache\") or self.cache[1] is None:\n        return\n    if output_index is None:\n        output_index = 0\n    if isinstance(attrs, torch.Tensor | dict):\n        previous = self.cache[1][output_index][index]\n        if previous is None or overwrite:\n            self.cache[1][output_index][index] = attrs\n    else:\n        for i, t in enumerate(attrs):\n            previous = self.cache[1][i][index]\n            if previous is None or overwrite:\n                self.cache[1][i][index] = t\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.cache_item","title":"cache_item","text":"<pre><code>cache_item(\n    index: int,\n    items: torch.Tensor | list[torch.Tensor],\n    output_index: int | None = None,\n    overwrite: bool = False,\n)\n</code></pre> <p>Cache item (overwrite by default off).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index where to cache the attribute.</p> required <code>items</code> <code>torch.Tensor | list[torch.Tensor]</code> <p>Item(s) to be cached.</p> required <code>output_index</code> <code>int | None</code> <p>Index of the list of caches, if output contains multiple items.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True and there already is a cached element at the given index, that item is overwritten.</p> <code>False</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def cache_item(\n    self,\n    index: int,\n    items: torch.Tensor | list[torch.Tensor],\n    output_index: int | None = None,\n    overwrite: bool = False,\n):\n    \"\"\"Cache item (overwrite by default off).\n\n    Args:\n        index: Index where to cache the attribute.\n        items: Item(s) to be cached.\n        output_index: Index of the list of caches, if output contains multiple items.\n        overwrite: If True and there already is a cached element at the given index,\n          that item is overwritten.\n    \"\"\"\n    if not hasattr(self, \"cache\") or self.cache[0] is None:\n        return\n    if output_index is None:\n        output_index = 0\n    if isinstance(items, torch.Tensor):\n        previous = self.cache[0][output_index][index]\n        if previous is None or overwrite:\n            self.cache[0][output_index][index] = items\n    else:\n        for i, t in enumerate(items):\n            previous = self.cache[0][i][index]\n            if previous is None or overwrite:\n                self.cache[0][i][index] = t\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Exit the frame, i.e. close all open files and purge the cache.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def close(self):\n    \"\"\"Exit the frame, i.e. close all open files and purge the cache.\"\"\"\n    self.purge_cache(reset=False)\n    for f in self.frame:\n        f.close()\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.default_sort_key","title":"default_sort_key  <code>staticmethod</code>","text":"<pre><code>default_sort_key(x: Any) -&gt; int\n</code></pre> <p>Default sort key for HDF5 groups in the frame tree.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>@staticmethod\ndef default_sort_key(x: Any) -&gt; int:\n    \"\"\"Default sort key for HDF5 groups in the frame tree.\"\"\"\n    return int(x.split(\"/\")[-1]) if x.split(\"/\")[-1].isdigit() else 0\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.filter_tree","title":"filter_tree","text":"<pre><code>filter_tree(\n    groups: tuple[str, ...],\n    omit_keys: tuple[str, ...] | None = None,\n    strip_root: bool = True,\n) -&gt; list[list[str]]\n</code></pre> <p>Filter frame structure (HDF5 Groups) of each frame (HDF5 File instance).</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>tuple[str, ...]</code> <p>Names of groups to check against the entire frame structure.</p> required <code>omit_keys</code> <code>tuple[str, ...] | None</code> <p>Keywords to ignore while filtering (for speed-up when <code>frame_tree</code> is massive).</p> <code>None</code> <code>strip_root</code> <code>bool</code> <p>If True, the groups starting with <code>/</code> are stripped.</p> <code>True</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def filter_tree(\n    self,\n    groups: tuple[str, ...],\n    omit_keys: tuple[str, ...] | None = None,\n    strip_root: bool = True,\n) -&gt; list[list[str]]:\n    \"\"\"Filter frame structure (HDF5 Groups) of each frame (HDF5 File instance).\n\n    Args:\n        groups: Names of groups to check against the entire frame structure.\n        omit_keys: Keywords to ignore while filtering\n          (for speed-up when `frame_tree` is massive).\n        strip_root: If True, the groups starting with `/` are stripped.\n    \"\"\"\n    filtered_groups: tuple[str, ...] = ()\n    for g in groups:\n        g = g[1:] if g.startswith(\"/\") else g\n        for i, t in enumerate(self.frame_tree):\n            matches = fnmatch.filter(t, g)\n            if omit_keys:\n                matches = [m for m in matches if not any(k in m for k in omit_keys)]\n            filtered_groups += tuple(matches)\n    return tuple(dict.fromkeys(filtered_groups).keys())\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.get_cached_attrs","title":"get_cached_attrs","text":"<pre><code>get_cached_attrs(\n    index: int,\n) -&gt; list[torch.Tensor | dict | None]\n</code></pre> <p>Fetch attrs if in cache.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def get_cached_attrs(self, index: int) -&gt; list[torch.Tensor | dict | None]:\n    \"\"\"Fetch attrs if in cache.\"\"\"\n    if hasattr(self, \"cache\") and self.cache[1] is not None:\n        return [c[index] for c in self.cache[1]]\n    return None\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.get_cached_item","title":"get_cached_item","text":"<pre><code>get_cached_item(index: int) -&gt; list[torch.Tensor | None]\n</code></pre> <p>Fetch item(s) if in cache.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def get_cached_item(self, index: int) -&gt; list[torch.Tensor | None]:\n    \"\"\"Fetch item(s) if in cache.\"\"\"\n    if hasattr(self, \"cache\") and self.cache[0] is not None:\n        return [c[index] for c in self.cache[0]]\n    return [None for _ in self.indexed_dims]\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.glob_path","title":"glob_path  <code>staticmethod</code>","text":"<pre><code>glob_path(\n    path: str | Path | list[str] | list[Path],\n) -&gt; list[Path]\n</code></pre> <p>Glob path recursively for HDF5 files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | list[str] | list[Path]</code> <p>Filename, path or list, can contain wildcards <code>*</code> or <code>**</code>.</p> required Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>@staticmethod\ndef glob_path(path: str | Path | list[str] | list[Path]) -&gt; list[Path]:\n    \"\"\"Glob path recursively for HDF5 files.\n\n    Args:\n        path: Filename, path or list, can contain wildcards `*` or `**`.\n    \"\"\"\n    files: list[Path] = []\n    root, file_key = HDF5Dataset._split_glob(path)\n    for p, k in zip(root, file_key):\n        if k is None:\n            files.append(Path(p))\n        else:\n            path_files = [\n                f\n                for f in Path(p).rglob(k)\n                if f.is_file() and f.suffix in H5_FILE_EXTENSIONS\n            ]\n            files += sorted(path_files)\n    files = [p for p in files if p.exists()]\n    return files\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.info","title":"info","text":"<pre><code>info(\n    print_: bool = True,\n    show_data_info: bool = True,\n    show_attrs_info: bool = True,\n) -&gt; str\n</code></pre> <p>Print summary information about the frame.</p> <p>Parameters:</p> Name Type Description Default <code>print_</code> <code>bool</code> <p>Print to stdout, otherwise only return summary string.</p> <code>True</code> <code>show_data_info</code> <code>bool</code> <p>Include detailed info about the datasets in the frame.</p> <code>True</code> <code>show_attrs_info</code> <code>bool</code> <p>Include detailed info about the attribute sets in the frame.</p> <code>True</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def info(\n    self,\n    print_: bool = True,\n    show_data_info: bool = True,\n    show_attrs_info: bool = True,\n) -&gt; str:\n    \"\"\"Print summary information about the frame.\n\n    Args:\n        print_: Print to stdout, otherwise only return summary string.\n        show_data_info: Include detailed info about the datasets in the frame.\n        show_attrs_info: Include detailed info about the attribute sets in the frame.\n    \"\"\"\n    dims = self.dims if isinstance(self.dims, tuple) else self.indexed_dims\n    summary = str(self)\n    summary += \"\\n\" + \"-\" * 50 + \"\\n\"\n    summary += f\"Serial size:             \\t ({self.serial_size[0].as_str()}, {self.serial_size[1].as_str()})\\n\"\n    summary += f\"Sample size:             \\t ({self.sample_serial_size[0].as_str()}, {self.sample_serial_size[1].as_str()})\\n\"\n    summary += f\"Cache size:              \\t {self.cached_bytes.as_str()} / {self.cache_size.as_str()}\\n\"\n    summary += f\"Number of files:         \\t {self.n_files}\\n\"\n    summary += f\"Selected groups:         \\t {self.groups}\\n\"\n    summary += f\"Number of datasets:      \\t {self.n_datasets}\\n\"\n    summary += f\"Number of samples:       \\t {len(self)}\\n\"\n    summary += f\"Selected attrs:          \\t {self.attr_groups_selected}\\n\"\n    summary += f\"Number of attribute sets:\\t {self.n_attrsets}\\n\"\n    summary += f\"Number of attrs:         \\t {self.n_attrs}\\n\"\n    if show_data_info:\n        data_info = [\n            [f\"H5PyDataset={di}, shape={si}\" for di, si in zip(d, s)]\n            for d, s in zip(self.data_groups, self.raw_dims)\n        ]\n        summary += f\"Data {dims}:\\n{pprint.pformat(data_info)}\\n\"\n    if show_attrs_info:\n        attrs_info = self.frame_attrs\n        summary += f\"Attrs:\\n{pprint.pformat(attrs_info)}\\n\"\n    if print_:\n        print(summary)\n    return summary\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.init_cache","title":"init_cache","text":"<pre><code>init_cache(\n    cache: int | float | str | None = \"4G\",\n    attrs_cache: int | float | str | None = \"64M\",\n)\n</code></pre> <p>Initialize the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>int | float | str | None</code> <p>Cache size for data items.</p> <code>'4G'</code> <code>attrs_cache</code> <code>int | float | str | None</code> <p>Cache size for attributes.</p> <code>'64M'</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def init_cache(\n    self,\n    cache: int | float | str | None = \"4G\",\n    attrs_cache: int | float | str | None = \"64M\",\n):\n    \"\"\"Initialize the cache.\n\n    Args:\n        cache: Cache size for data items.\n        attrs_cache: Cache size for attributes.\n    \"\"\"\n    if cache and cache is not None:\n        output_length = max(len(self.indexed_dims), 1)\n        cache = nbytes(cache) / output_length\n        self.cache = (\n            [\n                SharedArray(shape=dims, size=cache.as_bstr(), dtype=self.dtype)\n                for dims in self.indexed_dims\n            ],\n        )\n    else:\n        self.cache = (None,)\n    if self.n_attrs &gt; 0 and attrs_cache is not None:\n        output_length = max(len(self.indexed_dims), 1)\n        attrs_cache = nbytes(attrs_cache) / output_length\n        if self.attrs_is_data:\n            self.cache += (\n                [\n                    SharedArray(shape=dims, size=attrs_cache.as_bstr(), dtype=dtype)\n                    for dims, dtype in zip(self.attrs_dims, self.attrs_dtype)\n                ],\n            )\n        elif self.attrs_is_dict:\n            self.cache += (\n                [\n                    SharedDictList(\n                        n=dims[0],\n                        size=attrs_cache.as_bstr(),\n                        slot_size=self.sample_serial_size[1].as_bstr(),\n                        descr=f\"shm_list_{i}\",\n                    )\n                    for i, dims in enumerate(self.attrs_dims)\n                ],\n            )\n    else:\n        self.cache += (None,)\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.is_aligned","title":"is_aligned  <code>staticmethod</code>","text":"<pre><code>is_aligned(dims: tuple | list[list[tuple]]) -&gt; bool\n</code></pre> <p>Test if the model is aligned, i.e. same sample size for parallel reads.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>tuple | list[list[tuple]]</code> <p>Dimensions to be tested for alignment.</p> required Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>@staticmethod\ndef is_aligned(dims: tuple | list[list[tuple]]) -&gt; bool:\n    \"\"\"Test if the model is aligned, i.e. same sample size for parallel reads.\n\n    Args:\n        dims: Dimensions to be tested for alignment.\n    \"\"\"\n    if isinstance(dims, tuple):\n        return True\n    sizes = [[di[0] for di in d] for d in dims]\n    collated_sizes = HDF5Dataset._collate(sizes)\n    return all([len(set(cs)) == 1 for cs in collated_sizes])\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.load_frame","title":"load_frame","text":"<pre><code>load_frame(**kwargs) -&gt; list[H5PyFile] | None\n</code></pre> <p>Load HDF5 file instances.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments for <code>h5py.File</code>.</p> <code>{}</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def load_frame(self, **kwargs) -&gt; list[H5PyFile] | None:\n    \"\"\"Load HDF5 file instances.\n\n    Args:\n        kwargs: Keyword arguments for `h5py.File`.\n    \"\"\"\n    self._frame = [H5PyFile(f, \"r\", **kwargs) for f in self.files]\n    self._virt_frame = []\n    return self._frame\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.load_frame_tree","title":"load_frame_tree","text":"<pre><code>load_frame_tree(\n    sort_key: Callable | None = None,\n) -&gt; list[list[str]]\n</code></pre> <p>Load tree of HDF5 group for each frame (HDF5 file instance).</p> <p>Parameters:</p> Name Type Description Default <code>sort_key</code> <code>Callable | None</code> <p>Sorting key function for the list of HDF5 groups.</p> <code>None</code> Note <p>This method walks through the entire HDF5 group tree and could take some time.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def load_frame_tree(self, sort_key: Callable | None = None) -&gt; list[list[str]]:\n    \"\"\"Load tree of HDF5 group for each frame (HDF5 file instance).\n\n    Args:\n        sort_key: Sorting key function for the list of HDF5 groups.\n\n    Note:\n        This method walks through the entire HDF5 group tree and could take some time.\n    \"\"\"\n    tree: list[list[str]] = []\n    sort_key = sort_key if sort_key else self.sort_key\n    for i, f in enumerate(self.frame):\n        tree.append([])\n        f.visit(tree[i].append)\n    self._frame_tree = [sorted(t, key=sort_key) for t in tree]\n    return self._frame_tree\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.make_index","title":"make_index","text":"<pre><code>make_index() -&gt; list[H5PyDataset]\n</code></pre> <p>Index frame datasets, apply collation and parallelization settings.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def make_index(self) -&gt; list[H5PyDataset]:\n    \"\"\"Index frame datasets, apply collation and parallelization settings.\"\"\"\n    if not hasattr(self, \"indexed_datasets\"):\n        dims = self.dims\n        datasets = self.frame_datasets\n        if not datasets:\n            self.indexed_datasets = datasets\n            return None\n        if self.collate:\n            datasets = self._collate_fn(datasets)\n        if self.pair:\n            datasets = self._pair(datasets)\n        if self.parallel or self.pair:\n            datasets = self._parallelize(datasets)\n        elif self.squash and isinstance(dims, tuple):\n            datasets = self._contiguate(datasets)\n        dims = [dims] if isinstance(dims, tuple) else dims\n        datasets = [\n            self._stitch_datasets(ds, layout=sh) for ds, sh in zip(datasets, dims)\n        ]\n        self.indexed_datasets = datasets\n    return self.indexed_datasets\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.pin_attrs","title":"pin_attrs","text":"<pre><code>pin_attrs(\n    attr_groups: tuple[str, ...] | None = None,\n) -&gt; list[H5PyAttrs | H5PyDataset]\n</code></pre> <p>Find and pin attribute sets (metadata) in the frame.</p> <p>Parameters:</p> Name Type Description Default <code>attr_groups</code> <code>tuple[str, ...] | None</code> <p>Location(s) or filter pattern(s) for HDF5 groups containing HDF5 attributes (allows wildcards '' and '*'). Note, if <code>None</code>, no attributes are pinned.</p> <code>None</code> <p>Note, if no attribute sets are found, separate (other than group-selected) datasets are pinned.</p> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def pin_attrs(\n    self, attr_groups: tuple[str, ...] | None = None\n) -&gt; list[H5PyAttrs | H5PyDataset]:\n    \"\"\"Find and pin attribute sets (metadata) in the frame.\n\n    Args:\n        attr_groups: Location(s) or filter pattern(s) for HDF5 groups\n          containing HDF5 attributes (allows wildcards '*' and '**'). Note,\n          if `None`, no attributes are pinned.\n\n    Note, if no attribute sets are found, separate (other than group-selected)\n    datasets are pinned.\n    \"\"\"\n    attrs: list[list[H5PyAttrs | H5PyDataset]] = []\n    if attr_groups is None or not attr_groups:\n        self.frame_attrs = []\n    if attr_groups and any(\"*\" in g for g in attr_groups):\n        attr_groups = self.filter_tree(attr_groups)\n    if attr_groups:\n        for i, f in enumerate(self.frame):\n            # look for HDF5 Attributes\n            attrs_list = [\n                f[g].attrs for g in attr_groups if g in f and f[g].attrs.keys()\n            ]\n            # if no HDF5 Attributes found, look for other HDF5 Datasets\n            if not attrs_list:\n                attrs_list = [\n                    f[g]\n                    for g in attr_groups\n                    if g in f and isinstance(f[g], H5PyDataset)\n                ]\n                # check if attrs are just data and filter the list\n                if len(self.frame_datasets) &gt; i:\n                    data_list = self.frame_datasets[i]\n                    for d in data_list:\n                        for i, a in enumerate(attrs_list):\n                            attr_is_data = (\n                                d.shape == a.shape\n                                and d.size == a.size\n                                and d.nbytes == a.nbytes\n                                and d.name == a.name\n                            )\n                            if attr_is_data:\n                                attrs_list.pop(i)\n            if attrs_list:\n                attrs.append(attrs_list)\n    self.frame_attrs = attrs\n    return self.frame_attrs\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.pin_data","title":"pin_data","text":"<pre><code>pin_data(\n    groups: tuple[str, ...] | None = (\"*\",),\n) -&gt; list[H5PyDataset]\n</code></pre> <p>Find and pin datasets in the frame.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>tuple[str, ...] | None</code> <p>Location(s) or filter pattern(s) for HDF5 groups containing HDF5 datasets (allows wildcards '' and '*'). Note, if <code>None</code>, the entire HDF5 tree will be checked, which could take a long time for HDF5 files that contain many groups.</p> <code>('*',)</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def pin_data(self, groups: tuple[str, ...] | None = (\"*\",)) -&gt; list[H5PyDataset]:\n    \"\"\"Find and pin datasets in the frame.\n\n    Args:\n        groups: Location(s) or filter pattern(s) for HDF5 groups containing\n          HDF5 datasets (allows wildcards '*' and '**'). Note, if `None`, the\n          entire HDF5 tree will be checked, which could take a long time for\n          HDF5 files that contain many groups.\n    \"\"\"\n    datasets: list[list[H5PyDataset]] = []\n    if groups is None:\n        groups = self.groups or (\"*\",)\n    if any(\"*\" in g for g in groups):\n        groups = self.filter_tree(groups, omit_keys=self.data_ignores)\n    for f in self.frame:\n        datasets.append(\n            [f[g] for g in groups if g in f and isinstance(f[g], H5PyDataset)]\n        )\n    self.frame_datasets = datasets\n    return self.frame_datasets\n</code></pre>"},{"location":"reference/chuchichaestli/data/dataset/#chuchichaestli.data.dataset.HDF5Dataset.purge_cache","title":"purge_cache","text":"<pre><code>purge_cache(reset: bool = False)\n</code></pre> <p>Purge the cache.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>If True, the cache instance attribute is reinitialized.</p> <code>False</code> Source code in <code>src/chuchichaestli/data/dataset.py</code> <pre><code>def purge_cache(self, reset: bool = False):\n    \"\"\"Purge the cache.\n\n    Args:\n        reset: If True, the cache instance attribute is reinitialized.\n    \"\"\"\n    cache_sizes = [None, None]\n    for i, c in enumerate(self.cache):\n        if c is not None:\n            cache_sizes[i] = 0\n            for ci in c:\n                cache_sizes[i] += ci.cache_size\n                ci.clear_allocation()\n    del self.cache\n    if reset:\n        self.init_cache(*cache_sizes)\n</code></pre>"},{"location":"reference/chuchichaestli/debug/","title":"chuchichaestli.debug","text":""},{"location":"reference/chuchichaestli/debug/#chuchichaestli.debug","title":"chuchichaestli.debug","text":"<p>Debugging utilities module of chuchichaestli.</p> <p>Modules:</p> Name Description <code>memory_usage</code> <p>Memory debugging utils.</p> <code>visitor</code> <p>Visitor pattern implementation for PyTorch models.</p> <p>Functions:</p> Name Description <code>as_bytes</code> <p>Convert bytes strings to integers.</p> <code>cli_pbar</code> <p>Construct/print a progressbar of given relative length, optionally with labels.</p>"},{"location":"reference/chuchichaestli/debug/#chuchichaestli.debug.as_bytes","title":"as_bytes","text":"<pre><code>as_bytes(byte_str: str = 'MB') -&gt; int\n</code></pre> <p>Convert bytes strings to integers.</p> Source code in <code>src/chuchichaestli/debug/__init__.py</code> <pre><code>def as_bytes(byte_str: str = \"MB\") -&gt; int:\n    \"\"\"Convert bytes strings to integers.\"\"\"\n    match byte_str.lower():\n        case \"kib\":\n            return 1 &lt;&lt; 10\n        case \"mib\":\n            return 1 &lt;&lt; 20\n        case \"gib\":\n            return 1 &lt;&lt; 30\n        case \"tib\":\n            return 1 &lt;&lt; 40\n        case \"kb\":\n            return 1_000\n        case \"mb\":\n            return 1_000_000\n        case \"gb\":\n            return 1_000_000_000\n        case \"tb\":\n            return 1_000_000_000_000\n        case _:\n            return 1\n</code></pre>"},{"location":"reference/chuchichaestli/debug/#chuchichaestli.debug.cli_pbar","title":"cli_pbar","text":"<pre><code>cli_pbar(\n    r_fill: float,\n    prefix: str | list = \"\",\n    postfix: str | list = \"\",\n    bar_length: int = 60,\n    fill_symbol: str = \"#\",\n    empty_symbol: str = \"-\",\n    float_fmt: str = \"{:.2f}\",\n    int_fmt: str = \"{:4d}\",\n) -&gt; str\n</code></pre> <p>Construct/print a progressbar of given relative length, optionally with labels.</p> <p>Parameters:</p> Name Type Description Default <code>r_fill</code> <code>float</code> <p>rate of progress, a number between 0 and 1</p> required <code>prefix</code> <code>str | list</code> <p>label before the progressbar</p> <code>''</code> <code>postfix</code> <code>str | list</code> <p>label after the progressbar</p> <code>''</code> <code>bar_length</code> <code>int</code> <p>maximum length of the progressbar; automatically determined if negative</p> <code>60</code> <code>fill_symbol</code> <code>str</code> <p>symbol indicating the progressbar's filled status</p> <code>'#'</code> <code>empty_symbol</code> <code>str</code> <p>symbol indicating the progressbar's empty status</p> <code>'-'</code> <code>float_fmt</code> <code>str</code> <p>string format for floats in prefix</p> <code>'{:.2f}'</code> <code>int_fmt</code> <code>str</code> <p>string format for ints in prefix</p> <code>'{:4d}'</code> Source code in <code>src/chuchichaestli/debug/__init__.py</code> <pre><code>def cli_pbar(\n    r_fill: float,\n    prefix: str | list = \"\",\n    postfix: str | list = \"\",\n    bar_length: int = 60,\n    fill_symbol: str = \"#\",\n    empty_symbol: str = \"-\",\n    float_fmt: str = \"{:.2f}\",\n    int_fmt: str = \"{:4d}\",\n) -&gt; str:\n    \"\"\"Construct/print a progressbar of given relative length, optionally with labels.\n\n    Args:\n        r_fill (float): rate of progress, a number between 0 and 1\n        prefix (str | list): label before the progressbar\n        postfix (str | list): label after the progressbar\n        bar_length (int): maximum length of the progressbar; automatically determined if negative\n        fill_symbol (str): symbol indicating the progressbar's filled status\n        empty_symbol (str): symbol indicating the progressbar's empty status\n        float_fmt (str): string format for floats in prefix\n        int_fmt (str): string format for ints in prefix\n    \"\"\"\n    if isinstance(prefix, list | tuple):\n        for i, p in enumerate(prefix):\n            if isinstance(p, float):\n                prefix[i] = float_fmt.format(p)\n            elif isinstance(p, int):\n                prefix[i] = int_fmt.format(p)\n            if not isinstance(prefix[i], str):\n                prefix[i] = f\"{prefix[i]}\"\n        prefix = \" \".join(prefix)\n    if isinstance(postfix, list | tuple):\n        for i, p in enumerate(postfix):\n            if isinstance(p, float):\n                postfix[i] = float_fmt.format(p)\n            elif isinstance(p, int):\n                postfix[i] = int_fmt.format(p)\n            if not isinstance(postfix[i], str):\n                postfix[i] = f\"{postfix[i]}\"\n        postfix = \" \".join(postfix)\n    bar = fill_symbol * int(r_fill * bar_length) + empty_symbol * int(\n        (1 - r_fill) * bar_length\n    )\n    if bar_length &gt; 0:\n        line = f\"{prefix} [{bar}] {postfix}\"\n    else:\n        line = f\"{prefix}\\t{postfix}\"\n    return line\n</code></pre>"},{"location":"reference/chuchichaestli/debug/memory_usage/","title":"chuchichaestli.debug.memory_usage","text":""},{"location":"reference/chuchichaestli/debug/memory_usage/#chuchichaestli.debug.memory_usage","title":"chuchichaestli.debug.memory_usage","text":"<p>Memory debugging utils.</p> <p>Classes:</p> Name Description <code>CudaMemoryStatsVisitor</code> <p>Visitor that collects memory statistics on CUDA GPUs.</p> <code>MPSMemoryAllocationVisitor</code> <p>Visitor that collects memory statistics on Apple systems.</p>"},{"location":"reference/chuchichaestli/debug/memory_usage/#chuchichaestli.debug.memory_usage.CudaMemoryStatsVisitor","title":"CudaMemoryStatsVisitor","text":"<pre><code>CudaMemoryStatsVisitor(**kwargs)\n</code></pre> <p>               Bases: <code>Visitor</code></p> <p>Visitor that collects memory statistics on CUDA GPUs.</p> <p>Initialize the visitor.</p> <p>Methods:</p> Name Description <code>has_cuda</code> <p>Check if CUDA is built and available.</p> Source code in <code>src/chuchichaestli/debug/memory_usage.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the visitor.\"\"\"\n    if \"hook_default\" in kwargs:\n        hook_default = kwargs.pop(\"hook_default\")\n    else:\n        hook_default = Hook(self._hook, HookDirection.Forward)\n    super().__init__(hook_default=hook_default, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/debug/memory_usage/#chuchichaestli.debug.memory_usage.CudaMemoryStatsVisitor.has_cuda","title":"has_cuda  <code>staticmethod</code>","text":"<pre><code>has_cuda(verbose: bool = False) -&gt; bool\n</code></pre> <p>Check if CUDA is built and available.</p> Source code in <code>src/chuchichaestli/debug/memory_usage.py</code> <pre><code>@staticmethod\ndef has_cuda(verbose: bool = False) -&gt; bool:\n    \"\"\"Check if CUDA is built and available.\"\"\"\n    is_avail = torch.cuda.is_available()\n    is_built = torch.backends.cuda.is_built()\n    is_cuda = is_avail and is_built\n    if verbose:\n        if not is_built:\n            if not is_avail:\n                print(\n                    \"CUDA not available most likely because you do not have a CUDA-enabled \"\n                    \"device on this machine.\"\n                )\n            else:\n                print(\n                    \"CUDA not available because the current PyTorch install was not built with \"\n                    \"CUDA enabled.\"\n                )\n    return is_cuda\n</code></pre>"},{"location":"reference/chuchichaestli/debug/memory_usage/#chuchichaestli.debug.memory_usage.MPSMemoryAllocationVisitor","title":"MPSMemoryAllocationVisitor","text":"<pre><code>MPSMemoryAllocationVisitor(**kwargs)\n</code></pre> <p>               Bases: <code>Visitor</code></p> <p>Visitor that collects memory statistics on Apple systems.</p> <p>Initialize the visitor.</p> <p>Methods:</p> Name Description <code>has_mps</code> <p>Check if MPS is built and available.</p> Source code in <code>src/chuchichaestli/debug/memory_usage.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the visitor.\"\"\"\n    if \"hook_default\" in kwargs:\n        hook_default = kwargs.pop(\"hook_default\")\n    else:\n        hook_default = Hook(self._hook, HookDirection.Forward)\n    super().__init__(hook_default=hook_default, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/debug/memory_usage/#chuchichaestli.debug.memory_usage.MPSMemoryAllocationVisitor.has_mps","title":"has_mps  <code>staticmethod</code>","text":"<pre><code>has_mps(verbose: bool = False) -&gt; bool\n</code></pre> <p>Check if MPS is built and available.</p> Source code in <code>src/chuchichaestli/debug/memory_usage.py</code> <pre><code>@staticmethod\ndef has_mps(verbose: bool = False) -&gt; bool:\n    \"\"\"Check if MPS is built and available.\"\"\"\n    is_avail = torch.backends.mps.is_available()\n    is_built = torch.backends.mps.is_built()\n    is_mps = is_avail and is_built\n    if verbose:\n        if not is_avail:\n            if not is_built:\n                print(\n                    \"MPS not available because the current PyTorch install was not built with \"\n                    \"MPS enabled.\"\n                )\n            else:\n                print(\n                    \"MPS not available because the current MacOS version is not 12.3+ and/or \"\n                    \"you do not have an MPS-enabled device on this machine.\"\n                )\n    return is_mps\n</code></pre>"},{"location":"reference/chuchichaestli/debug/visitor/","title":"chuchichaestli.debug.visitor","text":""},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor","title":"chuchichaestli.debug.visitor","text":"<p>Visitor pattern implementation for PyTorch models.</p> <p>Classes:</p> Name Description <code>Hook</code> <p>Represents a hook (function to call and direction to attach the hook).</p> <code>HookDirection</code> <p>Enum that represents the direction of a hook.</p> <code>Visitor</code> <p>Generic implementation of a Visitor.</p>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.Hook","title":"Hook  <code>dataclass</code>","text":"<pre><code>Hook(\n    fn: Callable[[nn.Module, Any], Any],\n    direction: HookDirection,\n)\n</code></pre> <p>Represents a hook (function to call and direction to attach the hook).</p>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.HookDirection","title":"HookDirection","text":"<p>               Bases: <code>Enum</code></p> <p>Enum that represents the direction of a hook.</p>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.Visitor","title":"Visitor","text":"<pre><code>Visitor(\n    hook_default: Hook | None = None,\n    hook_map: dict[object, Hook] = {},\n    max_depth: int | None = None,\n    _memory_stats: list = [],\n)\n</code></pre> <p>Generic implementation of a Visitor.</p> <p>Initialize the Visitor.</p> <p>Parameters:</p> Name Type Description Default <code>hook_default</code> <code>Hook | None</code> <p>The default hook to use.</p> <code>None</code> <code>hook_map</code> <code>dict[object, Hook]</code> <p>A mapping from layer types to hooks.</p> <code>{}</code> <code>max_depth</code> <code>int | None</code> <p>The maximum depth to visit.</p> <code>None</code> <code>_memory_stats</code> <code>list</code> <p>Precached memory statistics history.</p> <code>[]</code> <p>Methods:</p> Name Description <code>report</code> <p>Report the memory statistics for the visited module(s).</p> <code>unlink</code> <p>Unlink hooks and maps.</p> <code>visit</code> <p>Function called when visiting an object.</p> <p>Attributes:</p> Name Type Description <code>memory_stats</code> <p>Return the collected memory statistics.</p> Source code in <code>src/chuchichaestli/debug/visitor.py</code> <pre><code>def __init__(\n    self,\n    hook_default: Hook | None = None,\n    hook_map: dict[object, Hook] = {},\n    max_depth: int | None = None,\n    _memory_stats: list = [],\n):\n    \"\"\"Initialize the Visitor.\n\n    Args:\n        hook_default: The default hook to use.\n        hook_map: A mapping from layer types to hooks.\n        max_depth: The maximum depth to visit.\n        _memory_stats: Precached memory statistics history.\n    \"\"\"\n    self.hook_default = hook_default\n    self.hook_map = hook_map\n    self.max_depth = max_depth\n    self._memory_stats = _memory_stats\n</code></pre>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.Visitor.memory_stats","title":"memory_stats  <code>property</code>","text":"<pre><code>memory_stats\n</code></pre> <p>Return the collected memory statistics.</p>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.Visitor.report","title":"report","text":"<pre><code>report(\n    unit: str = \"MB\",\n    with_bar: bool = True,\n    bar_length: int = 60,\n    verbose: bool = True,\n) -&gt; list[str] | None\n</code></pre> <p>Report the memory statistics for the visited module(s).</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>str</code> <p>The byte unit; one in [KB, MB, GB, TB, KiB, MiB, GiB, TiB].</p> <code>'MB'</code> <code>with_bar</code> <code>bool</code> <p>Add simple horizontal percentage bars.</p> <code>True</code> <code>bar_length</code> <code>int</code> <p>The length of the percentage bars in number of characters.</p> <code>60</code> <code>verbose</code> <code>bool</code> <p>Directly print lines to stdout.</p> <code>True</code> Source code in <code>src/chuchichaestli/debug/visitor.py</code> <pre><code>def report(\n    self,\n    unit: str = \"MB\",\n    with_bar: bool = True,\n    bar_length: int = 60,\n    verbose: bool = True,\n) -&gt; list[str] | None:\n    \"\"\"Report the memory statistics for the visited module(s).\n\n    Args:\n        unit (str): The byte unit; one in [KB, MB, GB, TB, KiB, MiB, GiB, TiB].\n        with_bar (bool): Add simple horizontal percentage bars.\n        bar_length (int): The length of the percentage bars in number of characters.\n        verbose (bool): Directly print lines to stdout.\n    \"\"\"\n    if not hasattr(self, \"memory_stats\"):\n        return []\n    divisor = as_bytes(unit)\n    mem_stats = self.memory_stats.copy()\n    if not mem_stats:\n        return []\n    total_alloc = sum(module[\"allocated\"] for module in mem_stats)\n    for stat in mem_stats:\n        stat[\"rel_alloc\"] = stat[\"allocated\"] / total_alloc\n        stat[\"allocated\"] /= divisor\n    total_alloc /= divisor\n\n    lines = []\n    module_name_length = max([len(stat[\"module\"]) for stat in mem_stats])\n\n    for stat in mem_stats:\n        prefix = [stat[\"module\"].ljust(module_name_length), stat[\"allocated\"], unit]\n        postfix = [stat[\"rel_alloc\"] * 100, \"%\"]\n        line = cli_pbar(stat[\"rel_alloc\"], prefix, postfix, bar_length=bar_length)\n        lines.append(line)\n    if verbose:\n        print(\"\\n\".join(lines))\n    return lines\n</code></pre>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.Visitor.unlink","title":"unlink","text":"<pre><code>unlink()\n</code></pre> <p>Unlink hooks and maps.</p> Source code in <code>src/chuchichaestli/debug/visitor.py</code> <pre><code>def unlink(self):\n    \"\"\"Unlink hooks and maps.\"\"\"\n    self.hook_default = None\n    self.hook_map = {}\n</code></pre>"},{"location":"reference/chuchichaestli/debug/visitor/#chuchichaestli.debug.visitor.Visitor.visit","title":"visit","text":"<pre><code>visit(\n    visitee, depth: int = 0, caller: nn.Module | None = None\n)\n</code></pre> <p>Function called when visiting an object.</p> Source code in <code>src/chuchichaestli/debug/visitor.py</code> <pre><code>def visit(self, visitee, depth: int = 0, caller: nn.Module | None = None):\n    \"\"\"Function called when visiting an object.\"\"\"\n    # print(visitee, depth, caller)\n    if self.max_depth and depth &gt; self.max_depth:\n        return\n    for layer in visitee.modules():\n        if caller is layer:\n            return\n        layer_type = type(layer)\n        hook = self.hook_map.setdefault(layer_type, self.hook_default)\n        match hook.direction:\n            case HookDirection.Forward:\n                layer.register_forward_hook(hook.fn)\n            case HookDirection.Backward:\n                layer.register_backward_hook(hook.fn)\n            case HookDirection.Both:\n                layer.register_forward_hook(hook.fn)\n                layer.register_backward_hook(hook.fn)\n        self.visit(layer, depth=depth + 1, caller=layer)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/","title":"chuchichaestli.diffusion","text":""},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion","title":"chuchichaestli.diffusion","text":"<p>Diffusion schedulers module of chuchichaestli.</p> <p>Modules:</p> Name Description <code>ddpm</code> <p>Implementation of DDPM and its variants.</p> <code>distributions</code> <p>Auxiliary classes for sampling noise from different distributions.</p> <p>Classes:</p> Name Description <code>BBDM</code> <p>Brownian Bridge Diffusion Model (BBDM).</p> <code>CFGDDPM</code> <p>Classifier-free conditional diffusion probabilistic model (CF-DDPM).</p> <code>DDIM</code> <p>Denoising Diffusion Implicit Model (DDIM).</p> <code>DDPM</code> <p>Diffusion Probabilistic Model (DDPM) noise process.</p> <code>InDI</code> <p>Degradation process for inversion by direct iteration (InDI).</p> <code>PriorGrad</code> <p>PriorGrad noise process.</p>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.BBDM","title":"BBDM","text":"<pre><code>BBDM(\n    num_timesteps: int,\n    s: float = 1.0,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    *args,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Brownian Bridge Diffusion Model (BBDM).</p> <p>As described in the paper: \"BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models\" by Li et al. (2022);  see https://arxiv.org/abs/2205.07680.</p> <p>Initialize the Brownian Bridge Diffusion Model.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>s</code> <code>float</code> <p>Scaling factor for the Brownian Bridge.</p> <code>1.0</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>noise_distribution</code> <code>DistributionAdapter | None</code> <p>Noise distribution to use for the diffusion process.</p> <code>None</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the Brownian Bridge Diffusion Model.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    s: float = 1.0,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the Brownian Bridge Diffusion Model.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        s: Scaling factor for the Brownian Bridge.\n        device: Device to use for the computation.\n        noise_distribution: Noise distribution to use for the diffusion process.\n        args: Additional positional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(num_timesteps, device, noise_distribution, *args, **kwargs)\n    self.s = s\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.BBDM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    x_t = c\n    coef_shape = [x_t.shape[0]] + [1] * (x_t.dim() - 1)\n    for t in reversed(range(1, self.num_time_steps + 1)):\n        z = self.sample_noise(x_t.shape) if t &gt; 1 else torch.zeros_like(x_t)\n        tt = torch.full(coef_shape, t, device=self.device)\n        m_t = tt / self.num_time_steps\n        m_tm1 = (tt - 1) / self.num_time_steps\n        delta_t = 2 * self.s * (m_t - m_t**2)\n        delta_tm1 = 2 * self.s * (m_tm1 - m_tm1**2)\n        delta_t_tm1 = (\n            delta_t - delta_tm1 * ((1 - m_t) ** 2 / (1 - m_tm1) ** 2)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        delta_tilde_t = (\n            (delta_t_tm1 - delta_tm1) / delta_t\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        c_xt = (\n            (delta_tm1 / delta_t) * ((1 - m_t) / (1 - m_tm1))\n            + (delta_t_tm1 / delta_t) * (1 - m_tm1)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        c_yt = (\n            m_tm1 - m_t * ((1 - m_t) / (1 - m_tm1)) * (delta_tm1 / delta_t)\n            if t &lt; self.num_time_steps\n            else m_tm1\n        )\n        c_et = (\n            (1 - m_t) * (delta_t_tm1 / delta_t)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        x_t = (\n            c_xt * x_t\n            + c_yt * c\n            + c_et * model(x_t, t)\n            + torch.sqrt(delta_tilde_t) * z\n        )\n        if yield_intermediate:\n            yield x_t\n    yield x_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.BBDM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_0: Tensor,\n    condition: Tensor,\n    timesteps: Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Tensor, Tensor, Tensor]\n</code></pre> <p>Noise step for the Brownian Bridge Diffusion Model.</p> <p>Parameters:</p> Name Type Description Default <code>x_0</code> <code>Tensor</code> <p>Clean input tensor.</p> required <code>condition</code> <code>Tensor</code> <p>Condition tensor.</p> required <code>timesteps</code> <code>Tensor | None</code> <p>Timesteps to use for the diffusion process.</p> <code>None</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def noise_step(\n    self,\n    x_0: Tensor,\n    condition: Tensor,\n    timesteps: Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Noise step for the Brownian Bridge Diffusion Model.\n\n    Args:\n        x_0: Clean input tensor.\n        condition: Condition tensor.\n        timesteps: Timesteps to use for the diffusion process.\n        args: Additional positional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    if timesteps is not None:\n        x_0 = (\n            x_0.unsqueeze(0)\n            .expand(len(timesteps), *x_0.shape)\n            .reshape(-1, *x_0.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_0.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_0.shape[0])\n    noise = self.sample_noise(x_0.shape)\n    s_shape = [-1] + [1] * (x_0.dim() - 1)\n    m_t = (timesteps / self.num_time_steps).reshape(s_shape)\n    delta_t = 2 * self.s * (m_t - m_t**2).reshape(s_shape)\n\n    print(m_t.shape, x_0.shape, condition.shape, delta_t.shape, noise.shape)\n    return (\n        (1 - m_t) * x_0 + m_t * condition + torch.sqrt(delta_t) * noise,\n        noise,\n        timesteps,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.CFGDDPM","title":"CFGDDPM","text":"<pre><code>CFGDDPM(\n    num_timesteps: int,\n    beta_start: float = -20,\n    beta_end: float = 20,\n    uncond_prob: float = 0.1,\n    guidance_strength: float = 0.0,\n    noise_interpolation_coeff: float = 0.3,\n    device: str = \"cpu\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Classifier-free conditional diffusion probabilistic model (CF-DDPM).</p> <p>As described in the paper: \"Classifier-Free Diffusion Guidance\" by Ho and Salimans (2022); see https://arxiv.org/abs/2207.12598.</p> <p>Initialize the DDPM algorithm.</p> <p>A note on parameter selection: - num_timesteps = 256 and w = 0.3 performs best in terms of FID - num_timesteps = 128 and w = 4.0 performs best in terms of IS. - uncond_prob in [0.1, 0.2] is always better than 0.5.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta. This corresponds to lambda_min in the paper. Naming retained for consistency with other schedules.</p> <code>-20</code> <code>beta_end</code> <code>float</code> <p>End value for beta. This corresponds to lambda_max in the paper. Naming retained for consistency with other schedules.</p> <code>20</code> <code>uncond_prob</code> <code>float</code> <p>Probability of dropping the condition (p_uncond in paper).</p> <code>0.1</code> <code>guidance_strength</code> <code>float</code> <p>Strength of the guidance signal (w in paper). 0 means no guidance in generation.</p> <code>0.0</code> <code>noise_interpolation_coeff</code> <code>float</code> <p>Coefficient for noise interpolation (v in paper).</p> <code>0.3</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>alpha</code> <p>Compute alpha from lambda.</p> <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sigma</code> <p>Compute sigma from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = -20,\n    beta_end: float = 20,\n    uncond_prob: float = 0.1,\n    guidance_strength: float = 0.0,\n    noise_interpolation_coeff: float = 0.3,\n    device: str = \"cpu\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDPM algorithm.\n\n    A note on parameter selection:\n    - num_timesteps = 256 and w = 0.3 performs best in terms of FID\n    - num_timesteps = 128 and w = 4.0 performs best in terms of IS.\n    - uncond_prob in [0.1, 0.2] is always better than 0.5.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta. This corresponds to lambda_min in the paper. Naming retained for consistency with other schedules.\n        beta_end: End value for beta. This corresponds to lambda_max in the paper. Naming retained for consistency with other schedules.\n        uncond_prob: Probability of dropping the condition (p_uncond in paper).\n        guidance_strength: Strength of the guidance signal (w in paper). 0 means no guidance in generation.\n        noise_interpolation_coeff: Coefficient for noise interpolation (v in paper).\n        device: Device to use for the computation.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    beta_start = torch.tensor(beta_start, device=device)\n    beta_end = torch.tensor(beta_end, device=device)\n    self.b = torch.arctan(torch.exp(-beta_end / 2))\n    self.a = torch.arctan(torch.exp(-beta_start / 2)) - self.b\n    self.p = torch.tensor(uncond_prob, device=device)\n    self.w = torch.tensor(guidance_strength, device=device)\n    self.v = torch.tensor(noise_interpolation_coeff, device=device)\n    self.u_generation = torch.linspace(0, 1, num_timesteps, device=device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.CFGDDPM.alpha","title":"alpha  <code>staticmethod</code>","text":"<pre><code>alpha(lambda_t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute alpha from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>@staticmethod\ndef alpha(lambda_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute alpha from lambda.\"\"\"\n    alpha_square = torch.sigmoid(lambda_t)\n    return torch.sqrt(alpha_square)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.CFGDDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    zeros = torch.zeros_like(c)\n    z_t = self.sample_noise(c.shape)\n\n    for t in reversed(range(1, self.num_time_steps)):\n        lambda_t = -2 * torch.log(torch.tan(self.a * self.u_generation[t] + self.b))\n        lambda_tp1 = -2 * torch.log(\n            torch.tan(self.a * self.u_generation[t - 1] + self.b)\n        )\n        eps_t = model(torch.cat([zeros, z_t], dim=1), t)\n        ceps_t = model(torch.cat([c, z_t], dim=1), t)\n        eps_tilde_t = (1 + self.w) * ceps_t - self.w * eps_t\n        alpha_lambda_t = self.alpha(lambda_t)\n        sigma_lambda_t = self.sigma(lambda_t)\n        sigma_lambda_tp1 = self.sigma(lambda_tp1)\n        x_tilde_t = (z_t - sigma_lambda_t * eps_tilde_t) / alpha_lambda_t\n        if yield_intermediate:\n            yield x_tilde_t\n        if t &lt; self.num_time_steps:\n            mu_tilde = (\n                torch.exp(lambda_t - lambda_tp1)\n                * (self.alpha(lambda_tp1) / self.alpha(lambda_t))\n                * z_t\n            ) + (1 - torch.exp(lambda_t - lambda_tp1)) * self.alpha(\n                lambda_tp1\n            ) * x_tilde_t\n\n            sigma_tilde_squared = (\n                1 - torch.exp(lambda_t - lambda_tp1)\n            ) * sigma_lambda_tp1**2\n\n            sigma_lambda_t_reverse = (\n                1 - torch.exp(lambda_t - lambda_tp1)\n            ) * sigma_lambda_t**2\n\n            print(\n                lambda_t,\n                lambda_tp1,\n                lambda_t - lambda_tp1,\n                torch.exp(lambda_t - lambda_tp1),\n                1 - torch.exp(lambda_t - lambda_tp1),\n            )\n\n            std = (\n                sigma_tilde_squared ** (1 - self.v) * sigma_lambda_t_reverse**self.v\n            )\n            z_t = torch.normal(mu_tilde, std)\n\n    yield x_tilde_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.CFGDDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>args</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    u = torch.rand(x_t.shape, device=self.device)\n    lam = -2 * torch.log(torch.tan(self.a * u + self.b))\n    alpha_square = torch.sigmoid(lam)\n    alpha = torch.sqrt(alpha_square)\n    sigma = torch.sqrt(1 - alpha_square)\n    noise = self.sample_noise(x_t.shape)\n\n    zeros = torch.zeros_like(condition)\n    condition = torch.where(\n        torch.bernoulli(self.p * torch.ones_like(x_t)).type(torch.bool),\n        zeros,\n        condition,\n    )\n    x_t = alpha * x_t + sigma * noise\n    x_t = torch.cat([condition, x_t], dim=1)\n\n    return x_t, noise, lam\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.CFGDDPM.sigma","title":"sigma  <code>staticmethod</code>","text":"<pre><code>sigma(lambda_t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute sigma from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>@staticmethod\ndef sigma(lambda_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute sigma from lambda.\"\"\"\n    alpha_square = torch.sigmoid(lambda_t)\n    return torch.sqrt(1 - alpha_square)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.DDIM","title":"DDIM","text":"<pre><code>DDIM(\n    num_timesteps: int,\n    num_sample_steps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>Denoising Diffusion Implicit Model (DDIM).</p> <p>As described in the paper: \"Denoising Diffusion Implicit Models\" by Song et al. (2020); see https://arxiv.org/abs/2010.02502.</p> <p>Initialize the DDIM scheme.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of timesteps.</p> required <code>num_sample_steps</code> <code>int</code> <p>Number of timesteps to sample.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Annealing schedule</p> <code>'linear'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the DDPM model using DDIM.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddim.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    num_sample_steps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDIM scheme.\n\n    Args:\n        num_timesteps: Number of timesteps.\n        num_sample_steps: Number of timesteps to sample.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use.\n        schedule: Annealing schedule\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        num_timesteps, beta_start, beta_end, device, schedule, **kwargs\n    )\n    # Reverse the timesteps for sampling\n    self.tau = torch.linspace(\n        self.num_time_steps - 1, 0, num_sample_steps, dtype=int\n    ).to(device)  # Select timesteps in reverse order\n\n    # Precompute alpha_cumprod_prev\n    self.alpha_cumprod_prev = torch.cat(\n        [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]]\n    )\n    self.sqrt_alpha_cumprod_prev = torch.sqrt(self.alpha_cumprod_prev)\n    self.sqrt_1m_alpha_cumprod_prev = torch.sqrt(1.0 - self.alpha_cumprod_prev)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.DDIM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Tensor | Generator[Tensor, None, torch.Tensor]\n</code></pre> <p>Sample from the DDPM model using DDIM.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddim.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Tensor | Generator[Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the DDPM model using DDIM.\"\"\"\n    if condition is not None:\n        c = (\n            condition.unsqueeze(0)\n            .expand(n, *condition.shape)\n            .reshape(-1, *condition.shape[1:])\n        )\n        x_tau = self.sample_noise(c.shape).to(self.device)\n    elif shape is not None:\n        x_tau = self.sample_noise((n, *shape)).to(self.device)\n    else:\n        raise ValueError(\"Either condition or shape must be provided.\")\n    coef_shape = [-1] + [1] * (x_tau.dim() - 1)\n\n    for idx, t in enumerate(self.tau):\n        t = int(t)\n        if condition is not None:\n            eps_tau = model(torch.cat([c, x_tau], dim=1), t)\n        else:\n            eps_tau = model(x_tau, t)\n        x_0_hat = (\n            x_tau - self.sqrt_1m_alpha_cumprod[t] * eps_tau\n        ) / self.sqrt_alpha_cumprod[t].reshape(coef_shape)\n\n        x_tau = (\n            self.sqrt_alpha_cumprod_prev[t] * x_0_hat\n            + self.sqrt_1m_alpha_cumprod_prev[t] * eps_tau\n        )\n        if yield_intermediate:\n            yield x_tau\n\n    yield x_tau\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.DDPM","title":"DDPM","text":"<pre><code>DDPM(\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Diffusion Probabilistic Model (DDPM) noise process.</p> <p>As described in the paper: \"Denoising Diffusion Probabilistic Models\" by Ho et al. (2020); see https://arxiv.org/abs/2006.11239.</p> <p>Initialize the DDPM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDPM algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    self.num_time_steps = num_timesteps\n    self.beta = SCHEDULES[schedule](beta_start, beta_end, num_timesteps, device)\n    self.alpha = 1.0 - self.beta\n    self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n    self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n    self.sqrt_1m_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n    self.coef_inner = (1 - self.alpha) / self.sqrt_1m_alpha_cumprod\n    self.coef_outer = 1.0 / torch.sqrt(self.alpha)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.DDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.</p> <code>None</code> <code>shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the generated tensor. Only used if condition is None.</p> <code>None</code> <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.\n        shape: Shape of the generated tensor. Only used if condition is None.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    if condition is not None:\n        c = (\n            condition.unsqueeze(0)\n            .expand(n, *condition.shape)\n            .reshape(-1, *condition.shape[1:])\n        )\n        x_t = self.sample_noise(c.shape)\n    elif shape is not None:\n        x_t = self.sample_noise((n,) + shape)\n    else:\n        raise ValueError(\"Either condition or shape must be provided.\")\n    coef_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        if condition is not None:\n            eps_t = model(torch.cat([c, x_t], dim=1), t)\n        else:\n            eps_t = model(x_t, t)\n        coef_inner_t = self.coef_inner[t].reshape(coef_shape)\n        coef_outer_t = self.coef_outer[t].reshape(coef_shape)\n        x_tm1 = coef_outer_t * (x_t - coef_inner_t * eps_t)\n\n        if t &gt; 0:\n            noise = self.sample_noise(x_t.shape)\n            sigma_t = self.beta[t] ** 0.5\n            x_t = x_tm1 + sigma_t * noise\n            if yield_intermediate:\n                yield x_t\n\n    yield x_tm1\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.DDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>timesteps</code> <code>torch.Tensor | None</code> <p>Optional timesteps to sample noise for (will be sampled randomly if None).</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        timesteps: Optional timesteps to sample noise for (will be sampled randomly if None).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x_t = (\n            x_t.unsqueeze(0)\n            .expand(len(timesteps), *x_t.shape)\n            .reshape(-1, *x_t.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_t.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_t.shape[0])\n\n    noise = self.sample_noise(x_t.shape)\n\n    s_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    s1 = self.sqrt_alpha_cumprod[timesteps].reshape(s_shape)\n    s2 = self.sqrt_1m_alpha_cumprod[timesteps].reshape(s_shape)\n\n    x_t = s1 * x_t + s2 * noise\n    if condition is not None:\n        x_t = torch.cat([condition, x_t], dim=1)\n\n    return x_t, noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.InDI","title":"InDI","text":"<pre><code>InDI(\n    num_timesteps: int,\n    epsilon: float | torch.Tensor = 0.01,\n    device: str = \"cpu\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Degradation process for inversion by direct iteration (InDI).</p> <p>As described in the paper: \"Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration\" by Delbracio and Milanfar (2023); see https://arxiv.org/abs/2303.11435.</p> <p>Initialize the InDI algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>epsilon</code> <code>float | torch.Tensor</code> <p>Noise level. Can be a scalar or a tensor of shape (num_timesteps,). For eps_t = eps_0 / sqrt(t), the noise perturbation is a pure Brownian motion. For eps = eps_0 = cst, the InDI scheme is recovered.</p> <code>0.01</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sample_timesteps</code> <p>Sample timesteps for the InDI diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    epsilon: float | torch.Tensor = 0.01,\n    device: str = \"cpu\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the InDI algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        epsilon: Noise level. Can be a scalar or a tensor of shape (num_timesteps,).\n            For eps_t = eps_0 / sqrt(t), the noise perturbation is a pure Brownian motion.\n            For eps = eps_0 = cst, the InDI scheme is recovered.\n        device: Device to use for the computation.\n        kwargs: Additional keyword arguments.\n\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    self.num_time_steps = num_timesteps\n    self.delta = 1.0 / num_timesteps\n    if isinstance(epsilon, float):\n        self.epsilon = torch.full((num_timesteps,), epsilon, device=device)\n    else:\n        self.epsilon = epsilon\n        self.epsilon.to(device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.InDI.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n\n    x_t = c + self.epsilon[-1] * self.sample_noise(c.shape)\n    for i in reversed(range(1, self.num_time_steps)):\n        eps_t = self.epsilon[i]\n        eps_tmdelta = self.epsilon[i - 1]\n\n        t = i / self.num_time_steps\n        noise = self.sample_noise(c.shape)\n\n        x_tmdelta = (\n            (self.delta / t) * model(x_t, i)\n            + (1 - self.delta / t) * x_t\n            + (t - self.delta) * torch.sqrt(eps_tmdelta**2 - eps_t**2) * noise\n        )\n        x_t = x_tmdelta\n        if yield_intermediate:\n            yield x_t\n\n    yield x_tmdelta\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.InDI.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>High quality sample, tensor of shape (batch_size, *).</p> required <code>y</code> <code>torch.Tensor</code> <p>Corresponding low quality sample, tensor of shape (batch_size, *).</p> required <code>timesteps</code> <code>torch.Tensor | None</code> <p>Timesteps to sample noise from. If None, timesteps are sampled.</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def noise_step(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x: High quality sample, tensor of shape (batch_size, *).\n        y: Corresponding low quality sample, tensor of shape (batch_size, *).\n        timesteps: Timesteps to sample noise from. If None, timesteps are sampled.\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x = (\n            x.unsqueeze(0)\n            .expand(len(timesteps), *x.shape)\n            .reshape(-1, *x.shape[1:])\n        )\n        timesteps = (\n            timesteps.repeat_interleave(x.shape[0] // len(timesteps)) * self.delta\n        )\n    else:\n        timesteps = self.sample_timesteps(x.shape[0])\n    timesteps = timesteps.view(-1, *([1] * (x.dim() - 1)))\n    x_t = (1 - timesteps) * x + timesteps * y\n    # TODO: Verify that this is correct.\n    noise = x_t - x\n\n    return x_t, noise, timesteps.view(-1)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.InDI.sample_timesteps","title":"sample_timesteps","text":"<pre><code>sample_timesteps(batch_size: int) -&gt; torch.Tensor\n</code></pre> <p>Sample timesteps for the InDI diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples to generate.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (batch_size,) with the sampled timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def sample_timesteps(self, batch_size: int) -&gt; torch.Tensor:\n    \"\"\"Sample timesteps for the InDI diffusion process.\n\n    Args:\n        batch_size: Number of samples to generate.\n\n    Returns:\n        Tensor of shape (batch_size,) with the sampled timesteps.\n    \"\"\"\n    return (\n        torch.randint(0, self.num_time_steps, (batch_size,), device=self.device)\n        * self.delta\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.PriorGrad","title":"PriorGrad","text":"<pre><code>PriorGrad(\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>PriorGrad noise process.</p> <p>As described in the paper: \"PriorGrad: Improving conditional denoising diffusion models with data-dependent adataptive prior\" by Lee et al. (2021); see https://arxiv.org/abs/2106.06406.</p> <p>Initialize the PriorGrad algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float | torch.Tensor</code> <p>Mean value of the noise process.</p> required <code>scale</code> <code>float | torch.Tensor</code> <p>Scale value of the noise process.</p> required <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def __init__(\n    self,\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    num_timesteps: int,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n):\n    \"\"\"Initialize the PriorGrad algorithm.\n\n    Args:\n        mean: Mean value of the noise process.\n        scale: Scale value of the noise process.\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    scale = scale.to(device)\n    mean = mean.to(device)\n    distr = NormalDistribution(0, scale, device=device)\n    super().__init__(\n        noise_distribution=distr,\n        num_timesteps=num_timesteps,\n        beta_start=beta_start,\n        beta_end=beta_end,\n        device=device,\n        schedule=schedule,\n        **kwargs,\n    )\n    self.mean = mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.PriorGrad.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    x_t = self.sample_noise(c.shape)\n    coef_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        eps_t = model(torch.cat([c, x_t], dim=1), t)\n        coef_inner_t = self.coef_inner[t].reshape(coef_shape)\n        coef_outer_t = self.coef_outer[t].reshape(coef_shape)\n        x_tm1 = coef_outer_t * (x_t - coef_inner_t * eps_t)\n\n        if t &gt; 0:\n            noise = self.sample_noise(x_t.shape)\n            sigma_t = self.beta[t] ** 0.5\n            x_t = x_tm1 + sigma_t * noise\n            if yield_intermediate:\n                yield x_t\n\n    yield x_tm1 + self.mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/#chuchichaestli.diffusion.PriorGrad.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>timesteps</code> <code>torch.Tensor | None</code> <p>Optional timesteps to sample noise for (will be sampled randomly if None).</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        timesteps: Optional timesteps to sample noise for (will be sampled randomly if None).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x_t = (\n            x_t.unsqueeze(0)\n            .expand(len(timesteps), *x_t.shape)\n            .reshape(-1, *x_t.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_t.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_t.shape[0])\n    noise = self.sample_noise(x_t.shape)\n\n    s_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    s1 = self.sqrt_alpha_cumprod[timesteps].reshape(s_shape)\n    s2 = self.sqrt_1m_alpha_cumprod[timesteps].reshape(s_shape)\n\n    x_t = s1 * (x_t - self.mean) + s2 * noise\n\n    return torch.cat([condition, x_t], dim=1), noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/","title":"chuchichaestli.diffusion.ddpm","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm","title":"chuchichaestli.diffusion.ddpm","text":"<p>Implementation of DDPM and its variants.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base class for diffusion processes.</p> <code>bbdm</code> <p>Implementation of the Brownian Bridge Diffusion Model.</p> <code>cfg_ddpm</code> <p>Implementation of the classifier-free conditional DDPM (CF-DDPM) noise process.</p> <code>ddim</code> <p>Implementation of the Denoising Diffusion Implicit Model.</p> <code>ddpm</code> <p>Implementation of the Diffusion Probabilistic Model (DDPM) noise process.</p> <code>indi</code> <p>Implementation of Inversion by Direct Iteration (InDI).</p> <code>prior_grad</code> <p>Implementation of the PriorGrad noise process.</p> <code>shift_ddpm</code> <p>Implementation of the Shifted Diffusion Probabilistic Model (ShiftDDPM) noise process.</p> <p>Classes:</p> Name Description <code>BBDM</code> <p>Brownian Bridge Diffusion Model (BBDM).</p> <code>CFGDDPM</code> <p>Classifier-free conditional diffusion probabilistic model (CF-DDPM).</p> <code>DDIM</code> <p>Denoising Diffusion Implicit Model (DDIM).</p> <code>DDPM</code> <p>Diffusion Probabilistic Model (DDPM) noise process.</p> <code>InDI</code> <p>Degradation process for inversion by direct iteration (InDI).</p> <code>PriorGrad</code> <p>PriorGrad noise process.</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.BBDM","title":"BBDM","text":"<pre><code>BBDM(\n    num_timesteps: int,\n    s: float = 1.0,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    *args,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Brownian Bridge Diffusion Model (BBDM).</p> <p>As described in the paper: \"BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models\" by Li et al. (2022);  see https://arxiv.org/abs/2205.07680.</p> <p>Initialize the Brownian Bridge Diffusion Model.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>s</code> <code>float</code> <p>Scaling factor for the Brownian Bridge.</p> <code>1.0</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>noise_distribution</code> <code>DistributionAdapter | None</code> <p>Noise distribution to use for the diffusion process.</p> <code>None</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the Brownian Bridge Diffusion Model.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    s: float = 1.0,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the Brownian Bridge Diffusion Model.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        s: Scaling factor for the Brownian Bridge.\n        device: Device to use for the computation.\n        noise_distribution: Noise distribution to use for the diffusion process.\n        args: Additional positional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(num_timesteps, device, noise_distribution, *args, **kwargs)\n    self.s = s\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.BBDM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    x_t = c\n    coef_shape = [x_t.shape[0]] + [1] * (x_t.dim() - 1)\n    for t in reversed(range(1, self.num_time_steps + 1)):\n        z = self.sample_noise(x_t.shape) if t &gt; 1 else torch.zeros_like(x_t)\n        tt = torch.full(coef_shape, t, device=self.device)\n        m_t = tt / self.num_time_steps\n        m_tm1 = (tt - 1) / self.num_time_steps\n        delta_t = 2 * self.s * (m_t - m_t**2)\n        delta_tm1 = 2 * self.s * (m_tm1 - m_tm1**2)\n        delta_t_tm1 = (\n            delta_t - delta_tm1 * ((1 - m_t) ** 2 / (1 - m_tm1) ** 2)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        delta_tilde_t = (\n            (delta_t_tm1 - delta_tm1) / delta_t\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        c_xt = (\n            (delta_tm1 / delta_t) * ((1 - m_t) / (1 - m_tm1))\n            + (delta_t_tm1 / delta_t) * (1 - m_tm1)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        c_yt = (\n            m_tm1 - m_t * ((1 - m_t) / (1 - m_tm1)) * (delta_tm1 / delta_t)\n            if t &lt; self.num_time_steps\n            else m_tm1\n        )\n        c_et = (\n            (1 - m_t) * (delta_t_tm1 / delta_t)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        x_t = (\n            c_xt * x_t\n            + c_yt * c\n            + c_et * model(x_t, t)\n            + torch.sqrt(delta_tilde_t) * z\n        )\n        if yield_intermediate:\n            yield x_t\n    yield x_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.BBDM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_0: Tensor,\n    condition: Tensor,\n    timesteps: Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Tensor, Tensor, Tensor]\n</code></pre> <p>Noise step for the Brownian Bridge Diffusion Model.</p> <p>Parameters:</p> Name Type Description Default <code>x_0</code> <code>Tensor</code> <p>Clean input tensor.</p> required <code>condition</code> <code>Tensor</code> <p>Condition tensor.</p> required <code>timesteps</code> <code>Tensor | None</code> <p>Timesteps to use for the diffusion process.</p> <code>None</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def noise_step(\n    self,\n    x_0: Tensor,\n    condition: Tensor,\n    timesteps: Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Noise step for the Brownian Bridge Diffusion Model.\n\n    Args:\n        x_0: Clean input tensor.\n        condition: Condition tensor.\n        timesteps: Timesteps to use for the diffusion process.\n        args: Additional positional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    if timesteps is not None:\n        x_0 = (\n            x_0.unsqueeze(0)\n            .expand(len(timesteps), *x_0.shape)\n            .reshape(-1, *x_0.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_0.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_0.shape[0])\n    noise = self.sample_noise(x_0.shape)\n    s_shape = [-1] + [1] * (x_0.dim() - 1)\n    m_t = (timesteps / self.num_time_steps).reshape(s_shape)\n    delta_t = 2 * self.s * (m_t - m_t**2).reshape(s_shape)\n\n    print(m_t.shape, x_0.shape, condition.shape, delta_t.shape, noise.shape)\n    return (\n        (1 - m_t) * x_0 + m_t * condition + torch.sqrt(delta_t) * noise,\n        noise,\n        timesteps,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.CFGDDPM","title":"CFGDDPM","text":"<pre><code>CFGDDPM(\n    num_timesteps: int,\n    beta_start: float = -20,\n    beta_end: float = 20,\n    uncond_prob: float = 0.1,\n    guidance_strength: float = 0.0,\n    noise_interpolation_coeff: float = 0.3,\n    device: str = \"cpu\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Classifier-free conditional diffusion probabilistic model (CF-DDPM).</p> <p>As described in the paper: \"Classifier-Free Diffusion Guidance\" by Ho and Salimans (2022); see https://arxiv.org/abs/2207.12598.</p> <p>Initialize the DDPM algorithm.</p> <p>A note on parameter selection: - num_timesteps = 256 and w = 0.3 performs best in terms of FID - num_timesteps = 128 and w = 4.0 performs best in terms of IS. - uncond_prob in [0.1, 0.2] is always better than 0.5.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta. This corresponds to lambda_min in the paper. Naming retained for consistency with other schedules.</p> <code>-20</code> <code>beta_end</code> <code>float</code> <p>End value for beta. This corresponds to lambda_max in the paper. Naming retained for consistency with other schedules.</p> <code>20</code> <code>uncond_prob</code> <code>float</code> <p>Probability of dropping the condition (p_uncond in paper).</p> <code>0.1</code> <code>guidance_strength</code> <code>float</code> <p>Strength of the guidance signal (w in paper). 0 means no guidance in generation.</p> <code>0.0</code> <code>noise_interpolation_coeff</code> <code>float</code> <p>Coefficient for noise interpolation (v in paper).</p> <code>0.3</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>alpha</code> <p>Compute alpha from lambda.</p> <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sigma</code> <p>Compute sigma from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = -20,\n    beta_end: float = 20,\n    uncond_prob: float = 0.1,\n    guidance_strength: float = 0.0,\n    noise_interpolation_coeff: float = 0.3,\n    device: str = \"cpu\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDPM algorithm.\n\n    A note on parameter selection:\n    - num_timesteps = 256 and w = 0.3 performs best in terms of FID\n    - num_timesteps = 128 and w = 4.0 performs best in terms of IS.\n    - uncond_prob in [0.1, 0.2] is always better than 0.5.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta. This corresponds to lambda_min in the paper. Naming retained for consistency with other schedules.\n        beta_end: End value for beta. This corresponds to lambda_max in the paper. Naming retained for consistency with other schedules.\n        uncond_prob: Probability of dropping the condition (p_uncond in paper).\n        guidance_strength: Strength of the guidance signal (w in paper). 0 means no guidance in generation.\n        noise_interpolation_coeff: Coefficient for noise interpolation (v in paper).\n        device: Device to use for the computation.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    beta_start = torch.tensor(beta_start, device=device)\n    beta_end = torch.tensor(beta_end, device=device)\n    self.b = torch.arctan(torch.exp(-beta_end / 2))\n    self.a = torch.arctan(torch.exp(-beta_start / 2)) - self.b\n    self.p = torch.tensor(uncond_prob, device=device)\n    self.w = torch.tensor(guidance_strength, device=device)\n    self.v = torch.tensor(noise_interpolation_coeff, device=device)\n    self.u_generation = torch.linspace(0, 1, num_timesteps, device=device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.CFGDDPM.alpha","title":"alpha  <code>staticmethod</code>","text":"<pre><code>alpha(lambda_t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute alpha from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>@staticmethod\ndef alpha(lambda_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute alpha from lambda.\"\"\"\n    alpha_square = torch.sigmoid(lambda_t)\n    return torch.sqrt(alpha_square)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.CFGDDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    zeros = torch.zeros_like(c)\n    z_t = self.sample_noise(c.shape)\n\n    for t in reversed(range(1, self.num_time_steps)):\n        lambda_t = -2 * torch.log(torch.tan(self.a * self.u_generation[t] + self.b))\n        lambda_tp1 = -2 * torch.log(\n            torch.tan(self.a * self.u_generation[t - 1] + self.b)\n        )\n        eps_t = model(torch.cat([zeros, z_t], dim=1), t)\n        ceps_t = model(torch.cat([c, z_t], dim=1), t)\n        eps_tilde_t = (1 + self.w) * ceps_t - self.w * eps_t\n        alpha_lambda_t = self.alpha(lambda_t)\n        sigma_lambda_t = self.sigma(lambda_t)\n        sigma_lambda_tp1 = self.sigma(lambda_tp1)\n        x_tilde_t = (z_t - sigma_lambda_t * eps_tilde_t) / alpha_lambda_t\n        if yield_intermediate:\n            yield x_tilde_t\n        if t &lt; self.num_time_steps:\n            mu_tilde = (\n                torch.exp(lambda_t - lambda_tp1)\n                * (self.alpha(lambda_tp1) / self.alpha(lambda_t))\n                * z_t\n            ) + (1 - torch.exp(lambda_t - lambda_tp1)) * self.alpha(\n                lambda_tp1\n            ) * x_tilde_t\n\n            sigma_tilde_squared = (\n                1 - torch.exp(lambda_t - lambda_tp1)\n            ) * sigma_lambda_tp1**2\n\n            sigma_lambda_t_reverse = (\n                1 - torch.exp(lambda_t - lambda_tp1)\n            ) * sigma_lambda_t**2\n\n            print(\n                lambda_t,\n                lambda_tp1,\n                lambda_t - lambda_tp1,\n                torch.exp(lambda_t - lambda_tp1),\n                1 - torch.exp(lambda_t - lambda_tp1),\n            )\n\n            std = (\n                sigma_tilde_squared ** (1 - self.v) * sigma_lambda_t_reverse**self.v\n            )\n            z_t = torch.normal(mu_tilde, std)\n\n    yield x_tilde_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.CFGDDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>args</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    u = torch.rand(x_t.shape, device=self.device)\n    lam = -2 * torch.log(torch.tan(self.a * u + self.b))\n    alpha_square = torch.sigmoid(lam)\n    alpha = torch.sqrt(alpha_square)\n    sigma = torch.sqrt(1 - alpha_square)\n    noise = self.sample_noise(x_t.shape)\n\n    zeros = torch.zeros_like(condition)\n    condition = torch.where(\n        torch.bernoulli(self.p * torch.ones_like(x_t)).type(torch.bool),\n        zeros,\n        condition,\n    )\n    x_t = alpha * x_t + sigma * noise\n    x_t = torch.cat([condition, x_t], dim=1)\n\n    return x_t, noise, lam\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.CFGDDPM.sigma","title":"sigma  <code>staticmethod</code>","text":"<pre><code>sigma(lambda_t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute sigma from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>@staticmethod\ndef sigma(lambda_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute sigma from lambda.\"\"\"\n    alpha_square = torch.sigmoid(lambda_t)\n    return torch.sqrt(1 - alpha_square)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.DDIM","title":"DDIM","text":"<pre><code>DDIM(\n    num_timesteps: int,\n    num_sample_steps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>Denoising Diffusion Implicit Model (DDIM).</p> <p>As described in the paper: \"Denoising Diffusion Implicit Models\" by Song et al. (2020); see https://arxiv.org/abs/2010.02502.</p> <p>Initialize the DDIM scheme.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of timesteps.</p> required <code>num_sample_steps</code> <code>int</code> <p>Number of timesteps to sample.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Annealing schedule</p> <code>'linear'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the DDPM model using DDIM.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddim.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    num_sample_steps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDIM scheme.\n\n    Args:\n        num_timesteps: Number of timesteps.\n        num_sample_steps: Number of timesteps to sample.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use.\n        schedule: Annealing schedule\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        num_timesteps, beta_start, beta_end, device, schedule, **kwargs\n    )\n    # Reverse the timesteps for sampling\n    self.tau = torch.linspace(\n        self.num_time_steps - 1, 0, num_sample_steps, dtype=int\n    ).to(device)  # Select timesteps in reverse order\n\n    # Precompute alpha_cumprod_prev\n    self.alpha_cumprod_prev = torch.cat(\n        [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]]\n    )\n    self.sqrt_alpha_cumprod_prev = torch.sqrt(self.alpha_cumprod_prev)\n    self.sqrt_1m_alpha_cumprod_prev = torch.sqrt(1.0 - self.alpha_cumprod_prev)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.DDIM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Tensor | Generator[Tensor, None, torch.Tensor]\n</code></pre> <p>Sample from the DDPM model using DDIM.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddim.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Tensor | Generator[Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the DDPM model using DDIM.\"\"\"\n    if condition is not None:\n        c = (\n            condition.unsqueeze(0)\n            .expand(n, *condition.shape)\n            .reshape(-1, *condition.shape[1:])\n        )\n        x_tau = self.sample_noise(c.shape).to(self.device)\n    elif shape is not None:\n        x_tau = self.sample_noise((n, *shape)).to(self.device)\n    else:\n        raise ValueError(\"Either condition or shape must be provided.\")\n    coef_shape = [-1] + [1] * (x_tau.dim() - 1)\n\n    for idx, t in enumerate(self.tau):\n        t = int(t)\n        if condition is not None:\n            eps_tau = model(torch.cat([c, x_tau], dim=1), t)\n        else:\n            eps_tau = model(x_tau, t)\n        x_0_hat = (\n            x_tau - self.sqrt_1m_alpha_cumprod[t] * eps_tau\n        ) / self.sqrt_alpha_cumprod[t].reshape(coef_shape)\n\n        x_tau = (\n            self.sqrt_alpha_cumprod_prev[t] * x_0_hat\n            + self.sqrt_1m_alpha_cumprod_prev[t] * eps_tau\n        )\n        if yield_intermediate:\n            yield x_tau\n\n    yield x_tau\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.DDPM","title":"DDPM","text":"<pre><code>DDPM(\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Diffusion Probabilistic Model (DDPM) noise process.</p> <p>As described in the paper: \"Denoising Diffusion Probabilistic Models\" by Ho et al. (2020); see https://arxiv.org/abs/2006.11239.</p> <p>Initialize the DDPM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDPM algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    self.num_time_steps = num_timesteps\n    self.beta = SCHEDULES[schedule](beta_start, beta_end, num_timesteps, device)\n    self.alpha = 1.0 - self.beta\n    self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n    self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n    self.sqrt_1m_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n    self.coef_inner = (1 - self.alpha) / self.sqrt_1m_alpha_cumprod\n    self.coef_outer = 1.0 / torch.sqrt(self.alpha)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.DDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.</p> <code>None</code> <code>shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the generated tensor. Only used if condition is None.</p> <code>None</code> <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.\n        shape: Shape of the generated tensor. Only used if condition is None.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    if condition is not None:\n        c = (\n            condition.unsqueeze(0)\n            .expand(n, *condition.shape)\n            .reshape(-1, *condition.shape[1:])\n        )\n        x_t = self.sample_noise(c.shape)\n    elif shape is not None:\n        x_t = self.sample_noise((n,) + shape)\n    else:\n        raise ValueError(\"Either condition or shape must be provided.\")\n    coef_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        if condition is not None:\n            eps_t = model(torch.cat([c, x_t], dim=1), t)\n        else:\n            eps_t = model(x_t, t)\n        coef_inner_t = self.coef_inner[t].reshape(coef_shape)\n        coef_outer_t = self.coef_outer[t].reshape(coef_shape)\n        x_tm1 = coef_outer_t * (x_t - coef_inner_t * eps_t)\n\n        if t &gt; 0:\n            noise = self.sample_noise(x_t.shape)\n            sigma_t = self.beta[t] ** 0.5\n            x_t = x_tm1 + sigma_t * noise\n            if yield_intermediate:\n                yield x_t\n\n    yield x_tm1\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.DDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>timesteps</code> <code>torch.Tensor | None</code> <p>Optional timesteps to sample noise for (will be sampled randomly if None).</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        timesteps: Optional timesteps to sample noise for (will be sampled randomly if None).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x_t = (\n            x_t.unsqueeze(0)\n            .expand(len(timesteps), *x_t.shape)\n            .reshape(-1, *x_t.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_t.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_t.shape[0])\n\n    noise = self.sample_noise(x_t.shape)\n\n    s_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    s1 = self.sqrt_alpha_cumprod[timesteps].reshape(s_shape)\n    s2 = self.sqrt_1m_alpha_cumprod[timesteps].reshape(s_shape)\n\n    x_t = s1 * x_t + s2 * noise\n    if condition is not None:\n        x_t = torch.cat([condition, x_t], dim=1)\n\n    return x_t, noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.InDI","title":"InDI","text":"<pre><code>InDI(\n    num_timesteps: int,\n    epsilon: float | torch.Tensor = 0.01,\n    device: str = \"cpu\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Degradation process for inversion by direct iteration (InDI).</p> <p>As described in the paper: \"Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration\" by Delbracio and Milanfar (2023); see https://arxiv.org/abs/2303.11435.</p> <p>Initialize the InDI algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>epsilon</code> <code>float | torch.Tensor</code> <p>Noise level. Can be a scalar or a tensor of shape (num_timesteps,). For eps_t = eps_0 / sqrt(t), the noise perturbation is a pure Brownian motion. For eps = eps_0 = cst, the InDI scheme is recovered.</p> <code>0.01</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sample_timesteps</code> <p>Sample timesteps for the InDI diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    epsilon: float | torch.Tensor = 0.01,\n    device: str = \"cpu\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the InDI algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        epsilon: Noise level. Can be a scalar or a tensor of shape (num_timesteps,).\n            For eps_t = eps_0 / sqrt(t), the noise perturbation is a pure Brownian motion.\n            For eps = eps_0 = cst, the InDI scheme is recovered.\n        device: Device to use for the computation.\n        kwargs: Additional keyword arguments.\n\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    self.num_time_steps = num_timesteps\n    self.delta = 1.0 / num_timesteps\n    if isinstance(epsilon, float):\n        self.epsilon = torch.full((num_timesteps,), epsilon, device=device)\n    else:\n        self.epsilon = epsilon\n        self.epsilon.to(device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.InDI.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n\n    x_t = c + self.epsilon[-1] * self.sample_noise(c.shape)\n    for i in reversed(range(1, self.num_time_steps)):\n        eps_t = self.epsilon[i]\n        eps_tmdelta = self.epsilon[i - 1]\n\n        t = i / self.num_time_steps\n        noise = self.sample_noise(c.shape)\n\n        x_tmdelta = (\n            (self.delta / t) * model(x_t, i)\n            + (1 - self.delta / t) * x_t\n            + (t - self.delta) * torch.sqrt(eps_tmdelta**2 - eps_t**2) * noise\n        )\n        x_t = x_tmdelta\n        if yield_intermediate:\n            yield x_t\n\n    yield x_tmdelta\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.InDI.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>High quality sample, tensor of shape (batch_size, *).</p> required <code>y</code> <code>torch.Tensor</code> <p>Corresponding low quality sample, tensor of shape (batch_size, *).</p> required <code>timesteps</code> <code>torch.Tensor | None</code> <p>Timesteps to sample noise from. If None, timesteps are sampled.</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def noise_step(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x: High quality sample, tensor of shape (batch_size, *).\n        y: Corresponding low quality sample, tensor of shape (batch_size, *).\n        timesteps: Timesteps to sample noise from. If None, timesteps are sampled.\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x = (\n            x.unsqueeze(0)\n            .expand(len(timesteps), *x.shape)\n            .reshape(-1, *x.shape[1:])\n        )\n        timesteps = (\n            timesteps.repeat_interleave(x.shape[0] // len(timesteps)) * self.delta\n        )\n    else:\n        timesteps = self.sample_timesteps(x.shape[0])\n    timesteps = timesteps.view(-1, *([1] * (x.dim() - 1)))\n    x_t = (1 - timesteps) * x + timesteps * y\n    # TODO: Verify that this is correct.\n    noise = x_t - x\n\n    return x_t, noise, timesteps.view(-1)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.InDI.sample_timesteps","title":"sample_timesteps","text":"<pre><code>sample_timesteps(batch_size: int) -&gt; torch.Tensor\n</code></pre> <p>Sample timesteps for the InDI diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples to generate.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (batch_size,) with the sampled timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def sample_timesteps(self, batch_size: int) -&gt; torch.Tensor:\n    \"\"\"Sample timesteps for the InDI diffusion process.\n\n    Args:\n        batch_size: Number of samples to generate.\n\n    Returns:\n        Tensor of shape (batch_size,) with the sampled timesteps.\n    \"\"\"\n    return (\n        torch.randint(0, self.num_time_steps, (batch_size,), device=self.device)\n        * self.delta\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.PriorGrad","title":"PriorGrad","text":"<pre><code>PriorGrad(\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>PriorGrad noise process.</p> <p>As described in the paper: \"PriorGrad: Improving conditional denoising diffusion models with data-dependent adataptive prior\" by Lee et al. (2021); see https://arxiv.org/abs/2106.06406.</p> <p>Initialize the PriorGrad algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float | torch.Tensor</code> <p>Mean value of the noise process.</p> required <code>scale</code> <code>float | torch.Tensor</code> <p>Scale value of the noise process.</p> required <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def __init__(\n    self,\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    num_timesteps: int,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n):\n    \"\"\"Initialize the PriorGrad algorithm.\n\n    Args:\n        mean: Mean value of the noise process.\n        scale: Scale value of the noise process.\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    scale = scale.to(device)\n    mean = mean.to(device)\n    distr = NormalDistribution(0, scale, device=device)\n    super().__init__(\n        noise_distribution=distr,\n        num_timesteps=num_timesteps,\n        beta_start=beta_start,\n        beta_end=beta_end,\n        device=device,\n        schedule=schedule,\n        **kwargs,\n    )\n    self.mean = mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.PriorGrad.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    x_t = self.sample_noise(c.shape)\n    coef_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        eps_t = model(torch.cat([c, x_t], dim=1), t)\n        coef_inner_t = self.coef_inner[t].reshape(coef_shape)\n        coef_outer_t = self.coef_outer[t].reshape(coef_shape)\n        x_tm1 = coef_outer_t * (x_t - coef_inner_t * eps_t)\n\n        if t &gt; 0:\n            noise = self.sample_noise(x_t.shape)\n            sigma_t = self.beta[t] ** 0.5\n            x_t = x_tm1 + sigma_t * noise\n            if yield_intermediate:\n                yield x_t\n\n    yield x_tm1 + self.mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/#chuchichaestli.diffusion.ddpm.PriorGrad.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>timesteps</code> <code>torch.Tensor | None</code> <p>Optional timesteps to sample noise for (will be sampled randomly if None).</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        timesteps: Optional timesteps to sample noise for (will be sampled randomly if None).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x_t = (\n            x_t.unsqueeze(0)\n            .expand(len(timesteps), *x_t.shape)\n            .reshape(-1, *x_t.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_t.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_t.shape[0])\n    noise = self.sample_noise(x_t.shape)\n\n    s_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    s1 = self.sqrt_alpha_cumprod[timesteps].reshape(s_shape)\n    s2 = self.sqrt_1m_alpha_cumprod[timesteps].reshape(s_shape)\n\n    x_t = s1 * (x_t - self.mean) + s2 * noise\n\n    return torch.cat([condition, x_t], dim=1), noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/base/","title":"chuchichaestli.diffusion.ddpm.base","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/base/#chuchichaestli.diffusion.ddpm.base","title":"chuchichaestli.diffusion.ddpm.base","text":"<p>Base class for diffusion processes.</p> <p>Classes:</p> Name Description <code>DiffusionProcess</code> <p>Base class for diffusion processes.</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/base/#chuchichaestli.diffusion.ddpm.base.DiffusionProcess","title":"DiffusionProcess","text":"<pre><code>DiffusionProcess(\n    timesteps: int,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    generator: torch.Generator | None = None,\n    *args,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for diffusion processes.</p> <p>Initialize the diffusion process.</p> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sample_noise</code> <p>Sample noise for the diffusion process.</p> <code>sample_timesteps</code> <p>Sample timesteps for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/base.py</code> <pre><code>def __init__(\n    self,\n    timesteps: int,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    generator: torch.Generator | None = None,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the diffusion process.\"\"\"\n    self.num_time_steps = timesteps\n    self.device = device\n    self.noise_distribution = noise_distribution\n    self.generator = generator\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/base/#chuchichaestli.diffusion.ddpm.base.DiffusionProcess.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Generator[torch.Tensor, None, torch.Tensor]\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.</p> <code>None</code> <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/base/#chuchichaestli.diffusion.ddpm.base.DiffusionProcess.noise_step","title":"noise_step  <code>abstractmethod</code>","text":"<pre><code>noise_step(\n    x_t: torch.Tensor, *args, **kwargs\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/base.py</code> <pre><code>@abstractmethod\ndef noise_step(\n    self, x_t: torch.Tensor, *args, **kwargs\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/base/#chuchichaestli.diffusion.ddpm.base.DiffusionProcess.sample_noise","title":"sample_noise","text":"<pre><code>sample_noise(shape: torch.Size) -&gt; torch.Tensor\n</code></pre> <p>Sample noise for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>torch.Size</code> <p>Shape of the noise tensor.</p> required <code>mean</code> <p>Mean of the noise tensor.</p> required <code>scale</code> <p>Scale of the noise tensor.</p> required Source code in <code>src/chuchichaestli/diffusion/ddpm/base.py</code> <pre><code>def sample_noise(self, shape: torch.Size) -&gt; torch.Tensor:\n    \"\"\"Sample noise for the diffusion process.\n\n    Args:\n        shape: Shape of the noise tensor.\n        mean: Mean of the noise tensor.\n        scale: Scale of the noise tensor.\n    \"\"\"\n    if self.noise_distribution is not None:\n        return self.noise_distribution(shape)\n    return torch.randn(shape, generator=self.generator, device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/base/#chuchichaestli.diffusion.ddpm.base.DiffusionProcess.sample_timesteps","title":"sample_timesteps","text":"<pre><code>sample_timesteps(n: int) -&gt; torch.Tensor\n</code></pre> <p>Sample timesteps for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/base.py</code> <pre><code>def sample_timesteps(self, n: int) -&gt; torch.Tensor:\n    \"\"\"Sample timesteps for the diffusion process.\"\"\"\n    return torch.randint(0, self.num_time_steps, (n,), device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/bbdm/","title":"chuchichaestli.diffusion.ddpm.bbdm","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/bbdm/#chuchichaestli.diffusion.ddpm.bbdm","title":"chuchichaestli.diffusion.ddpm.bbdm","text":"<p>Implementation of the Brownian Bridge Diffusion Model.</p> <p>Classes:</p> Name Description <code>BBDM</code> <p>Brownian Bridge Diffusion Model (BBDM).</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/bbdm/#chuchichaestli.diffusion.ddpm.bbdm.BBDM","title":"BBDM","text":"<pre><code>BBDM(\n    num_timesteps: int,\n    s: float = 1.0,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    *args,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Brownian Bridge Diffusion Model (BBDM).</p> <p>As described in the paper: \"BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models\" by Li et al. (2022);  see https://arxiv.org/abs/2205.07680.</p> <p>Initialize the Brownian Bridge Diffusion Model.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>s</code> <code>float</code> <p>Scaling factor for the Brownian Bridge.</p> <code>1.0</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>noise_distribution</code> <code>DistributionAdapter | None</code> <p>Noise distribution to use for the diffusion process.</p> <code>None</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the Brownian Bridge Diffusion Model.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    s: float = 1.0,\n    device: str = \"cpu\",\n    noise_distribution: DistributionAdapter | None = None,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the Brownian Bridge Diffusion Model.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        s: Scaling factor for the Brownian Bridge.\n        device: Device to use for the computation.\n        noise_distribution: Noise distribution to use for the diffusion process.\n        args: Additional positional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(num_timesteps, device, noise_distribution, *args, **kwargs)\n    self.s = s\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/bbdm/#chuchichaestli.diffusion.ddpm.bbdm.BBDM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    x_t = c\n    coef_shape = [x_t.shape[0]] + [1] * (x_t.dim() - 1)\n    for t in reversed(range(1, self.num_time_steps + 1)):\n        z = self.sample_noise(x_t.shape) if t &gt; 1 else torch.zeros_like(x_t)\n        tt = torch.full(coef_shape, t, device=self.device)\n        m_t = tt / self.num_time_steps\n        m_tm1 = (tt - 1) / self.num_time_steps\n        delta_t = 2 * self.s * (m_t - m_t**2)\n        delta_tm1 = 2 * self.s * (m_tm1 - m_tm1**2)\n        delta_t_tm1 = (\n            delta_t - delta_tm1 * ((1 - m_t) ** 2 / (1 - m_tm1) ** 2)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        delta_tilde_t = (\n            (delta_t_tm1 - delta_tm1) / delta_t\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        c_xt = (\n            (delta_tm1 / delta_t) * ((1 - m_t) / (1 - m_tm1))\n            + (delta_t_tm1 / delta_t) * (1 - m_tm1)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        c_yt = (\n            m_tm1 - m_t * ((1 - m_t) / (1 - m_tm1)) * (delta_tm1 / delta_t)\n            if t &lt; self.num_time_steps\n            else m_tm1\n        )\n        c_et = (\n            (1 - m_t) * (delta_t_tm1 / delta_t)\n            if t &lt; self.num_time_steps\n            else torch.zeros_like(tt)\n        )\n        x_t = (\n            c_xt * x_t\n            + c_yt * c\n            + c_et * model(x_t, t)\n            + torch.sqrt(delta_tilde_t) * z\n        )\n        if yield_intermediate:\n            yield x_t\n    yield x_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/bbdm/#chuchichaestli.diffusion.ddpm.bbdm.BBDM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_0: Tensor,\n    condition: Tensor,\n    timesteps: Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Tensor, Tensor, Tensor]\n</code></pre> <p>Noise step for the Brownian Bridge Diffusion Model.</p> <p>Parameters:</p> Name Type Description Default <code>x_0</code> <code>Tensor</code> <p>Clean input tensor.</p> required <code>condition</code> <code>Tensor</code> <p>Condition tensor.</p> required <code>timesteps</code> <code>Tensor | None</code> <p>Timesteps to use for the diffusion process.</p> <code>None</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/bbdm.py</code> <pre><code>def noise_step(\n    self,\n    x_0: Tensor,\n    condition: Tensor,\n    timesteps: Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Noise step for the Brownian Bridge Diffusion Model.\n\n    Args:\n        x_0: Clean input tensor.\n        condition: Condition tensor.\n        timesteps: Timesteps to use for the diffusion process.\n        args: Additional positional arguments.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    if timesteps is not None:\n        x_0 = (\n            x_0.unsqueeze(0)\n            .expand(len(timesteps), *x_0.shape)\n            .reshape(-1, *x_0.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_0.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_0.shape[0])\n    noise = self.sample_noise(x_0.shape)\n    s_shape = [-1] + [1] * (x_0.dim() - 1)\n    m_t = (timesteps / self.num_time_steps).reshape(s_shape)\n    delta_t = 2 * self.s * (m_t - m_t**2).reshape(s_shape)\n\n    print(m_t.shape, x_0.shape, condition.shape, delta_t.shape, noise.shape)\n    return (\n        (1 - m_t) * x_0 + m_t * condition + torch.sqrt(delta_t) * noise,\n        noise,\n        timesteps,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/","title":"chuchichaestli.diffusion.ddpm.cfg_ddpm","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/#chuchichaestli.diffusion.ddpm.cfg_ddpm","title":"chuchichaestli.diffusion.ddpm.cfg_ddpm","text":"<p>Implementation of the classifier-free conditional DDPM (CF-DDPM) noise process.</p> <p>Classes:</p> Name Description <code>CFGDDPM</code> <p>Classifier-free conditional diffusion probabilistic model (CF-DDPM).</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/#chuchichaestli.diffusion.ddpm.cfg_ddpm.CFGDDPM","title":"CFGDDPM","text":"<pre><code>CFGDDPM(\n    num_timesteps: int,\n    beta_start: float = -20,\n    beta_end: float = 20,\n    uncond_prob: float = 0.1,\n    guidance_strength: float = 0.0,\n    noise_interpolation_coeff: float = 0.3,\n    device: str = \"cpu\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Classifier-free conditional diffusion probabilistic model (CF-DDPM).</p> <p>As described in the paper: \"Classifier-Free Diffusion Guidance\" by Ho and Salimans (2022); see https://arxiv.org/abs/2207.12598.</p> <p>Initialize the DDPM algorithm.</p> <p>A note on parameter selection: - num_timesteps = 256 and w = 0.3 performs best in terms of FID - num_timesteps = 128 and w = 4.0 performs best in terms of IS. - uncond_prob in [0.1, 0.2] is always better than 0.5.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta. This corresponds to lambda_min in the paper. Naming retained for consistency with other schedules.</p> <code>-20</code> <code>beta_end</code> <code>float</code> <p>End value for beta. This corresponds to lambda_max in the paper. Naming retained for consistency with other schedules.</p> <code>20</code> <code>uncond_prob</code> <code>float</code> <p>Probability of dropping the condition (p_uncond in paper).</p> <code>0.1</code> <code>guidance_strength</code> <code>float</code> <p>Strength of the guidance signal (w in paper). 0 means no guidance in generation.</p> <code>0.0</code> <code>noise_interpolation_coeff</code> <code>float</code> <p>Coefficient for noise interpolation (v in paper).</p> <code>0.3</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>alpha</code> <p>Compute alpha from lambda.</p> <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sigma</code> <p>Compute sigma from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = -20,\n    beta_end: float = 20,\n    uncond_prob: float = 0.1,\n    guidance_strength: float = 0.0,\n    noise_interpolation_coeff: float = 0.3,\n    device: str = \"cpu\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDPM algorithm.\n\n    A note on parameter selection:\n    - num_timesteps = 256 and w = 0.3 performs best in terms of FID\n    - num_timesteps = 128 and w = 4.0 performs best in terms of IS.\n    - uncond_prob in [0.1, 0.2] is always better than 0.5.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta. This corresponds to lambda_min in the paper. Naming retained for consistency with other schedules.\n        beta_end: End value for beta. This corresponds to lambda_max in the paper. Naming retained for consistency with other schedules.\n        uncond_prob: Probability of dropping the condition (p_uncond in paper).\n        guidance_strength: Strength of the guidance signal (w in paper). 0 means no guidance in generation.\n        noise_interpolation_coeff: Coefficient for noise interpolation (v in paper).\n        device: Device to use for the computation.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    beta_start = torch.tensor(beta_start, device=device)\n    beta_end = torch.tensor(beta_end, device=device)\n    self.b = torch.arctan(torch.exp(-beta_end / 2))\n    self.a = torch.arctan(torch.exp(-beta_start / 2)) - self.b\n    self.p = torch.tensor(uncond_prob, device=device)\n    self.w = torch.tensor(guidance_strength, device=device)\n    self.v = torch.tensor(noise_interpolation_coeff, device=device)\n    self.u_generation = torch.linspace(0, 1, num_timesteps, device=device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/#chuchichaestli.diffusion.ddpm.cfg_ddpm.CFGDDPM.alpha","title":"alpha  <code>staticmethod</code>","text":"<pre><code>alpha(lambda_t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute alpha from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>@staticmethod\ndef alpha(lambda_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute alpha from lambda.\"\"\"\n    alpha_square = torch.sigmoid(lambda_t)\n    return torch.sqrt(alpha_square)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/#chuchichaestli.diffusion.ddpm.cfg_ddpm.CFGDDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    zeros = torch.zeros_like(c)\n    z_t = self.sample_noise(c.shape)\n\n    for t in reversed(range(1, self.num_time_steps)):\n        lambda_t = -2 * torch.log(torch.tan(self.a * self.u_generation[t] + self.b))\n        lambda_tp1 = -2 * torch.log(\n            torch.tan(self.a * self.u_generation[t - 1] + self.b)\n        )\n        eps_t = model(torch.cat([zeros, z_t], dim=1), t)\n        ceps_t = model(torch.cat([c, z_t], dim=1), t)\n        eps_tilde_t = (1 + self.w) * ceps_t - self.w * eps_t\n        alpha_lambda_t = self.alpha(lambda_t)\n        sigma_lambda_t = self.sigma(lambda_t)\n        sigma_lambda_tp1 = self.sigma(lambda_tp1)\n        x_tilde_t = (z_t - sigma_lambda_t * eps_tilde_t) / alpha_lambda_t\n        if yield_intermediate:\n            yield x_tilde_t\n        if t &lt; self.num_time_steps:\n            mu_tilde = (\n                torch.exp(lambda_t - lambda_tp1)\n                * (self.alpha(lambda_tp1) / self.alpha(lambda_t))\n                * z_t\n            ) + (1 - torch.exp(lambda_t - lambda_tp1)) * self.alpha(\n                lambda_tp1\n            ) * x_tilde_t\n\n            sigma_tilde_squared = (\n                1 - torch.exp(lambda_t - lambda_tp1)\n            ) * sigma_lambda_tp1**2\n\n            sigma_lambda_t_reverse = (\n                1 - torch.exp(lambda_t - lambda_tp1)\n            ) * sigma_lambda_t**2\n\n            print(\n                lambda_t,\n                lambda_tp1,\n                lambda_t - lambda_tp1,\n                torch.exp(lambda_t - lambda_tp1),\n                1 - torch.exp(lambda_t - lambda_tp1),\n            )\n\n            std = (\n                sigma_tilde_squared ** (1 - self.v) * sigma_lambda_t_reverse**self.v\n            )\n            z_t = torch.normal(mu_tilde, std)\n\n    yield x_tilde_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/#chuchichaestli.diffusion.ddpm.cfg_ddpm.CFGDDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>args</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        args: Additional arguments.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    u = torch.rand(x_t.shape, device=self.device)\n    lam = -2 * torch.log(torch.tan(self.a * u + self.b))\n    alpha_square = torch.sigmoid(lam)\n    alpha = torch.sqrt(alpha_square)\n    sigma = torch.sqrt(1 - alpha_square)\n    noise = self.sample_noise(x_t.shape)\n\n    zeros = torch.zeros_like(condition)\n    condition = torch.where(\n        torch.bernoulli(self.p * torch.ones_like(x_t)).type(torch.bool),\n        zeros,\n        condition,\n    )\n    x_t = alpha * x_t + sigma * noise\n    x_t = torch.cat([condition, x_t], dim=1)\n\n    return x_t, noise, lam\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/cfg_ddpm/#chuchichaestli.diffusion.ddpm.cfg_ddpm.CFGDDPM.sigma","title":"sigma  <code>staticmethod</code>","text":"<pre><code>sigma(lambda_t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute sigma from lambda.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/cfg_ddpm.py</code> <pre><code>@staticmethod\ndef sigma(lambda_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute sigma from lambda.\"\"\"\n    alpha_square = torch.sigmoid(lambda_t)\n    return torch.sqrt(1 - alpha_square)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddim/","title":"chuchichaestli.diffusion.ddpm.ddim","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/ddim/#chuchichaestli.diffusion.ddpm.ddim","title":"chuchichaestli.diffusion.ddpm.ddim","text":"<p>Implementation of the Denoising Diffusion Implicit Model.</p> <p>Classes:</p> Name Description <code>DDIM</code> <p>Denoising Diffusion Implicit Model (DDIM).</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddim/#chuchichaestli.diffusion.ddpm.ddim.DDIM","title":"DDIM","text":"<pre><code>DDIM(\n    num_timesteps: int,\n    num_sample_steps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>Denoising Diffusion Implicit Model (DDIM).</p> <p>As described in the paper: \"Denoising Diffusion Implicit Models\" by Song et al. (2020); see https://arxiv.org/abs/2010.02502.</p> <p>Initialize the DDIM scheme.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of timesteps.</p> required <code>num_sample_steps</code> <code>int</code> <p>Number of timesteps to sample.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Annealing schedule</p> <code>'linear'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the DDPM model using DDIM.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddim.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    num_sample_steps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDIM scheme.\n\n    Args:\n        num_timesteps: Number of timesteps.\n        num_sample_steps: Number of timesteps to sample.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use.\n        schedule: Annealing schedule\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        num_timesteps, beta_start, beta_end, device, schedule, **kwargs\n    )\n    # Reverse the timesteps for sampling\n    self.tau = torch.linspace(\n        self.num_time_steps - 1, 0, num_sample_steps, dtype=int\n    ).to(device)  # Select timesteps in reverse order\n\n    # Precompute alpha_cumprod_prev\n    self.alpha_cumprod_prev = torch.cat(\n        [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]]\n    )\n    self.sqrt_alpha_cumprod_prev = torch.sqrt(self.alpha_cumprod_prev)\n    self.sqrt_1m_alpha_cumprod_prev = torch.sqrt(1.0 - self.alpha_cumprod_prev)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddim/#chuchichaestli.diffusion.ddpm.ddim.DDIM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Tensor | Generator[Tensor, None, torch.Tensor]\n</code></pre> <p>Sample from the DDPM model using DDIM.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddim.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; Tensor | Generator[Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the DDPM model using DDIM.\"\"\"\n    if condition is not None:\n        c = (\n            condition.unsqueeze(0)\n            .expand(n, *condition.shape)\n            .reshape(-1, *condition.shape[1:])\n        )\n        x_tau = self.sample_noise(c.shape).to(self.device)\n    elif shape is not None:\n        x_tau = self.sample_noise((n, *shape)).to(self.device)\n    else:\n        raise ValueError(\"Either condition or shape must be provided.\")\n    coef_shape = [-1] + [1] * (x_tau.dim() - 1)\n\n    for idx, t in enumerate(self.tau):\n        t = int(t)\n        if condition is not None:\n            eps_tau = model(torch.cat([c, x_tau], dim=1), t)\n        else:\n            eps_tau = model(x_tau, t)\n        x_0_hat = (\n            x_tau - self.sqrt_1m_alpha_cumprod[t] * eps_tau\n        ) / self.sqrt_alpha_cumprod[t].reshape(coef_shape)\n\n        x_tau = (\n            self.sqrt_alpha_cumprod_prev[t] * x_0_hat\n            + self.sqrt_1m_alpha_cumprod_prev[t] * eps_tau\n        )\n        if yield_intermediate:\n            yield x_tau\n\n    yield x_tau\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddpm/","title":"chuchichaestli.diffusion.ddpm.ddpm","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/ddpm/#chuchichaestli.diffusion.ddpm.ddpm","title":"chuchichaestli.diffusion.ddpm.ddpm","text":"<p>Implementation of the Diffusion Probabilistic Model (DDPM) noise process.</p> <p>Classes:</p> Name Description <code>DDPM</code> <p>Diffusion Probabilistic Model (DDPM) noise process.</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddpm/#chuchichaestli.diffusion.ddpm.ddpm.DDPM","title":"DDPM","text":"<pre><code>DDPM(\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Diffusion Probabilistic Model (DDPM) noise process.</p> <p>As described in the paper: \"Denoising Diffusion Probabilistic Models\" by Ho et al. (2020); see https://arxiv.org/abs/2006.11239.</p> <p>Initialize the DDPM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the DDPM algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    self.num_time_steps = num_timesteps\n    self.beta = SCHEDULES[schedule](beta_start, beta_end, num_timesteps, device)\n    self.alpha = 1.0 - self.beta\n    self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n    self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n    self.sqrt_1m_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n    self.coef_inner = (1 - self.alpha) / self.sqrt_1m_alpha_cumprod\n    self.coef_outer = 1.0 / torch.sqrt(self.alpha)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddpm/#chuchichaestli.diffusion.ddpm.ddpm.DDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.</p> <code>None</code> <code>shape</code> <code>tuple[int, ...] | None</code> <p>Shape of the generated tensor. Only used if condition is None.</p> <code>None</code> <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor | None = None,\n    shape: tuple[int, ...] | None = None,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, set to None and use the shape parameter instead.\n        shape: Shape of the generated tensor. Only used if condition is None.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    if condition is not None:\n        c = (\n            condition.unsqueeze(0)\n            .expand(n, *condition.shape)\n            .reshape(-1, *condition.shape[1:])\n        )\n        x_t = self.sample_noise(c.shape)\n    elif shape is not None:\n        x_t = self.sample_noise((n,) + shape)\n    else:\n        raise ValueError(\"Either condition or shape must be provided.\")\n    coef_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        if condition is not None:\n            eps_t = model(torch.cat([c, x_t], dim=1), t)\n        else:\n            eps_t = model(x_t, t)\n        coef_inner_t = self.coef_inner[t].reshape(coef_shape)\n        coef_outer_t = self.coef_outer[t].reshape(coef_shape)\n        x_tm1 = coef_outer_t * (x_t - coef_inner_t * eps_t)\n\n        if t &gt; 0:\n            noise = self.sample_noise(x_t.shape)\n            sigma_t = self.beta[t] ** 0.5\n            x_t = x_tm1 + sigma_t * noise\n            if yield_intermediate:\n                yield x_t\n\n    yield x_tm1\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/ddpm/#chuchichaestli.diffusion.ddpm.ddpm.DDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>timesteps</code> <code>torch.Tensor | None</code> <p>Optional timesteps to sample noise for (will be sampled randomly if None).</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/ddpm.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        timesteps: Optional timesteps to sample noise for (will be sampled randomly if None).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x_t = (\n            x_t.unsqueeze(0)\n            .expand(len(timesteps), *x_t.shape)\n            .reshape(-1, *x_t.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_t.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_t.shape[0])\n\n    noise = self.sample_noise(x_t.shape)\n\n    s_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    s1 = self.sqrt_alpha_cumprod[timesteps].reshape(s_shape)\n    s2 = self.sqrt_1m_alpha_cumprod[timesteps].reshape(s_shape)\n\n    x_t = s1 * x_t + s2 * noise\n    if condition is not None:\n        x_t = torch.cat([condition, x_t], dim=1)\n\n    return x_t, noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/indi/","title":"chuchichaestli.diffusion.ddpm.indi","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/indi/#chuchichaestli.diffusion.ddpm.indi","title":"chuchichaestli.diffusion.ddpm.indi","text":"<p>Implementation of Inversion by Direct Iteration (InDI).</p> <p>Classes:</p> Name Description <code>InDI</code> <p>Degradation process for inversion by direct iteration (InDI).</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/indi/#chuchichaestli.diffusion.ddpm.indi.InDI","title":"InDI","text":"<pre><code>InDI(\n    num_timesteps: int,\n    epsilon: float | torch.Tensor = 0.01,\n    device: str = \"cpu\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DiffusionProcess</code></p> <p>Degradation process for inversion by direct iteration (InDI).</p> <p>As described in the paper: \"Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration\" by Delbracio and Milanfar (2023); see https://arxiv.org/abs/2303.11435.</p> <p>Initialize the InDI algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>epsilon</code> <code>float | torch.Tensor</code> <p>Noise level. Can be a scalar or a tensor of shape (num_timesteps,). For eps_t = eps_0 / sqrt(t), the noise perturbation is a pure Brownian motion. For eps = eps_0 = cst, the InDI scheme is recovered.</p> <code>0.01</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> <code>sample_timesteps</code> <p>Sample timesteps for the InDI diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    epsilon: float | torch.Tensor = 0.01,\n    device: str = \"cpu\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the InDI algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        epsilon: Noise level. Can be a scalar or a tensor of shape (num_timesteps,).\n            For eps_t = eps_0 / sqrt(t), the noise perturbation is a pure Brownian motion.\n            For eps = eps_0 = cst, the InDI scheme is recovered.\n        device: Device to use for the computation.\n        kwargs: Additional keyword arguments.\n\n    \"\"\"\n    super().__init__(timesteps=num_timesteps, device=device, **kwargs)\n    self.num_time_steps = num_timesteps\n    self.delta = 1.0 / num_timesteps\n    if isinstance(epsilon, float):\n        self.epsilon = torch.full((num_timesteps,), epsilon, device=device)\n    else:\n        self.epsilon = epsilon\n        self.epsilon.to(device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/indi/#chuchichaestli.diffusion.ddpm.indi.InDI.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n\n    x_t = c + self.epsilon[-1] * self.sample_noise(c.shape)\n    for i in reversed(range(1, self.num_time_steps)):\n        eps_t = self.epsilon[i]\n        eps_tmdelta = self.epsilon[i - 1]\n\n        t = i / self.num_time_steps\n        noise = self.sample_noise(c.shape)\n\n        x_tmdelta = (\n            (self.delta / t) * model(x_t, i)\n            + (1 - self.delta / t) * x_t\n            + (t - self.delta) * torch.sqrt(eps_tmdelta**2 - eps_t**2) * noise\n        )\n        x_t = x_tmdelta\n        if yield_intermediate:\n            yield x_t\n\n    yield x_tmdelta\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/indi/#chuchichaestli.diffusion.ddpm.indi.InDI.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>High quality sample, tensor of shape (batch_size, *).</p> required <code>y</code> <code>torch.Tensor</code> <p>Corresponding low quality sample, tensor of shape (batch_size, *).</p> required <code>timesteps</code> <code>torch.Tensor | None</code> <p>Timesteps to sample noise from. If None, timesteps are sampled.</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def noise_step(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x: High quality sample, tensor of shape (batch_size, *).\n        y: Corresponding low quality sample, tensor of shape (batch_size, *).\n        timesteps: Timesteps to sample noise from. If None, timesteps are sampled.\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x = (\n            x.unsqueeze(0)\n            .expand(len(timesteps), *x.shape)\n            .reshape(-1, *x.shape[1:])\n        )\n        timesteps = (\n            timesteps.repeat_interleave(x.shape[0] // len(timesteps)) * self.delta\n        )\n    else:\n        timesteps = self.sample_timesteps(x.shape[0])\n    timesteps = timesteps.view(-1, *([1] * (x.dim() - 1)))\n    x_t = (1 - timesteps) * x + timesteps * y\n    # TODO: Verify that this is correct.\n    noise = x_t - x\n\n    return x_t, noise, timesteps.view(-1)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/indi/#chuchichaestli.diffusion.ddpm.indi.InDI.sample_timesteps","title":"sample_timesteps","text":"<pre><code>sample_timesteps(batch_size: int) -&gt; torch.Tensor\n</code></pre> <p>Sample timesteps for the InDI diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples to generate.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Tensor of shape (batch_size,) with the sampled timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/indi.py</code> <pre><code>def sample_timesteps(self, batch_size: int) -&gt; torch.Tensor:\n    \"\"\"Sample timesteps for the InDI diffusion process.\n\n    Args:\n        batch_size: Number of samples to generate.\n\n    Returns:\n        Tensor of shape (batch_size,) with the sampled timesteps.\n    \"\"\"\n    return (\n        torch.randint(0, self.num_time_steps, (batch_size,), device=self.device)\n        * self.delta\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/prior_grad/","title":"chuchichaestli.diffusion.ddpm.prior_grad","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/prior_grad/#chuchichaestli.diffusion.ddpm.prior_grad","title":"chuchichaestli.diffusion.ddpm.prior_grad","text":"<p>Implementation of the PriorGrad noise process.</p> <p>Classes:</p> Name Description <code>PriorGrad</code> <p>PriorGrad noise process.</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/prior_grad/#chuchichaestli.diffusion.ddpm.prior_grad.PriorGrad","title":"PriorGrad","text":"<pre><code>PriorGrad(\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>PriorGrad noise process.</p> <p>As described in the paper: \"PriorGrad: Improving conditional denoising diffusion models with data-dependent adataptive prior\" by Lee et al. (2021); see https://arxiv.org/abs/2106.06406.</p> <p>Initialize the PriorGrad algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float | torch.Tensor</code> <p>Mean value of the noise process.</p> required <code>scale</code> <code>float | torch.Tensor</code> <p>Scale value of the noise process.</p> required <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>Noise step for the diffusion process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def __init__(\n    self,\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor,\n    num_timesteps: int,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    **kwargs,\n):\n    \"\"\"Initialize the PriorGrad algorithm.\n\n    Args:\n        mean: Mean value of the noise process.\n        scale: Scale value of the noise process.\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    scale = scale.to(device)\n    mean = mean.to(device)\n    distr = NormalDistribution(0, scale, device=device)\n    super().__init__(\n        noise_distribution=distr,\n        num_timesteps=num_timesteps,\n        beta_start=beta_start,\n        beta_end=beta_end,\n        device=device,\n        schedule=schedule,\n        **kwargs,\n    )\n    self.mean = mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/prior_grad/#chuchichaestli.diffusion.ddpm.prior_grad.PriorGrad.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    c = (\n        condition.unsqueeze(0)\n        .expand(n, *condition.shape)\n        .reshape(-1, *condition.shape[1:])\n    )\n    x_t = self.sample_noise(c.shape)\n    coef_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        eps_t = model(torch.cat([c, x_t], dim=1), t)\n        coef_inner_t = self.coef_inner[t].reshape(coef_shape)\n        coef_outer_t = self.coef_outer[t].reshape(coef_shape)\n        x_tm1 = coef_outer_t * (x_t - coef_inner_t * eps_t)\n\n        if t &gt; 0:\n            noise = self.sample_noise(x_t.shape)\n            sigma_t = self.beta[t] ** 0.5\n            x_t = x_tm1 + sigma_t * noise\n            if yield_intermediate:\n                yield x_t\n\n    yield x_tm1 + self.mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/prior_grad/#chuchichaestli.diffusion.ddpm.prior_grad.PriorGrad.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Noise step for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>torch.Tensor</code> <p>Tensor of shape (batch_size, *).</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>timesteps</code> <code>torch.Tensor | None</code> <p>Optional timesteps to sample noise for (will be sampled randomly if None).</p> <code>None</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the sampled tensor, noise tensor and timesteps.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/prior_grad.py</code> <pre><code>def noise_step(\n    self,\n    x_t: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    timesteps: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Noise step for the diffusion process.\n\n    Args:\n        x_t: Tensor of shape (batch_size, *).\n        condition: Tensor of shape (batch_size, *).\n        timesteps: Optional timesteps to sample noise for (will be sampled randomly if None).\n        *args: Additional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of the sampled tensor, noise tensor and timesteps.\n    \"\"\"\n    if timesteps is not None:\n        x_t = (\n            x_t.unsqueeze(0)\n            .expand(len(timesteps), *x_t.shape)\n            .reshape(-1, *x_t.shape[1:])\n        )\n        timesteps = timesteps.repeat_interleave(x_t.shape[0] // len(timesteps))\n    else:\n        timesteps = self.sample_timesteps(x_t.shape[0])\n    noise = self.sample_noise(x_t.shape)\n\n    s_shape = [-1] + [1] * (x_t.dim() - 1)\n\n    s1 = self.sqrt_alpha_cumprod[timesteps].reshape(s_shape)\n    s2 = self.sqrt_1m_alpha_cumprod[timesteps].reshape(s_shape)\n\n    x_t = s1 * (x_t - self.mean) + s2 * noise\n\n    return torch.cat([condition, x_t], dim=1), noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/shift_ddpm/","title":"chuchichaestli.diffusion.ddpm.shift_ddpm","text":""},{"location":"reference/chuchichaestli/diffusion/ddpm/shift_ddpm/#chuchichaestli.diffusion.ddpm.shift_ddpm","title":"chuchichaestli.diffusion.ddpm.shift_ddpm","text":"<p>Implementation of the Shifted Diffusion Probabilistic Model (ShiftDDPM) noise process.</p> <p>Classes:</p> Name Description <code>ShiftDDPM</code> <p>Shift DDPM noise process that improves conditioned DDPMs.</p>"},{"location":"reference/chuchichaestli/diffusion/ddpm/shift_ddpm/#chuchichaestli.diffusion.ddpm.shift_ddpm.ShiftDDPM","title":"ShiftDDPM","text":"<pre><code>ShiftDDPM(\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    E: torch.Module = None,\n)\n</code></pre> <p>               Bases: <code>DDPM</code></p> <p>Shift DDPM noise process that improves conditioned DDPMs.</p> <p>Intended for the use if latent diffusion, but could probably be used for other tasks as well.</p> <p>As described in the paper: \"ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories\" by Zhang et al. (2023); see https://arxiv.org/abs/2302.02373.</p> <p>Initialize the ShiftDDPM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num_timesteps</code> <code>int</code> <p>Number of time steps in the diffusion process.</p> required <code>beta_start</code> <code>float</code> <p>Start value for beta.</p> <code>0.0001</code> <code>beta_end</code> <code>float</code> <p>End value for beta.</p> <code>0.02</code> <code>device</code> <code>str</code> <p>Device to use for the computation.</p> <code>'cpu'</code> <code>schedule</code> <code>str</code> <p>Schedule for beta.</p> <code>'linear'</code> <code>E</code> <code>torch.Module</code> <p>Prior shift predictor.</p> <code>None</code> <p>Methods:</p> Name Description <code>generate</code> <p>Sample from the diffusion process.</p> <code>noise_step</code> <p>A single step of the noise process.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/shift_ddpm.py</code> <pre><code>def __init__(\n    self,\n    num_timesteps: int,\n    beta_start: float = 0.0001,\n    beta_end: float = 0.02,\n    device: str = \"cpu\",\n    schedule: str = \"linear\",\n    E: torch.Module = None,\n) -&gt; None:\n    \"\"\"Initialize the ShiftDDPM algorithm.\n\n    Args:\n        num_timesteps: Number of time steps in the diffusion process.\n        beta_start: Start value for beta.\n        beta_end: End value for beta.\n        device: Device to use for the computation.\n        schedule: Schedule for beta.\n        E: Prior shift predictor.\n    \"\"\"\n    super().__init__(num_timesteps, beta_start, beta_end, device, schedule)\n    self.k = self.sqrt_alpha_cumprod * (1 - self.sqrt_alpha_cumprod)\n    self.E = E\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/shift_ddpm/#chuchichaestli.diffusion.ddpm.shift_ddpm.ShiftDDPM.generate","title":"generate","text":"<pre><code>generate(\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; (\n    torch.Tensor\n    | Generator[torch.Tensor, None, torch.Tensor]\n)\n</code></pre> <p>Sample from the diffusion process.</p> <p>Samples n times the number of conditions from the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Model to use for sampling.</p> required <code>E</code> <p>Prior shift predictor.</p> required <code>condition</code> <code>torch.Tensor</code> <p>Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.</p> required <code>n</code> <code>int</code> <p>Number of samples to generate (batch size).</p> <code>1</code> <code>yield_intermediate</code> <code>bool</code> <p>Yield intermediate results. This turns the function into a generator.</p> <code>False</code> <code>*args</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword</p> <code>{}</code> Source code in <code>src/chuchichaestli/diffusion/ddpm/shift_ddpm.py</code> <pre><code>def generate(\n    self,\n    model: Any,\n    condition: torch.Tensor,\n    n: int = 1,\n    yield_intermediate: bool = False,\n    *args,\n    **kwargs,\n) -&gt; torch.Tensor | Generator[torch.Tensor, None, torch.Tensor]:\n    \"\"\"Sample from the diffusion process.\n\n    Samples n times the number of conditions from the diffusion process.\n\n    Args:\n        model: Model to use for sampling.\n        E: Prior shift predictor.\n        condition: Tensor to condition generation on. For unconditional generation, supply a zero-tensor of the sample shape.\n        n: Number of samples to generate (batch size).\n        yield_intermediate: Yield intermediate results. This turns the function into a generator.\n        *args: Additional arguments.\n        **kwargs: Additional keyword\n    \"\"\"\n    if n &gt; 1:\n        condition = condition.repeat(n, 1)\n    E = self.E(condition)\n    x_t = self.sample_noise(condition.shape)\n\n    for t in reversed(range(0, self.num_time_steps)):\n        eps_t = model(torch.cat([condition, x_t], dim=1), t)\n        z = self.sample_noise(x_t.shape) if t &gt; 1 else 0.0\n        s_t = self.k[t] * E\n        s_tm1 = self.k[t - 1] * E\n        x_tm1 = self.coef_outer[t] * (\n            x_t - (self.beta[t] / self.sqrt_1m_alpha_cumprod[t]) * eps_t\n        )\n        x_tm1 -= (\n            torch.sqrt(self.alpha[t])(1 - self.alpha_cumprod[t - 1])\n            / (1 - self.alpha_cumprod[t])\n        ) * s_t + s_tm1\n        x_tm1 += (\n            torch.sqrt(\n                ((1 - self.alpha_cumprod[t - 1]) / (1 - self.alpha_cumprod[t]))\n                * self.beta[t]\n            )\n            * z\n        )\n        x_t = x_tm1\n        if yield_intermediate:\n            yield x_t\n    return x_t\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/ddpm/shift_ddpm/#chuchichaestli.diffusion.ddpm.shift_ddpm.ShiftDDPM.noise_step","title":"noise_step","text":"<pre><code>noise_step(\n    x_0: torch.Tensor,\n    condition: torch.Tensor | None = None,\n    *args,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>A single step of the noise process.</p> <p>Parameters:</p> Name Type Description Default <code>x_0</code> <code>torch.Tensor</code> <p>Ground truth input tensor</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>Tensor of shape (batch_size, *).</p> <code>None</code> <code>args</code> <p>Additional arguments</p> <code>()</code> <code>kwargs</code> <p>Additional keyword</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Tuple of the noised tensor, noise, and the timestep.</p> Source code in <code>src/chuchichaestli/diffusion/ddpm/shift_ddpm.py</code> <pre><code>def noise_step(\n    self, x_0: torch.Tensor, condition: torch.Tensor | None = None, *args, **kwargs\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"A single step of the noise process.\n\n    Args:\n        x_0: Ground truth input tensor\n        condition: Tensor of shape (batch_size, *).\n        args: Additional arguments\n        kwargs: Additional keyword\n\n    Returns:\n        Tuple of the noised tensor, noise, and the timestep.\n    \"\"\"\n    E = self.E(condition)\n    timesteps = self.sample_timesteps(x_0.shape[0])\n    noise = self.sample_noise(x_0.shape)\n    s_t = self.k[timesteps] * E\n    x_t = (\n        self.sqrt_alpha_cumprod[timesteps] * x_0\n        + s_t\n        + self.coef_inner[timesteps] * noise\n    )\n    noise = (\n        x_t - self.sqrt_alpha_cumprod[timesteps] * x_0\n    ) / self.sqrt_1m_alpha_cumprod[timesteps]\n    return x_t, noise, timesteps\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/distributions/","title":"chuchichaestli.diffusion.distributions","text":""},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions","title":"chuchichaestli.diffusion.distributions","text":"<p>Auxiliary classes for sampling noise from different distributions.</p> <p>Classes:</p> Name Description <code>DistributionAdapter</code> <p>Base class for distribution adapters.</p> <code>HalfNormalDistribution</code> <p>Half normal distribution adapter.</p> <code>NormalDistribution</code> <p>Normal distribution adapter.</p>"},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions.DistributionAdapter","title":"DistributionAdapter","text":"<pre><code>DistributionAdapter(device: str = 'cpu')\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for distribution adapters.</p> <p>Initialize the distribution adapter.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Sample noise from the distribution.</p> Source code in <code>src/chuchichaestli/diffusion/distributions.py</code> <pre><code>def __init__(self, device: str = \"cpu\") -&gt; None:\n    \"\"\"Initialize the distribution adapter.\"\"\"\n    self.device = device\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions.DistributionAdapter.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(shape: torch.Size) -&gt; torch.Tensor\n</code></pre> <p>Sample noise from the distribution.</p> Source code in <code>src/chuchichaestli/diffusion/distributions.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self,\n    shape: torch.Size,\n) -&gt; torch.Tensor:\n    \"\"\"Sample noise from the distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions.HalfNormalDistribution","title":"HalfNormalDistribution","text":"<pre><code>HalfNormalDistribution(\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor = 1.0,\n    device: str = \"cpu\",\n)\n</code></pre> <p>               Bases: <code>DistributionAdapter</code></p> <p>Half normal distribution adapter.</p> <p>Initialize the half normal distribution adapter.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Sample noise from the distribution.</p> Source code in <code>src/chuchichaestli/diffusion/distributions.py</code> <pre><code>def __init__(\n    self,\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor = 1.0,\n    device: str = \"cpu\",\n) -&gt; None:\n    \"\"\"Initialize the half normal distribution adapter.\"\"\"\n    super().__init__(device)\n    self.mean = torch.tensor(mean, device=device)\n    self.scale = torch.tensor(scale, device=device)\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions.HalfNormalDistribution.__call__","title":"__call__","text":"<pre><code>__call__(shape: torch.Size) -&gt; torch.Tensor\n</code></pre> <p>Sample noise from the distribution.</p> Source code in <code>src/chuchichaestli/diffusion/distributions.py</code> <pre><code>def __call__(\n    self,\n    shape: torch.Size,\n) -&gt; torch.Tensor:\n    \"\"\"Sample noise from the distribution.\"\"\"\n    return torch.randn(shape, device=self.device).abs() * self.scale + self.mean\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions.NormalDistribution","title":"NormalDistribution","text":"<pre><code>NormalDistribution(\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor = 1.0,\n    device: str = \"cpu\",\n)\n</code></pre> <p>               Bases: <code>DistributionAdapter</code></p> <p>Normal distribution adapter.</p> <p>Initialize the normal distribution adapter.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Sample noise from the distribution.</p> Source code in <code>src/chuchichaestli/diffusion/distributions.py</code> <pre><code>def __init__(\n    self,\n    mean: float | torch.Tensor,\n    scale: float | torch.Tensor = 1.0,\n    device: str = \"cpu\",\n) -&gt; None:\n    \"\"\"Initialize the normal distribution adapter.\"\"\"\n    super().__init__(device)\n    self.mean = mean\n    self.scale = scale\n</code></pre>"},{"location":"reference/chuchichaestli/diffusion/distributions/#chuchichaestli.diffusion.distributions.NormalDistribution.__call__","title":"__call__","text":"<pre><code>__call__(shape: torch.Size) -&gt; torch.Tensor\n</code></pre> <p>Sample noise from the distribution.</p> Source code in <code>src/chuchichaestli/diffusion/distributions.py</code> <pre><code>def __call__(\n    self,\n    shape: torch.Size,\n) -&gt; torch.Tensor:\n    \"\"\"Sample noise from the distribution.\"\"\"\n    noise = torch.randn(shape, device=self.device)\n    return noise * self.scale + self.mean\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/","title":"chuchichaestli.metrics","text":""},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics","title":"chuchichaestli.metrics","text":"<p>Metrics module of chuchichaestli: various loss &amp; evaluation metric implementations.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base evaluation metric class and utility functions implementation.</p> <code>fid</code> <p>FID evaluation metric, including InceptionV3 for feature mapping.</p> <code>functional</code> <p>Functional image quality metric implementations.</p> <code>lpips</code> <p>LPIPS loss metric, including various feature extraction backbones.</p> <code>mse</code> <p>MSE evaluation metric.</p> <code>psnr</code> <p>Peak-Signal-to-Noise Ratio evaluation metric.</p> <code>ssim</code> <p>Structural Similarity Index Measure evaluation metric and loss.</p> <p>Classes:</p> Name Description <code>FID</code> <p>Frechet inception distance.</p> <code>LPIPSLoss</code> <p>LPIPS loss implementation.</p> <code>MSE</code> <p>Mean-squared error.</p> <code>PSNR</code> <p>Peak-signal-to-noise ratio.</p> <code>SSIM</code> <p>Structural similarity index measure.</p> <code>SSIMLoss</code> <p>Structural similarity index measure loss function.</p>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.FID","title":"FID","text":"<pre><code>FID(\n    model: Module | None = None,\n    feature_dim: int | None = None,\n    device: torch.device | None = None,\n    n_images: int = 0,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>EvalMetric</code></p> <p>Frechet inception distance.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model from which to extract features.</p> <code>None</code> <code>feature_dim</code> <code>int | None</code> <p>Feature dimension of the model output; if <code>None</code>, the feature dimension is determined automatically if possible).</p> <code>None</code> <code>device</code> <code>torch.device | None</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>n_images</code> <code>int</code> <p>Number of images seen by the internal state.</p> <code>0</code> <code>kwargs</code> <p>Additional keyword arguments (passed to parent class).</p> <code>{}</code> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>reset</code> <p>Reset the current metrics state.</p> <code>to</code> <p>Perform tensor device conversion for all internal tensors.</p> <code>update</code> <p>Compute metric on new input and update current state.</p> <p>Attributes:</p> Name Type Description <code>feature_dim</code> <p>Feature dimension of the output.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def __init__(\n    self,\n    model: Module | None = None,\n    feature_dim: int | None = None,\n    device: torch.device | None = None,\n    n_images: int = 0,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        model: Model from which to extract features.\n        feature_dim: Feature dimension of the model output; if `None`, the\n          feature dimension is determined automatically if possible).\n        device: Tensor allocation/computation device.\n        n_images: Number of images seen by the internal state.\n        kwargs: Additional keyword arguments (passed to parent class).\n    \"\"\"\n    super().__init__(device=device, n_images=n_images, **kwargs)\n    if model is None:\n        model = FIDInceptionV3()\n    self.model = model.to(self.device)\n    self.model.eval()\n    self.model.requires_grad_(False)\n    self._feature_dim = feature_dim\n    self.n_images_fake = torch.tensor(n_images // 2, device=self.device)\n    self.n_images_real = torch.tensor(n_images - n_images // 2, device=self.device)\n    self.aggregate_fake = torch.zeros(self.feature_dim, device=self.device)\n    self.aggregate_real = torch.zeros(self.feature_dim, device=self.device)\n    self.aggregate_cov_fake = torch.zeros(\n        (self.feature_dim, self.feature_dim), device=self.device\n    )\n    self.aggregate_cov_real = torch.zeros(\n        (self.feature_dim, self.feature_dim), device=self.device\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.FID.feature_dim","title":"feature_dim  <code>property</code>","text":"<pre><code>feature_dim\n</code></pre> <p>Feature dimension of the output.</p>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.FID.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    if (self.n_images_fake &lt; 1) or (self.n_images_real &lt; 1):\n        warnings.warn(\n            \"Computing FID requires at least 1 real images and 1 fake images.\"\n        )\n        return torch.tensor(0.0)\n    mean_fake = (self.aggregate_fake / self.n_images_fake).unsqueeze(0)\n    mean_real = (self.aggregate_real / self.n_images_real).unsqueeze(0)\n    n_cov_fake = self.aggregate_cov_fake - self.n_images_fake * torch.matmul(\n        mean_fake.T, mean_fake\n    )\n    cov_fake = n_cov_fake / max(self.n_images_fake - 1, 1)\n    n_cov_real = self.aggregate_cov_real - self.n_images_real * torch.matmul(\n        mean_real.T, mean_real\n    )\n    cov_real = n_cov_real / max(self.n_images_real - 1, 1)\n    return self._calculate_frechet_distance(\n        mean_real.squeeze(), cov_real, mean_fake.squeeze(), cov_fake\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.FID.reset","title":"reset","text":"<pre><code>reset(**kwargs)\n</code></pre> <p>Reset the current metrics state.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def reset(self, **kwargs):\n    \"\"\"Reset the current metrics state.\"\"\"\n    self.__init__(\n        min_value=self.min_value.item(),\n        max_value=self.max_value.item(),\n        device=self.device,\n        model=self.model,\n        feature_dim=self._feature_dim,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.FID.to","title":"to","text":"<pre><code>to(device: torch.device = None)\n</code></pre> <p>Perform tensor device conversion for all internal tensors.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>Tensor allocation/computation device.</p> <code>None</code> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def to(self, device: torch.device = None):\n    \"\"\"Perform tensor device conversion for all internal tensors.\n\n    Args:\n        device: Tensor allocation/computation device.\n    \"\"\"\n    super().to(device)\n    self.model = self.model.to(device)\n    self.n_images_fake = self.n_images_fake.to(device=device)\n    self.n_images_real = self.n_images_real.to(device=device)\n    self.aggregate_fake = self.aggregate_fake.to(device=device)\n    self.aggregate_real = self.aggregate_real.to(device=device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.FID.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor | None = None,\n    prediction: torch.Tensor | None = None,\n    **kwargs,\n)\n</code></pre> <p>Compute metric on new input and update current state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor | None</code> <p>Observed (real) data.</p> <code>None</code> <code>prediction</code> <code>torch.Tensor | None</code> <p>Predicted (fake) data.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments for parent class.</p> <code>{}</code> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self,\n    data: torch.Tensor | None = None,\n    prediction: torch.Tensor | None = None,\n    **kwargs,\n):\n    \"\"\"Compute metric on new input and update current state.\n\n    Args:\n        data: Observed (real) data.\n        prediction: Predicted (fake) data.\n        kwargs: Additional keyword arguments for parent class.\n    \"\"\"\n    sample = kwargs.pop(\"sample\", 0)\n    # fake images\n    if prediction is not None:\n        if prediction.ndim == 5:\n            prediction = as_batched_slices(prediction, sample=sample)\n        super().update(prediction, prediction, **kwargs)\n        features_fake = self.model(prediction)\n        self.n_images_fake += prediction.shape[0]\n        self.aggregate_fake += torch.sum(features_fake, dim=0)\n        self.aggregate_cov_fake += torch.matmul(features_fake.T, features_fake)\n    # real images\n    if data is not None:\n        if data.ndim == 5:\n            data = as_batched_slices(data, sample=sample)\n        super().update(data, data, **kwargs)\n        features_real = self.model(data)\n        self.n_images_real += data.shape[0]\n        self.aggregate_real += torch.sum(features_real, dim=0)\n        self.aggregate_cov_real += torch.matmul(features_real.T, features_real)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.LPIPSLoss","title":"LPIPSLoss","text":"<pre><code>LPIPSLoss(\n    model: FeatureExtractor\n    | Literal[\n        \"vgg16\",\n        \"vgg\",\n        \"alexnet\",\n        \"squeezenet\",\n        \"resnet18\",\n        \"resnet\",\n        \"convnext\",\n        \"vit\",\n        \"swinv2\",\n    ]\n    | None = None,\n    embedding: LPIPSEmbedding | bool | None = True,\n    pretrained_weights: Path | str | None = None,\n    finetune: bool = False,\n    reduction: Callable | None = torch.mean,\n    device: torch.device | None = None,\n    spatial_average: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>LPIPS loss implementation.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>FeatureExtractor | Literal['vgg16', 'vgg', 'alexnet', 'squeezenet', 'resnet18', 'resnet', 'convnext', 'vit', 'swinv2'] | None</code> <p>Model from which to extract features; either pre-initialized or one of [\"vgg16\", \"vgg\", \"alexnet\", \"squeezenet\", \"resnet18\", \"resnet\", \"convnext\", \"vit\", \"swinv2\"].</p> <code>None</code> <code>embedding</code> <code>LPIPSEmbedding | bool | None</code> <p>Linear embedding layers for feature mapping. An embedding can be passed directly, but needs to use as many layers as features are extracted from the inputs. If <code>True</code>, linear embedding layers are used to calculate the loss, if <code>False</code>, weighted sum across channel dimensions is used, if <code>None</code>, the raw latent least-squares are used.</p> <code>True</code> <code>pretrained_weights</code> <code>Path | str | None</code> <p>weights for the embedding layers to be loaded. If <code>None</code>, the embedding layers are randomly initialized.</p> <code>None</code> <code>finetune</code> <code>bool</code> <p>If <code>False</code>, freeze gradients of the embedding.</p> <code>False</code> <code>reduction</code> <code>Callable | None</code> <p>Reduction function, e.g. <code>torch.mean</code> or <code>torch.sum</code>.</p> <code>torch.mean</code> <code>device</code> <code>torch.device | None</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>spatial_average</code> <code>bool</code> <p>If <code>True</code>, averages across image dimensions.</p> <code>True</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward method.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(\n    self,\n    model: FeatureExtractor\n    | Literal[\n        \"vgg16\",\n        \"vgg\",\n        \"alexnet\",\n        \"squeezenet\",\n        \"resnet18\",\n        \"resnet\",\n        \"convnext\",\n        \"vit\",\n        \"swinv2\",\n    ]\n    | None = None,\n    embedding: LPIPSEmbedding | bool | None = True,\n    pretrained_weights: Path | str | None = None,\n    finetune: bool = False,\n    reduction: Callable | None = torch.mean,\n    device: torch.device | None = None,\n    spatial_average: bool = True,\n):\n    \"\"\"Constructor.\n\n    Args:\n        model: Model from which to extract features; either pre-initialized or one of\n          [\"vgg16\", \"vgg\", \"alexnet\", \"squeezenet\", \"resnet18\", \"resnet\",\n           \"convnext\", \"vit\", \"swinv2\"].\n        embedding: Linear embedding layers for feature mapping.\n          An embedding can be passed directly, but needs to use as many layers as\n          features are extracted from the inputs.\n          If `True`, linear embedding layers are used to calculate the loss,\n          if `False`, weighted sum across channel dimensions is used,\n          if `None`, the raw latent least-squares are used.\n        pretrained_weights: weights for the embedding layers to be loaded.\n          If `None`, the embedding layers are randomly initialized.\n        finetune: If `False`, freeze gradients of the embedding.\n        reduction: Reduction function, e.g. `torch.mean` or `torch.sum`.\n        device: Tensor allocation/computation device.\n        spatial_average: If `True`, averages across image dimensions.\n    \"\"\"\n    super().__init__()\n    device = torch.get_default_device() if device is None else device\n    if model is None:\n        model = LPIPSVGG16()\n    elif isinstance(model, str):\n        match model.lower():\n            case \"vgg16\" | \"vgg\":\n                model = LPIPSVGG16()\n            case \"alexnet\":\n                model = LPIPSAlexNet()\n            case \"squeezenet\":\n                model = LPIPSSqueezeNet()\n            case \"resnet18\" | \"resnet\":\n                model = LPIPSResNet18()\n            case \"convnext\":\n                model = LPIPSConvNeXt()\n            case \"vit\":\n                model = LPIPSViT()\n            case \"swinv2\":\n                model = LPIPSSwinV2()\n            case _:\n                model = LPIPSVGG16()\n    self.model = model.to(device)\n    self.model.eval()\n    self.model.requires_grad_(False)\n    if isinstance(embedding, bool) or embedding is None:\n        embedding = (\n            LPIPSEmbedding(self.model.feature_channels)\n            if embedding\n            else LPIPSNonEmbedding(self.model.feature_channels)\n        )\n    if pretrained_weights is not None:\n        embedding.load(pretrained_weights)\n    self.embedding = embedding.to(device)\n    if not finetune and self.embedding:\n        self.embedding.eval()\n        # self.embedding.requires_grad_(False)\n    else:\n        self.embedding.train()\n    self.spatial_average = spatial_average\n    self.reduction = reduction if reduction is not None else torch.nn.Identity()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.LPIPSLoss.forward","title":"forward","text":"<pre><code>forward(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    as_scores: bool = False,\n    **kwargs,\n) -&gt; torch.Tensor | Sequence[torch.Tensor]\n</code></pre> <p>Forward method.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def forward(\n    self, x1: torch.Tensor, x2: torch.Tensor, as_scores: bool = False, **kwargs\n) -&gt; torch.Tensor | Sequence[torch.Tensor]:\n    \"\"\"Forward method.\"\"\"\n    if x1.shape != x2.shape:\n        raise ValueError(f\"Input shapes must match, got {x1.shape} and {x2.shape}\")\n    if x1.ndim == 5:\n        sample = kwargs.get(\"sample\", 0)\n        x1 = as_batched_slices(x1, sample=sample)\n        x2 = as_batched_slices(x2, sample=sample)\n    reduction = kwargs.get(\"reduction\", self.reduction)\n    if reduction is None:\n        reduction = torch.nn.Identity()\n    x1, x2 = sanitize_ndim(x1), sanitize_ndim(x2)\n    x1, x2 = as_tri_channel(x1), as_tri_channel(x2)\n    w, h = x1.shape[2:]\n    feat1, feat2 = self._normalize(self.model(x1)), self._normalize(self.model(x2))\n    scores = [(f1 - f2) ** 2 for f1, f2 in zip(feat1, feat2)]\n    scores = self.embedding(scores)\n    scores = (\n        self._upsample(scores, size=(w, h))\n        if not self.spatial_average\n        else self._image_average(scores)\n    )\n    if as_scores:\n        return [reduction(s.squeeze()) for s in scores]\n    loss = sum(scores).squeeze()\n    return reduction(loss)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.MSE","title":"MSE","text":"<pre><code>MSE(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>EvalMetric</code></p> <p>Mean-squared error.</p> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>update</code> <p>Compute metric on new input and update current state.</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    self.device = torch.get_default_device() if device is None else device\n    self.is_nan = torch.tensor(False, device=self.device)\n    self.nan_count = torch.tensor(0, device=self.device)\n    self.min_value = torch.tensor(min_value, device=self.device)\n    self.max_value = torch.tensor(max_value, device=self.device)\n    self.n_observations = torch.tensor(n_observations, device=self.device)\n    self.n_images = torch.tensor(n_images, device=self.device)\n    self.value = torch.tensor(0, device=self.device)\n    self.aggregate = torch.tensor(0, device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.MSE.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/mse.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    if self.n_observations == 0:\n        return None\n    mse = self.aggregate / self.n_observations\n    self.value = mse\n    return mse.item()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.MSE.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = False,\n)\n</code></pre> <p>Compute metric on new input and update current state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>False</code> Source code in <code>src/chuchichaestli/metrics/mse.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = False\n):\n    \"\"\"Compute metric on new input and update current state.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    super().update(data, prediction, update_range=update_range)\n    square_error = torch.pow(data[~self.is_nan] - prediction[~self.is_nan], 2)\n    # sum square errors from batch\n    aggregate_batch = torch.sum(square_error)\n    self.aggregate = self.aggregate + aggregate_batch\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.PSNR","title":"PSNR","text":"<pre><code>PSNR(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>MSE</code></p> <p>Peak-signal-to-noise ratio.</p> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>update</code> <p>Compute metric on new input and update current state.</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    self.device = torch.get_default_device() if device is None else device\n    self.is_nan = torch.tensor(False, device=self.device)\n    self.nan_count = torch.tensor(0, device=self.device)\n    self.min_value = torch.tensor(min_value, device=self.device)\n    self.max_value = torch.tensor(max_value, device=self.device)\n    self.n_observations = torch.tensor(n_observations, device=self.device)\n    self.n_images = torch.tensor(n_images, device=self.device)\n    self.value = torch.tensor(0, device=self.device)\n    self.aggregate = torch.tensor(0, device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.PSNR.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/psnr.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    mse = super().compute()\n    if mse is None:\n        return None\n    elif mse == 0:\n        return torch.inf\n    psnr = 10 * torch.log10(torch.pow(self.data_range, 2) / mse)\n    self.value = psnr\n    return psnr.item()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.PSNR.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = True,\n)\n</code></pre> <p>Compute metric on new input and update current state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>True</code> Source code in <code>src/chuchichaestli/metrics/psnr.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = True\n):\n    \"\"\"Compute metric on new input and update current state.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    super().update(data, prediction, update_range=update_range)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.SSIM","title":"SSIM","text":"<pre><code>SSIM(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device = None,\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    kernel_type: Literal[\n        \"gaussian\", \"uniform\"\n    ] = \"gaussian\",\n    k1: float = 0.01,\n    k2: float = 0.03,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>EvalMetric</code></p> <p>Structural similarity index measure.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Minimum data value relative to metric computation.</p> <code>0</code> <code>max_value</code> <code>float</code> <p>Maximum data value relative to metric computation.</p> <code>1</code> <code>n_observations</code> <code>int</code> <p>Number of observations (pixels) seen by the internal state.</p> <code>0</code> <code>n_images</code> <code>int</code> <p>Number of images seen by the internal state.</p> <code>0</code> <code>device</code> <code>torch.device</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>kernel_size</code> <code>int | Sequence[int]</code> <p>Size of the moving window aka kernel.</p> <code>11</code> <code>kernel_sigma</code> <code>float | Sequence[float]</code> <p>Standard deviation for the Gaussian kernel.</p> <code>1.5</code> <code>kernel_type</code> <code>Literal['gaussian', 'uniform']</code> <p>Type of kernel; one of <code>['gaussian', 'uniform']</code>.</p> <code>'gaussian'</code> <code>k1</code> <code>float</code> <p>Algorithm parameter, K1 (small constant).</p> <code>0.01</code> <code>k2</code> <code>float</code> <p>Algorithm parameter, K1 (small constant).</p> <code>0.03</code> <code>kwargs</code> <p>Base class keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>reset</code> <p>Reset the current metrics state.</p> <code>update</code> <p>Compute metric, aggregate, and update internal state.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device = None,\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    kernel_type: Literal[\"gaussian\", \"uniform\"] = \"gaussian\",\n    k1: float = 0.01,\n    k2: float = 0.03,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kernel_size: Size of the moving window aka kernel.\n        kernel_sigma: Standard deviation for the Gaussian kernel.\n        kernel_type: Type of kernel; one of `['gaussian', 'uniform']`.\n        k1: Algorithm parameter, K1 (small constant).\n        k2: Algorithm parameter, K1 (small constant).\n        kwargs: Base class keyword arguments.\n    \"\"\"\n    super().__init__(\n        min_value=min_value,\n        max_value=max_value,\n        n_observations=n_observations,\n        n_images=n_images,\n        device=device,\n    )\n    self.kernel_type = kernel_type\n    self.kernel_size = kernel_size\n    self.kernel_sigma = kernel_sigma\n    self.k1 = k1\n    self.k2 = k2\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.SSIM.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    if self.n_images == 0:\n        return None\n    ssim = self.aggregate / self.n_images\n    self.value = ssim\n    return ssim.item()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.SSIM.reset","title":"reset","text":"<pre><code>reset(**kwargs) -&gt; float\n</code></pre> <p>Reset the current metrics state.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def reset(self, **kwargs) -&gt; float:\n    \"\"\"Reset the current metrics state.\"\"\"\n    self.__init__(\n        min_value=self.min_value.item(),\n        max_value=self.max_value.item(),\n        device=self.device,\n        kernel_size=self.kernel_size,\n        kernel_sigma=self.kernel_sigma,\n        kernel_type=self.kernel_type,\n        k1=self.k1,\n        k2=self.k2,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.SSIM.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = True,\n)\n</code></pre> <p>Compute metric, aggregate, and update internal state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>True</code> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = True\n):\n    \"\"\"Compute metric, aggregate, and update internal state.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    super().update(data, prediction, update_range=update_range)\n    ssim_full, _ = self._compute_ssim_and_cs(\n        data,\n        prediction,\n        self.kernel_size,\n        self.kernel_sigma,\n        self.kernel_type,\n        self.data_range,\n        self.k1,\n        self.k2,\n    )\n    batch_size = ssim_full.shape[0]\n    aggregate_batch = ssim_full.view(batch_size, -1).nanmean(1, keepdim=True)\n    self.aggregate = self.aggregate + torch.sum(aggregate_batch)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.SSIMLoss","title":"SSIMLoss","text":"<pre><code>SSIMLoss(\n    data_range: float = 1.0,\n    kernel_type: Literal[\n        \"gaussian\", \"uniform\"\n    ] = \"gaussian\",\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    k1: float = 0.01,\n    k2: float = 0.03,\n    reduction: Callable | None = torch.mean,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Structural similarity index measure loss function.</p> <p>Constructor.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Compute the SSIM loss.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 1.0,\n    kernel_type: Literal[\"gaussian\", \"uniform\"] = \"gaussian\",\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    k1: float = 0.01,\n    k2: float = 0.03,\n    reduction: Callable | None = torch.mean,\n):\n    \"\"\"Constructor.\"\"\"\n    super().__init__()\n    self.data_range = data_range\n    self.kernel_type = kernel_type\n    self.kernel_size = kernel_size\n    self.kernel_sigma = kernel_sigma\n    self.k1 = k1\n    self.k2 = k2\n    self.reduction = reduction if reduction is not None else torch.nn.Identity()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/#chuchichaestli.metrics.SSIMLoss.forward","title":"forward","text":"<pre><code>forward(\n    data: torch.Tensor, prediction: torch.Tensor, **kwargs\n) -&gt; torch.Tensor\n</code></pre> <p>Compute the SSIM loss.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>kwargs</code> <p>Additional keyword arguments, e.g., <code>reduction</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Loss value.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def forward(\n    self, data: torch.Tensor, prediction: torch.Tensor, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Compute the SSIM loss.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        kwargs: Additional keyword arguments, e.g., `reduction`.\n\n    Returns:\n        Loss value.\n    \"\"\"\n    reduction = kwargs.get(\"reduction\", self.reduction)\n    if reduction is None:\n        reduction = torch.nn.Identity()\n    ssim_full, _ = SSIM._compute_ssim_and_cs(\n        data,\n        prediction,\n        kernel_size=self.kernel_size,\n        kernel_sigma=self.kernel_sigma,\n        kernel_type=self.kernel_type,\n        data_range=self.data_range,\n        k1=self.k1,\n        k2=self.k2,\n    )\n    batch_size = ssim_full.shape[0]\n    ssim_batch = ssim_full.view(batch_size, -1).nanmean(1, keepdim=True)\n    loss = 1 - ssim_batch\n    return reduction(loss)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/","title":"chuchichaestli.metrics.base","text":""},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base","title":"chuchichaestli.metrics.base","text":"<p>Base evaluation metric class and utility functions implementation.</p> <p>Classes:</p> Name Description <code>EvalMetric</code> <p>Base class for image evaluation metrics.</p> <p>Functions:</p> Name Description <code>as_batched_slices</code> <p>Convert batches of volumetric 5D tensors into 4D slice-wise image tensors.</p> <code>as_tri_channel</code> <p>Morph input to resemble a three-channel image.</p> <code>sanitize_ndim</code> <p>Standardize image dimensionality to (B, C, W, H).</p>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.EvalMetric","title":"EvalMetric","text":"<pre><code>EvalMetric(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n)\n</code></pre> <p>Base class for image evaluation metrics.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Minimum data value relative to metric computation.</p> <code>0</code> <code>max_value</code> <code>float</code> <p>Maximum data value relative to metric computation.</p> <code>1</code> <code>n_observations</code> <code>int</code> <p>Number of observations (pixels) seen by the internal state.</p> <code>0</code> <code>n_images</code> <code>int</code> <p>Number of images seen by the internal state.</p> <code>0</code> <code>device</code> <code>torch.device | None</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>reset</code> <p>Reset internal state of the metric (keeps range values).</p> <code>to</code> <p>Perform tensor device conversion for all internal tensors.</p> <code>update</code> <p>Compute metric, aggregate, and update internal state.</p> <p>Attributes:</p> Name Type Description <code>data_range</code> <code>torch.Tensor</code> <p>Data range of the metric.</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    self.device = torch.get_default_device() if device is None else device\n    self.is_nan = torch.tensor(False, device=self.device)\n    self.nan_count = torch.tensor(0, device=self.device)\n    self.min_value = torch.tensor(min_value, device=self.device)\n    self.max_value = torch.tensor(max_value, device=self.device)\n    self.n_observations = torch.tensor(n_observations, device=self.device)\n    self.n_images = torch.tensor(n_images, device=self.device)\n    self.value = torch.tensor(0, device=self.device)\n    self.aggregate = torch.tensor(0, device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.EvalMetric.data_range","title":"data_range  <code>property</code> <code>writable</code>","text":"<pre><code>data_range: torch.Tensor\n</code></pre> <p>Data range of the metric.</p>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.EvalMetric.reset","title":"reset","text":"<pre><code>reset(**kwargs)\n</code></pre> <p>Reset internal state of the metric (keeps range values).</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def reset(self, **kwargs):\n    \"\"\"Reset internal state of the metric (keeps range values).\"\"\"\n    self.__init__(\n        min_value=self.min_value.item(),\n        max_value=self.max_value.item(),\n        device=self.device,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.EvalMetric.to","title":"to","text":"<pre><code>to(device: torch.device = None)\n</code></pre> <p>Perform tensor device conversion for all internal tensors.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>Tensor allocation/computation device.</p> <code>None</code> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def to(self, device: torch.device = None):\n    \"\"\"Perform tensor device conversion for all internal tensors.\n\n    Args:\n        device: Tensor allocation/computation device.\n    \"\"\"\n    self.device = device\n    self.is_nan = self.is_nan.to(device=self.device)\n    self.nan_count = self.nan_count.to(device=self.device)\n    self.min_value = self.min_value.to(device=self.device)\n    self.max_value = self.max_value.to(device=self.device)\n    self.n_observations = self.n_observations.to(device=self.device)\n    self.n_images = self.n_images.to(device=self.device)\n    self.value = self.value.to(device=self.device)\n    self.aggregate = self.aggregate.to(device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.EvalMetric.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = True,\n)\n</code></pre> <p>Compute metric, aggregate, and update internal state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data aka target.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data aka inferred target.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>True</code> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = True\n):\n    \"\"\"Compute metric, aggregate, and update internal state.\n\n    Args:\n        data: Observed data aka target.\n        prediction: Predicted data aka inferred target.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    if self.device != prediction.device:\n        self.to(prediction.device)\n    self.is_nan = torch.isnan(data) | torch.isnan(prediction)\n    self.nan_count += torch.sum(self.is_nan)\n    self.n_images += torch.tensor(prediction.shape[0], device=self.device)\n    self.n_observations += torch.tensor(\n        prediction[~self.is_nan].numel(), device=self.device\n    )\n    if update_range:\n        self.min_value = torch.minimum(\n            prediction[~self.is_nan].min(), self.min_value\n        )\n        self.min_value = torch.minimum(data[~self.is_nan].min(), self.min_value)\n        self.max_value = torch.maximum(\n            prediction[~self.is_nan].max(), self.max_value\n        )\n        self.max_value = torch.maximum(data[~self.is_nan].max(), self.max_value)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.as_batched_slices","title":"as_batched_slices","text":"<pre><code>as_batched_slices(\n    x: torch.Tensor, sample: int = 0\n) -&gt; torch.Tensor\n</code></pre> <p>Convert batches of volumetric 5D tensors into 4D slice-wise image tensors.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Volumetric 5D input tensor.</p> required <code>sample</code> <code>int</code> <p>If <code>&gt; 0</code>, the volume depth is sampled <code>sample</code> times from the centre.</p> <code>0</code> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def as_batched_slices(x: torch.Tensor, sample: int = 0) -&gt; torch.Tensor:\n    \"\"\"Convert batches of volumetric 5D tensors into 4D slice-wise image tensors.\n\n    Args:\n        x: Volumetric 5D input tensor.\n        sample: If `&gt; 0`, the volume depth is sampled `sample` times from the centre.\n    \"\"\"\n    if x.ndim == 5:\n        B, C, W, H, D = x.shape\n        if sample &gt; 0:\n            sample = min(sample, D)\n            center = D // 2\n            window = sample // 2\n            start = center - window\n            end = start + sample\n            if sample % 2 == 0:\n                start = center - window\n                end = center + window\n            x = x[..., start:end]\n            D = sample\n        x = x.permute(0, 4, 1, 2, 3).contiguous().view(B * D, C, W, H)\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.as_tri_channel","title":"as_tri_channel","text":"<pre><code>as_tri_channel(x: torch.Tensor)\n</code></pre> <p>Morph input to resemble a three-channel image.</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def as_tri_channel(x: torch.Tensor):\n    \"\"\"Morph input to resemble a three-channel image.\"\"\"\n    if x.shape[1] == 1:\n        x = x.repeat(1, 3, 1, 1)\n    elif x.shape[1] &lt; 3:\n        x = x[:, 0:1, :, :].repeat(1, 3, 1, 1)\n    if x.shape[1] &gt; 3:\n        raise ValueError(f\"Input has more than three channels ({x.shape[1]})!\")\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/base/#chuchichaestli.metrics.base.sanitize_ndim","title":"sanitize_ndim","text":"<pre><code>sanitize_ndim(\n    x: torch.Tensor,\n    check_2D: bool = True,\n    check_3D: bool = False,\n)\n</code></pre> <p>Standardize image dimensionality to (B, C, W, H).</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def sanitize_ndim(x: torch.Tensor, check_2D: bool = True, check_3D: bool = False):\n    \"\"\"Standardize image dimensionality to (B, C, W, H).\"\"\"\n    if x.ndim == 3:\n        x = x.unsqueeze(0)\n    if x.ndim == 2:\n        x = x.unsqueeze(0).unsqueeze(0)\n    if check_2D and check_3D and (x.ndim != 4 and x.ndim != 5):\n        raise ValueError(\n            f\"Require input of shape {'(C, W, H) or (B, C, W, H)' if check_2D else ''}\"\n            f\"{' or (B, C, W, H, D)' if check_3D else ''}.\"\n        )\n    elif check_3D and not check_2D and x.ndim != 5:\n        raise ValueError(\"Require input of shape (B, C, W, H, D).\")\n    elif check_2D and not check_3D and x.ndim != 4:\n        raise ValueError(\"Require input of shape (C, W, H) or (B, C, W, H).\")\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/","title":"chuchichaestli.metrics.fid","text":""},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid","title":"chuchichaestli.metrics.fid","text":"<p>FID evaluation metric, including InceptionV3 for feature mapping.</p> <p>Classes:</p> Name Description <code>FID</code> <p>Frechet inception distance.</p> <code>FIDInceptionV3</code> <p>InceptionV3 model for calculating FIDs.</p>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FID","title":"FID","text":"<pre><code>FID(\n    model: Module | None = None,\n    feature_dim: int | None = None,\n    device: torch.device | None = None,\n    n_images: int = 0,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>EvalMetric</code></p> <p>Frechet inception distance.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model from which to extract features.</p> <code>None</code> <code>feature_dim</code> <code>int | None</code> <p>Feature dimension of the model output; if <code>None</code>, the feature dimension is determined automatically if possible).</p> <code>None</code> <code>device</code> <code>torch.device | None</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>n_images</code> <code>int</code> <p>Number of images seen by the internal state.</p> <code>0</code> <code>kwargs</code> <p>Additional keyword arguments (passed to parent class).</p> <code>{}</code> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>reset</code> <p>Reset the current metrics state.</p> <code>to</code> <p>Perform tensor device conversion for all internal tensors.</p> <code>update</code> <p>Compute metric on new input and update current state.</p> <p>Attributes:</p> Name Type Description <code>feature_dim</code> <p>Feature dimension of the output.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def __init__(\n    self,\n    model: Module | None = None,\n    feature_dim: int | None = None,\n    device: torch.device | None = None,\n    n_images: int = 0,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        model: Model from which to extract features.\n        feature_dim: Feature dimension of the model output; if `None`, the\n          feature dimension is determined automatically if possible).\n        device: Tensor allocation/computation device.\n        n_images: Number of images seen by the internal state.\n        kwargs: Additional keyword arguments (passed to parent class).\n    \"\"\"\n    super().__init__(device=device, n_images=n_images, **kwargs)\n    if model is None:\n        model = FIDInceptionV3()\n    self.model = model.to(self.device)\n    self.model.eval()\n    self.model.requires_grad_(False)\n    self._feature_dim = feature_dim\n    self.n_images_fake = torch.tensor(n_images // 2, device=self.device)\n    self.n_images_real = torch.tensor(n_images - n_images // 2, device=self.device)\n    self.aggregate_fake = torch.zeros(self.feature_dim, device=self.device)\n    self.aggregate_real = torch.zeros(self.feature_dim, device=self.device)\n    self.aggregate_cov_fake = torch.zeros(\n        (self.feature_dim, self.feature_dim), device=self.device\n    )\n    self.aggregate_cov_real = torch.zeros(\n        (self.feature_dim, self.feature_dim), device=self.device\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FID.feature_dim","title":"feature_dim  <code>property</code>","text":"<pre><code>feature_dim\n</code></pre> <p>Feature dimension of the output.</p>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FID.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    if (self.n_images_fake &lt; 1) or (self.n_images_real &lt; 1):\n        warnings.warn(\n            \"Computing FID requires at least 1 real images and 1 fake images.\"\n        )\n        return torch.tensor(0.0)\n    mean_fake = (self.aggregate_fake / self.n_images_fake).unsqueeze(0)\n    mean_real = (self.aggregate_real / self.n_images_real).unsqueeze(0)\n    n_cov_fake = self.aggregate_cov_fake - self.n_images_fake * torch.matmul(\n        mean_fake.T, mean_fake\n    )\n    cov_fake = n_cov_fake / max(self.n_images_fake - 1, 1)\n    n_cov_real = self.aggregate_cov_real - self.n_images_real * torch.matmul(\n        mean_real.T, mean_real\n    )\n    cov_real = n_cov_real / max(self.n_images_real - 1, 1)\n    return self._calculate_frechet_distance(\n        mean_real.squeeze(), cov_real, mean_fake.squeeze(), cov_fake\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FID.reset","title":"reset","text":"<pre><code>reset(**kwargs)\n</code></pre> <p>Reset the current metrics state.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def reset(self, **kwargs):\n    \"\"\"Reset the current metrics state.\"\"\"\n    self.__init__(\n        min_value=self.min_value.item(),\n        max_value=self.max_value.item(),\n        device=self.device,\n        model=self.model,\n        feature_dim=self._feature_dim,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FID.to","title":"to","text":"<pre><code>to(device: torch.device = None)\n</code></pre> <p>Perform tensor device conversion for all internal tensors.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>Tensor allocation/computation device.</p> <code>None</code> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def to(self, device: torch.device = None):\n    \"\"\"Perform tensor device conversion for all internal tensors.\n\n    Args:\n        device: Tensor allocation/computation device.\n    \"\"\"\n    super().to(device)\n    self.model = self.model.to(device)\n    self.n_images_fake = self.n_images_fake.to(device=device)\n    self.n_images_real = self.n_images_real.to(device=device)\n    self.aggregate_fake = self.aggregate_fake.to(device=device)\n    self.aggregate_real = self.aggregate_real.to(device=device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FID.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor | None = None,\n    prediction: torch.Tensor | None = None,\n    **kwargs,\n)\n</code></pre> <p>Compute metric on new input and update current state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor | None</code> <p>Observed (real) data.</p> <code>None</code> <code>prediction</code> <code>torch.Tensor | None</code> <p>Predicted (fake) data.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments for parent class.</p> <code>{}</code> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self,\n    data: torch.Tensor | None = None,\n    prediction: torch.Tensor | None = None,\n    **kwargs,\n):\n    \"\"\"Compute metric on new input and update current state.\n\n    Args:\n        data: Observed (real) data.\n        prediction: Predicted (fake) data.\n        kwargs: Additional keyword arguments for parent class.\n    \"\"\"\n    sample = kwargs.pop(\"sample\", 0)\n    # fake images\n    if prediction is not None:\n        if prediction.ndim == 5:\n            prediction = as_batched_slices(prediction, sample=sample)\n        super().update(prediction, prediction, **kwargs)\n        features_fake = self.model(prediction)\n        self.n_images_fake += prediction.shape[0]\n        self.aggregate_fake += torch.sum(features_fake, dim=0)\n        self.aggregate_cov_fake += torch.matmul(features_fake.T, features_fake)\n    # real images\n    if data is not None:\n        if data.ndim == 5:\n            data = as_batched_slices(data, sample=sample)\n        super().update(data, data, **kwargs)\n        features_real = self.model(data)\n        self.n_images_real += data.shape[0]\n        self.aggregate_real += torch.sum(features_real, dim=0)\n        self.aggregate_cov_real += torch.matmul(features_real.T, features_real)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FIDInceptionV3","title":"FIDInceptionV3","text":"<pre><code>FIDInceptionV3(\n    weights: tv.Inception_V3_Weights | None = None,\n    use_default_transforms: bool = True,\n    mode: str = \"bilinear\",\n    antialias: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>InceptionV3 model for calculating FIDs.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>tv.Inception_V3_Weights | None</code> <p>The pretrained weights for the model; for details and possible values see https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights.</p> <code>None</code> <code>use_default_transforms</code> <code>bool</code> <p>If <code>True</code>, uses standard transforms for preprocessing (Inception_V3_Weights.IMAGENET1K_V1.transforms).</p> <code>True</code> <code>mode</code> <code>str</code> <p>If <code>use_default_transforms=False</code>, a simple interpolation in given mode is performed.</p> <code>'bilinear'</code> <code>antialias</code> <code>bool</code> <p>If <code>use_default_transforms=False</code> and <code>True</code>, antialiasing is used during interpolation.</p> <code>False</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward method for the FIDInceptionV3 model.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def __init__(\n    self,\n    weights: tv.Inception_V3_Weights | None = None,\n    use_default_transforms: bool = True,\n    mode: str = \"bilinear\",\n    antialias: bool = False,\n):\n    \"\"\"Constructor.\n\n    Args:\n        weights: The pretrained weights for the model; for details and possible values\n          see https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights.\n        use_default_transforms: If `True`, uses standard transforms for\n          preprocessing (Inception_V3_Weights.IMAGENET1K_V1.transforms).\n        mode: If `use_default_transforms=False`, a simple interpolation in\n          given mode is performed.\n        antialias: If `use_default_transforms=False` and `True`,\n          antialiasing is used during interpolation.\n    \"\"\"\n    super().__init__()\n    self.weights = (\n        tv.Inception_V3_Weights.IMAGENET1K_V1 if weights is None else weights\n    )\n    self.model = tv.inception_v3(weights=self.weights)\n    self.model.fc = torch.nn.Identity()\n    if use_default_transforms:\n        if isinstance(self.weights, str):\n            weights = tv.get_model_weights(\"inception_v3\")[self.weights]\n            self.transforms = weights.transforms()\n        else:\n            self.transforms = self.weights.transforms()\n    else:\n        self.transforms = partial(\n            interpolate,\n            size=(299, 299),\n            mode=mode,\n            align_corners=False,\n            antialias=antialias,\n        )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/fid/#chuchichaestli.metrics.fid.FIDInceptionV3.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward method for the FIDInceptionV3 model.</p> Source code in <code>src/chuchichaestli/metrics/fid.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward method for the FIDInceptionV3 model.\"\"\"\n    x = sanitize_ndim(x)\n    x = as_tri_channel(x)\n    x = self.transforms(x)\n    x = self.model(x)\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/functional/","title":"chuchichaestli.metrics.functional","text":""},{"location":"reference/chuchichaestli/metrics/functional/#chuchichaestli.metrics.functional","title":"chuchichaestli.metrics.functional","text":"<p>Functional image quality metric implementations.</p> <p>Functions:</p> Name Description <code>mse</code> <p>Compute mean-squared error (MSE) between observation and prediction.</p> <code>psnr</code> <p>Compute peak-signal-to-noise ratio between observation and prediction.</p> <code>psnr_from_batches</code> <p>Compute overall PSNR from individual PSNR values (optionally weighted by pixel counts).</p> <code>ssim</code> <p>Compute peak-signal-to-noise ratio between observation and prediction.</p>"},{"location":"reference/chuchichaestli/metrics/functional/#chuchichaestli.metrics.functional.lpips","title":"lpips","text":"<pre><code>lpips(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    model: Literal[\n        \"vgg16\",\n        \"vgg\",\n        \"alexnet\",\n        \"squeezenet\",\n        \"resnet18\",\n        \"resnet\",\n        \"convnext\",\n        \"vit\",\n        \"swinv2\",\n    ] = \"vgg16\",\n    embedding: bool = True,\n    embedding_weights: Path | str | None = None,\n    reduction: Callable | None = torch.mean,\n    spatial_average: bool = True,\n) -&gt; torch.Tensor\n</code></pre> <p>Compute LPIPS (Learned Perceptual Image Patch Similarity) between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>model</code> <code>Literal['vgg16', 'vgg', 'alexnet', 'squeezenet', 'resnet18', 'resnet', 'convnext', 'vit', 'swinv2']</code> <p>The type of network to use for LPIPS computation.</p> <code>'vgg16'</code> <code>embedding</code> <code>bool</code> <p>If <code>True</code>, uses linear embedding layers for feature mapping.</p> <code>True</code> <code>embedding_weights</code> <code>Path | str | None</code> <p>Path to pretrained weights for the embedding layers.</p> <code>None</code> <code>reduction</code> <code>Callable | None</code> <p>Reduction function, e.g. <code>torch.mean</code> or <code>torch.sum</code>.</p> <code>torch.mean</code> <code>spatial_average</code> <code>bool</code> <p>If <code>True</code>, averages across image dimensions.</p> <code>True</code> Source code in <code>src/chuchichaestli/metrics/functional.py</code> <pre><code>def lpips(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    model: Literal[\n        \"vgg16\",\n        \"vgg\",\n        \"alexnet\",\n        \"squeezenet\",\n        \"resnet18\",\n        \"resnet\",\n        \"convnext\",\n        \"vit\",\n        \"swinv2\",\n    ] = \"vgg16\",\n    embedding: bool = True,\n    embedding_weights: Path | str | None = None,\n    reduction: Callable | None = torch.mean,\n    spatial_average: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Compute LPIPS (Learned Perceptual Image Patch Similarity) between two tensors.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        model: The type of network to use for LPIPS computation.\n        embedding: If `True`, uses linear embedding layers for feature mapping.\n        embedding_weights: Path to pretrained weights for the embedding layers.\n        reduction: Reduction function, e.g. `torch.mean` or `torch.sum`.\n        spatial_average: If `True`, averages across image dimensions.\n    \"\"\"\n    lpips_fn = LPIPSLoss(\n        model=model,\n        embedding=embedding,\n        pretrained_weights=embedding_weights,\n        spatial_average=spatial_average,\n        reduction=reduction,\n    )\n    return lpips_fn(data, prediction)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/functional/#chuchichaestli.metrics.functional.mse","title":"mse","text":"<pre><code>mse(\n    data: torch.Tensor, prediction: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Compute mean-squared error (MSE) between observation and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required Source code in <code>src/chuchichaestli/metrics/functional.py</code> <pre><code>def mse(data: torch.Tensor, prediction: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute mean-squared error (MSE) between observation and prediction.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n    \"\"\"\n    data = sanitize_ndim(data, check_2D=True, check_3D=True)\n    prediction = sanitize_ndim(prediction, check_2D=True, check_3D=True)\n    is_nan = torch.isnan(data) | torch.isnan(prediction)\n    return torch.mean(torch.pow(data[~is_nan] - prediction[~is_nan], 2))\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/functional/#chuchichaestli.metrics.functional.psnr","title":"psnr","text":"<pre><code>psnr(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    data_range: float | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Compute peak-signal-to-noise ratio between observation and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>data_range</code> <code>float | None</code> <p>The data range of the images.</p> <code>None</code> Source code in <code>src/chuchichaestli/metrics/functional.py</code> <pre><code>def psnr(\n    data: torch.Tensor, prediction: torch.Tensor, data_range: float | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Compute peak-signal-to-noise ratio between observation and prediction.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        data_range: The data range of the images.\n    \"\"\"\n    if data_range is None:\n        data_range = 1.0\n    mean_squared_error = mse(data, prediction)\n    if mean_squared_error == 0:\n        return torch.inf\n    max2 = data_range**2\n    return 10 * torch.log10(max2 / mean_squared_error)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/functional/#chuchichaestli.metrics.functional.psnr_from_batches","title":"psnr_from_batches","text":"<pre><code>psnr_from_batches(\n    psnr_values: Sequence[torch.Tensor],\n    pixel_counts: Sequence[int] | None = None,\n    data_range: float | None = None,\n) -&gt; float\n</code></pre> <p>Compute overall PSNR from individual PSNR values (optionally weighted by pixel counts).</p> <p>Parameters:</p> Name Type Description Default <code>psnr_values</code> <code>Sequence[torch.Tensor]</code> <p>Pre-calculated PSNR values on individual batches.</p> required <code>pixel_counts</code> <code>Sequence[int] | None</code> <p>Pixel count weights if image sizes vary across batches (optional).</p> <code>None</code> <code>data_range</code> <code>float | None</code> <p>The data range of the image domain.</p> <code>None</code> Source code in <code>src/chuchichaestli/metrics/functional.py</code> <pre><code>def psnr_from_batches(\n    psnr_values: Sequence[torch.Tensor],\n    pixel_counts: Sequence[int] | None = None,\n    data_range: float | None = None,\n) -&gt; float:\n    \"\"\"Compute overall PSNR from individual PSNR values (optionally weighted by pixel counts).\n\n    Args:\n        psnr_values: Pre-calculated PSNR values on individual batches.\n        pixel_counts: Pixel count weights if image sizes vary across batches (optional).\n        data_range: The data range of the image domain.\n    \"\"\"\n    if pixel_counts is None:\n        pixel_counts = [1 for val in psnr_values]\n    if data_range is None:\n        data_range = 1.0\n    max2 = data_range**2\n    total_squared_error = 0.0\n    total_weight = 0\n    for psnr, w in zip(psnr_values, pixel_counts):\n        mse = max2 / (10 ** (psnr / 10))\n        total_squared_error += w * mse\n        total_weight += w\n    mean_squared_error = total_squared_error / total_weight\n    return 10 * torch.log10(max2 / mean_squared_error)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/functional/#chuchichaestli.metrics.functional.ssim","title":"ssim","text":"<pre><code>ssim(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    data_range: float = 1.0,\n    kernel_type: Literal[\n        \"gaussian\", \"uniform\"\n    ] = \"gaussian\",\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    k1: float = 0.01,\n    k2: float = 0.03,\n    reduction: Callable | None = torch.mean,\n) -&gt; float\n</code></pre> <p>Compute peak-signal-to-noise ratio between observation and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>data_range</code> <code>float</code> <p>The data range of the images.</p> <code>1.0</code> <code>kernel_size</code> <code>int | Sequence[int]</code> <p>The size of the kernel for computation.</p> <code>11</code> <code>kernel_sigma</code> <code>float | Sequence[float]</code> <p>The standard deviation of the kernel for computation.</p> <code>1.5</code> <code>kernel_type</code> <code>Literal['gaussian', 'uniform']</code> <p>The type of kernel for computation; one of <code>['gaussian', 'uniform']</code>.</p> <code>'gaussian'</code> <code>data_range</code> <code>float</code> <p>The data range of the images.</p> <code>1.0</code> <code>k1</code> <code>float</code> <p>the first stability constant.</p> <code>0.01</code> <code>k2</code> <code>float</code> <p>the second stability constant.</p> <code>0.03</code> <code>reduction</code> <code>Callable | None</code> <p>Reduction function, e.g. <code>torch.mean</code> or <code>torch.sum</code>.</p> <code>torch.mean</code> Source code in <code>src/chuchichaestli/metrics/functional.py</code> <pre><code>def ssim(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    data_range: float = 1.0,\n    kernel_type: Literal[\"gaussian\", \"uniform\"] = \"gaussian\",\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    k1: float = 0.01,\n    k2: float = 0.03,\n    reduction: Callable | None = torch.mean,\n) -&gt; float:\n    \"\"\"Compute peak-signal-to-noise ratio between observation and prediction.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        data_range: The data range of the images.\n        kernel_size: The size of the kernel for computation.\n        kernel_sigma: The standard deviation of the kernel for computation.\n        kernel_type: The type of kernel for computation; one of `['gaussian', 'uniform']`.\n        data_range: The data range of the images.\n        k1: the first stability constant.\n        k2: the second stability constant.\n        reduction: Reduction function, e.g. `torch.mean` or `torch.sum`.\n    \"\"\"\n    if data_range is None:\n        data_range = (\n            torch.maximum(data.max(), prediction.max()).item()\n            - torch.minimum(data.min(), prediction.min()).item()\n        )\n    if reduction is None:\n        reduction = torch.nn.Identity()\n    ssim_full, _ = SSIM._compute_ssim_and_cs(\n        data,\n        prediction,\n        kernel_size=kernel_size,\n        kernel_sigma=kernel_sigma,\n        kernel_type=kernel_type,\n        data_range=data_range,\n        k1=k1,\n        k2=k2,\n    )\n    batch_size = ssim_full.shape[0]\n    ssim_batch = ssim_full.view(batch_size, -1).nanmean(1, keepdim=True)\n    return reduction(ssim_batch)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/","title":"chuchichaestli.metrics.lpips","text":""},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips","title":"chuchichaestli.metrics.lpips","text":"<p>LPIPS loss metric, including various feature extraction backbones.</p> Note <p>The original implementation of LPIPS (https://arxiv.org/abs/1801.03924) uses a scaling layer, which adjusts the standardization based on the ImageNet stats <code>mean=[0.485, 0.456, 0.406]</code> and <code>std=[0.229, 0.224, 0.225]</code>. Note, these scalings are already included in the <code>FeatureExtractor</code> module via <code>torchvision.models.Weights.transforms()</code>.</p> <p>Classes:</p> Name Description <code>LPIPSAlexNet</code> <p>AlexNet feature extractor for LPIPSLoss.</p> <code>LPIPSConvNeXt</code> <p>ConvNeXt-Tiny feature extractor for LPIPSLoss.</p> <code>LPIPSEmbedding</code> <p>Linear deep embedding for LPIPS extracted features.</p> <code>LPIPSLoss</code> <p>LPIPS loss implementation.</p> <code>LPIPSResNet18</code> <p>ResNet18 feature extractor for LPIPSLoss.</p> <code>LPIPSSqueezeNet</code> <p>SqueezeNet feature extractor for LPIPSLoss.</p> <code>LPIPSSwinV2</code> <p>Small SwinTransformer-V2 feature extractor for LPIPSLoss.</p> <code>LPIPSVGG16</code> <p>VGG16 feature extractor for LPIPSLoss.</p> <code>LPIPSViT</code> <p>ViT-B-16 feature extractor for LPIPSLoss.</p>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.FeatureExtractor","title":"FeatureExtractor","text":"<pre><code>FeatureExtractor(\n    model_name: str,\n    default_weights: tv.Weights | str,\n    feature_nodes: dict,\n    weights: tv.Weights | str | None = None,\n    use_default_transforms: bool = True,\n    input_size: tuple[int, int] = (224, 224),\n    mode: str = \"bilinear\",\n    antialias: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Generic feature extractor base class.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model name of a feature extraction backbone.</p> required <code>default_weights</code> <code>tv.Weights | str</code> <p>The pretrained weights for the specific model; for details and possible values, see https://docs.pytorch.org/vision/main/_modules/torchvision/models/_api.html.</p> required <code>feature_nodes</code> <code>dict</code> <p>Features to be extracted; the dict keys are node names, and the values user-specified keys that should be sortable via <code>sorted</code>.</p> required <code>weights</code> <code>tv.Weights | str | None</code> <p>The user-specified pretrained weights in the child class.</p> <code>None</code> <code>use_default_transforms</code> <code>bool</code> <p>If <code>True</code>, uses standard transforms for preprocessing (VGG16_Weights.IMAGENET1K_V1.transforms).</p> <code>True</code> <code>input_size</code> <code>tuple[int, int]</code> <p>If <code>use_default_transforms=False</code>, this is the resize target size.</p> <code>(224, 224)</code> <code>mode</code> <code>str</code> <p>If <code>use_default_transforms=False</code>, a simple interpolation in given mode is performed.</p> <code>'bilinear'</code> <code>antialias</code> <code>bool</code> <p>If <code>use_default_transforms=False</code> and <code>True</code>, antialiasing is used during interpolation.</p> <code>False</code> <p>Methods:</p> Name Description <code>forward</code> <p>Feature extraction.</p> <p>Attributes:</p> Name Type Description <code>feature_channels</code> <p>Feature channels of the extractor's output.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    default_weights: tv.Weights | str,\n    feature_nodes: dict,\n    weights: tv.Weights | str | None = None,\n    use_default_transforms: bool = True,\n    input_size: tuple[int, int] = (224, 224),\n    mode: str = \"bilinear\",\n    antialias: bool = False,\n):\n    \"\"\"Constructor.\n\n    Args:\n        model_name: The model name of a feature extraction backbone.\n        default_weights: The pretrained weights for the specific model;\n          for details and possible values, see\n          https://docs.pytorch.org/vision/main/_modules/torchvision/models/_api.html.\n        feature_nodes: Features to be extracted; the dict keys are node names,\n          and the values user-specified keys that should be sortable via `sorted`.\n        weights: The user-specified pretrained weights in the child class.\n        use_default_transforms: If `True`, uses standard transforms for\n          preprocessing (VGG16_Weights.IMAGENET1K_V1.transforms).\n        input_size: If `use_default_transforms=False`, this is the resize target size.\n        mode: If `use_default_transforms=False`, a simple interpolation in\n          given mode is performed.\n        antialias: If `use_default_transforms=False` and `True`,\n          antialiasing is used during interpolation.\n    \"\"\"\n    self.extractor: GraphModule\n    super().__init__()\n    self.model_name = model_name\n    self.weights = default_weights if weights is None else weights\n    model = tv.get_model(model_name, weights=self.weights)\n    self.extractor = create_feature_extractor(model, feature_nodes)\n    self.input_size = input_size\n    if use_default_transforms:\n        if isinstance(self.weights, str):\n            weights = tv.get_model_weights(\"inception_v3\")[self.weights]\n            self.transforms = weights.transforms()\n        else:\n            self.transforms = self.weights.transforms()\n    else:\n        self.transforms = partial(\n            interpolate,\n            size=self.input_size,\n            mode=mode,\n            align_corners=False,\n            antialias=antialias,\n        )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.FeatureExtractor.feature_channels","title":"feature_channels  <code>property</code>","text":"<pre><code>feature_channels\n</code></pre> <p>Feature channels of the extractor's output.</p>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.FeatureExtractor.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor) -&gt; list[torch.Tensor]\n</code></pre> <p>Feature extraction.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"Feature extraction.\"\"\"\n    x = sanitize_ndim(x)\n    x = as_tri_channel(x)\n    x = self.transforms(x)\n    # Extract features\n    features = self.extractor(x)\n    outputs = []\n    for k in sorted(features.keys()):\n        feature = features[k]\n        # vision transformers flatten features\n        if feature.ndim == 3 and \"vit\" in self.model_name:\n            feature = feature[:, 1:, :]  # omit CLS token\n            B, N, D = feature.shape\n            H = W = int(N**0.5)\n            feature = feature.permute(0, 2, 1).contiguous().view(B, D, H, W)\n        # swin transformers are channel-last\n        elif \"swin\" in self.model_name:\n            feature = feature.permute(0, 3, 1, 2)\n        # elif feature.ndim == 2:\n        #     feature = feature.unsqueeze(-1).unsqueeze(-1)\n        outputs.append(feature)\n    return outputs\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSAlexNet","title":"LPIPSAlexNet","text":"<pre><code>LPIPSAlexNet(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>AlexNet feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"alexnet\",\n        default_weights=tv.AlexNet_Weights.IMAGENET1K_V1,\n        feature_nodes={\n            \"features.1\": 0,\n            \"features.4\": 1,\n            \"features.7\": 2,\n            \"features.9\": 3,\n            \"features.11\": 4,\n        },\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSConvNeXt","title":"LPIPSConvNeXt","text":"<pre><code>LPIPSConvNeXt(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>ConvNeXt-Tiny feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"convnext_tiny\",\n        default_weights=tv.ConvNeXt_Tiny_Weights.IMAGENET1K_V1,\n        feature_nodes={\n            \"features.1\": 0,\n            \"features.3\": 1,\n            \"features.5\": 2,\n            \"features.7\": 3,\n        },\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSEmbedding","title":"LPIPSEmbedding","text":"<pre><code>LPIPSEmbedding(\n    in_channels: list[int],\n    out_channels: int | list[int] = 1,\n    softplus: bool = False,\n    dropout: bool = False,\n    clamp_weights: bool = False,\n)\n</code></pre> <p>               Bases: <code>ModuleList</code></p> <p>Linear deep embedding for LPIPS extracted features.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Input channels for each linear embedding layer.</p> required <code>out_channels</code> <code>int | list[int]</code> <p>Output channels for each linear embedding layer.</p> <code>1</code> <code>softplus</code> <code>bool</code> <p>If <code>True</code>, uses a softplus activation after each embedding layer.</p> <code>False</code> <code>dropout</code> <code>bool</code> <p>If <code>True</code>, uses dropout in the embedding.</p> <code>False</code> <code>clamp_weights</code> <code>bool</code> <p>If <code>True</code>, the weights of the embedding layers are clamped; this is necessary to avoid negative values</p> <code>False</code> <p>Methods:</p> Name Description <code>clamp_weights</code> <p>Clamp weights of the convolutions (to avoid negative outputs).</p> <code>forward</code> <p>Forward method for a sequence of LPIPS feature loss tensors.</p> <code>load</code> <p>Load the model weights.</p> <code>save</code> <p>Save the model weights.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(\n    self,\n    in_channels: list[int],\n    out_channels: int | list[int] = 1,\n    softplus: bool = False,\n    dropout: bool = False,\n    clamp_weights: bool = False,\n):\n    \"\"\"Constructor.\n\n    Args:\n        in_channels: Input channels for each linear embedding layer.\n        out_channels: Output channels for each linear embedding layer.\n        softplus: If `True`, uses a softplus activation after each embedding layer.\n        dropout: If `True`, uses dropout in the embedding.\n        clamp_weights: If `True`, the weights of the embedding layers are clamped;\n          this is necessary to avoid negative values\n    \"\"\"\n    if not isinstance(out_channels, Iterable):\n        out_channels = [out_channels] * len(in_channels)\n    layers = []\n    for in_c, out_c in zip(in_channels, out_channels):\n        layer = LPIPSEmbeddingBlock(2, in_c, out_c, act=softplus, dropout=dropout)\n        layers.append(layer)\n    super().__init__(layers)\n    if clamp_weights:\n        self.clamp_weights()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSEmbedding.clamp_weights","title":"clamp_weights","text":"<pre><code>clamp_weights()\n</code></pre> <p>Clamp weights of the convolutions (to avoid negative outputs).</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def clamp_weights(self):\n    \"\"\"Clamp weights of the convolutions (to avoid negative outputs).\"\"\"\n    for module in self.modules():\n        if isinstance(module, torch.nn.Conv2d):\n            module.weight.data.clamp_(min=0.0)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    x: Sequence[torch.Tensor],\n) -&gt; Sequence[torch.Tensor]\n</code></pre> <p>Forward method for a sequence of LPIPS feature loss tensors.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def forward(self, x: Sequence[torch.Tensor]) -&gt; Sequence[torch.Tensor]:\n    \"\"\"Forward method for a sequence of LPIPS feature loss tensors.\"\"\"\n    assert len(x) == len(self)\n    scores = [layer(xi) for layer, xi in zip(self, x)]\n    return scores\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSEmbedding.load","title":"load","text":"<pre><code>load(path: Path | str = Path('lpips_embedding.pth'))\n</code></pre> <p>Load the model weights.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def load(self, path: Path | str = Path(\"lpips_embedding.pth\")):\n    \"\"\"Load the model weights.\"\"\"\n    path = Path(path)\n    if path.exists():\n        states = torch.load(path, weights_only=True)\n        self.load_state_dict(states)\n        self.eval()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSEmbedding.save","title":"save","text":"<pre><code>save(\n    path: Path | str = Path(\"lpips_embedding.pth\"),\n) -&gt; Path\n</code></pre> <p>Save the model weights.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def save(self, path: Path | str = Path(\"lpips_embedding.pth\")) -&gt; Path:\n    \"\"\"Save the model weights.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(self.state_dict(), path)\n    return path\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSLoss","title":"LPIPSLoss","text":"<pre><code>LPIPSLoss(\n    model: FeatureExtractor\n    | Literal[\n        \"vgg16\",\n        \"vgg\",\n        \"alexnet\",\n        \"squeezenet\",\n        \"resnet18\",\n        \"resnet\",\n        \"convnext\",\n        \"vit\",\n        \"swinv2\",\n    ]\n    | None = None,\n    embedding: LPIPSEmbedding | bool | None = True,\n    pretrained_weights: Path | str | None = None,\n    finetune: bool = False,\n    reduction: Callable | None = torch.mean,\n    device: torch.device | None = None,\n    spatial_average: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>LPIPS loss implementation.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>FeatureExtractor | Literal['vgg16', 'vgg', 'alexnet', 'squeezenet', 'resnet18', 'resnet', 'convnext', 'vit', 'swinv2'] | None</code> <p>Model from which to extract features; either pre-initialized or one of [\"vgg16\", \"vgg\", \"alexnet\", \"squeezenet\", \"resnet18\", \"resnet\", \"convnext\", \"vit\", \"swinv2\"].</p> <code>None</code> <code>embedding</code> <code>LPIPSEmbedding | bool | None</code> <p>Linear embedding layers for feature mapping. An embedding can be passed directly, but needs to use as many layers as features are extracted from the inputs. If <code>True</code>, linear embedding layers are used to calculate the loss, if <code>False</code>, weighted sum across channel dimensions is used, if <code>None</code>, the raw latent least-squares are used.</p> <code>True</code> <code>pretrained_weights</code> <code>Path | str | None</code> <p>weights for the embedding layers to be loaded. If <code>None</code>, the embedding layers are randomly initialized.</p> <code>None</code> <code>finetune</code> <code>bool</code> <p>If <code>False</code>, freeze gradients of the embedding.</p> <code>False</code> <code>reduction</code> <code>Callable | None</code> <p>Reduction function, e.g. <code>torch.mean</code> or <code>torch.sum</code>.</p> <code>torch.mean</code> <code>device</code> <code>torch.device | None</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>spatial_average</code> <code>bool</code> <p>If <code>True</code>, averages across image dimensions.</p> <code>True</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward method.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(\n    self,\n    model: FeatureExtractor\n    | Literal[\n        \"vgg16\",\n        \"vgg\",\n        \"alexnet\",\n        \"squeezenet\",\n        \"resnet18\",\n        \"resnet\",\n        \"convnext\",\n        \"vit\",\n        \"swinv2\",\n    ]\n    | None = None,\n    embedding: LPIPSEmbedding | bool | None = True,\n    pretrained_weights: Path | str | None = None,\n    finetune: bool = False,\n    reduction: Callable | None = torch.mean,\n    device: torch.device | None = None,\n    spatial_average: bool = True,\n):\n    \"\"\"Constructor.\n\n    Args:\n        model: Model from which to extract features; either pre-initialized or one of\n          [\"vgg16\", \"vgg\", \"alexnet\", \"squeezenet\", \"resnet18\", \"resnet\",\n           \"convnext\", \"vit\", \"swinv2\"].\n        embedding: Linear embedding layers for feature mapping.\n          An embedding can be passed directly, but needs to use as many layers as\n          features are extracted from the inputs.\n          If `True`, linear embedding layers are used to calculate the loss,\n          if `False`, weighted sum across channel dimensions is used,\n          if `None`, the raw latent least-squares are used.\n        pretrained_weights: weights for the embedding layers to be loaded.\n          If `None`, the embedding layers are randomly initialized.\n        finetune: If `False`, freeze gradients of the embedding.\n        reduction: Reduction function, e.g. `torch.mean` or `torch.sum`.\n        device: Tensor allocation/computation device.\n        spatial_average: If `True`, averages across image dimensions.\n    \"\"\"\n    super().__init__()\n    device = torch.get_default_device() if device is None else device\n    if model is None:\n        model = LPIPSVGG16()\n    elif isinstance(model, str):\n        match model.lower():\n            case \"vgg16\" | \"vgg\":\n                model = LPIPSVGG16()\n            case \"alexnet\":\n                model = LPIPSAlexNet()\n            case \"squeezenet\":\n                model = LPIPSSqueezeNet()\n            case \"resnet18\" | \"resnet\":\n                model = LPIPSResNet18()\n            case \"convnext\":\n                model = LPIPSConvNeXt()\n            case \"vit\":\n                model = LPIPSViT()\n            case \"swinv2\":\n                model = LPIPSSwinV2()\n            case _:\n                model = LPIPSVGG16()\n    self.model = model.to(device)\n    self.model.eval()\n    self.model.requires_grad_(False)\n    if isinstance(embedding, bool) or embedding is None:\n        embedding = (\n            LPIPSEmbedding(self.model.feature_channels)\n            if embedding\n            else LPIPSNonEmbedding(self.model.feature_channels)\n        )\n    if pretrained_weights is not None:\n        embedding.load(pretrained_weights)\n    self.embedding = embedding.to(device)\n    if not finetune and self.embedding:\n        self.embedding.eval()\n        # self.embedding.requires_grad_(False)\n    else:\n        self.embedding.train()\n    self.spatial_average = spatial_average\n    self.reduction = reduction if reduction is not None else torch.nn.Identity()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSLoss.forward","title":"forward","text":"<pre><code>forward(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    as_scores: bool = False,\n    **kwargs,\n) -&gt; torch.Tensor | Sequence[torch.Tensor]\n</code></pre> <p>Forward method.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def forward(\n    self, x1: torch.Tensor, x2: torch.Tensor, as_scores: bool = False, **kwargs\n) -&gt; torch.Tensor | Sequence[torch.Tensor]:\n    \"\"\"Forward method.\"\"\"\n    if x1.shape != x2.shape:\n        raise ValueError(f\"Input shapes must match, got {x1.shape} and {x2.shape}\")\n    if x1.ndim == 5:\n        sample = kwargs.get(\"sample\", 0)\n        x1 = as_batched_slices(x1, sample=sample)\n        x2 = as_batched_slices(x2, sample=sample)\n    reduction = kwargs.get(\"reduction\", self.reduction)\n    if reduction is None:\n        reduction = torch.nn.Identity()\n    x1, x2 = sanitize_ndim(x1), sanitize_ndim(x2)\n    x1, x2 = as_tri_channel(x1), as_tri_channel(x2)\n    w, h = x1.shape[2:]\n    feat1, feat2 = self._normalize(self.model(x1)), self._normalize(self.model(x2))\n    scores = [(f1 - f2) ** 2 for f1, f2 in zip(feat1, feat2)]\n    scores = self.embedding(scores)\n    scores = (\n        self._upsample(scores, size=(w, h))\n        if not self.spatial_average\n        else self._image_average(scores)\n    )\n    if as_scores:\n        return [reduction(s.squeeze()) for s in scores]\n    loss = sum(scores).squeeze()\n    return reduction(loss)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSNonEmbedding","title":"LPIPSNonEmbedding","text":"<pre><code>LPIPSNonEmbedding(in_channels: list[int])\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A simple weighted-sum channel reduction embedding for LPIPS features.</p> <p>Methods:</p> Name Description <code>clamp_weights</code> <p>Dummy function.</p> <code>forward</code> <p>Forward method for a sequence of LPIPS feature loss tensors.</p> <code>load</code> <p>Load the model weights.</p> <code>save</code> <p>Save the model weights.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, in_channels: list[int]):\n    super().__init__()\n    self.channels = in_channels\n    self.weights = torch.nn.ParameterList(\n        [\n            torch.nn.Parameter(torch.ones(in_c).view(1, in_c, 1, 1))\n            for in_c in self.channels\n        ]\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSNonEmbedding.clamp_weights","title":"clamp_weights","text":"<pre><code>clamp_weights()\n</code></pre> <p>Dummy function.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def clamp_weights(self):\n    \"\"\"Dummy function.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSNonEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    x: Sequence[torch.Tensor],\n) -&gt; Sequence[torch.Tensor]\n</code></pre> <p>Forward method for a sequence of LPIPS feature loss tensors.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def forward(self, x: Sequence[torch.Tensor]) -&gt; Sequence[torch.Tensor]:\n    \"\"\"Forward method for a sequence of LPIPS feature loss tensors.\"\"\"\n    assert len(x) == len(self.weights)\n    scores = [\n        (weights * xi).sum(dim=1, keepdim=True)\n        for xi, weights, C in zip(x, self.weights, self.channels)\n    ]\n    return scores\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSNonEmbedding.load","title":"load","text":"<pre><code>load(path: Path | str = Path('lpips_non_emedding.pth'))\n</code></pre> <p>Load the model weights.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def load(self, path: Path | str = Path(\"lpips_non_emedding.pth\")):\n    \"\"\"Load the model weights.\"\"\"\n    path = Path(path)\n    if path.exists():\n        states = torch.load(path, weights_only=True)\n        self.load_state_dict(states)\n        self.eval()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSNonEmbedding.save","title":"save","text":"<pre><code>save(\n    path: Path | str = Path(\"lpips_non_embedding.pth\"),\n) -&gt; Path\n</code></pre> <p>Save the model weights.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def save(self, path: Path | str = Path(\"lpips_non_embedding.pth\")) -&gt; Path:\n    \"\"\"Save the model weights.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(self.state_dict(), path)\n    return path\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSResNet18","title":"LPIPSResNet18","text":"<pre><code>LPIPSResNet18(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>ResNet18 feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"resnet18\",\n        default_weights=tv.ResNet18_Weights.IMAGENET1K_V1,\n        feature_nodes={\n            \"relu\": 0,\n            \"layer1.1.relu\": 1,\n            \"layer2.1.relu\": 2,\n            \"layer3.1.relu\": 3,\n            \"layer4.1.relu\": 4,\n        },\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSSqueezeNet","title":"LPIPSSqueezeNet","text":"<pre><code>LPIPSSqueezeNet(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>SqueezeNet feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"squeezenet1_1\",\n        default_weights=tv.SqueezeNet1_1_Weights.IMAGENET1K_V1,\n        feature_nodes={\n            \"features.1\": 0,\n            # \"features.3\": 1,\n            \"features.4\": 2,\n            # \"features.6\": 3,\n            \"features.7\": 4,\n            \"features.9\": 5,\n            \"features.10\": 6,\n            \"features.11\": 7,\n            \"features.12\": 8,\n        },\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSSwinV2","title":"LPIPSSwinV2","text":"<pre><code>LPIPSSwinV2(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>Small SwinTransformer-V2 feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"swin_v2_s\",\n        default_weights=tv.Swin_V2_S_Weights.IMAGENET1K_V1,\n        feature_nodes={\n            \"features.1\": 0,\n            \"features.3\": 1,\n            \"features.5.9\": 2,\n            \"features.5\": 3,\n            \"features.7\": 4,\n        },\n        input_size=(256, 256),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSVGG16","title":"LPIPSVGG16","text":"<pre><code>LPIPSVGG16(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>VGG16 feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"vgg16\",\n        default_weights=tv.VGG16_Weights.IMAGENET1K_V1,\n        feature_nodes={\n            \"features.3\": 0,\n            \"features.8\": 1,\n            \"features.15\": 2,\n            \"features.22\": 3,\n            \"features.29\": 4,\n        },\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/lpips/#chuchichaestli.metrics.lpips.LPIPSViT","title":"LPIPSViT","text":"<pre><code>LPIPSViT(**kwargs)\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>ViT-B-16 feature extractor for LPIPSLoss.</p> <p>Constructor.</p> Source code in <code>src/chuchichaestli/metrics/lpips.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(\n        model_name=\"vit_b_16\",\n        default_weights=tv.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1,\n        feature_nodes={\n            \"conv_proj\": 0,\n            \"encoder.layers.encoder_layer_1.ln_1\": 1,\n            \"encoder.layers.encoder_layer_2.ln_1\": 2,\n            \"encoder.layers.encoder_layer_5.ln_1\": 3,\n            \"encoder.layers.encoder_layer_8.ln_1\": 4,\n            \"encoder.ln\": 5,\n        },\n        input_size=(384, 384),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/mse/","title":"chuchichaestli.metrics.mse","text":""},{"location":"reference/chuchichaestli/metrics/mse/#chuchichaestli.metrics.mse","title":"chuchichaestli.metrics.mse","text":"<p>MSE evaluation metric.</p> Note <p>This metric cannot reliably be used for gradient propagation; use <code>torch.nn.MSELoss</code> for training instead.</p> <p>Classes:</p> Name Description <code>MSE</code> <p>Mean-squared error.</p>"},{"location":"reference/chuchichaestli/metrics/mse/#chuchichaestli.metrics.mse.MSE","title":"MSE","text":"<pre><code>MSE(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>EvalMetric</code></p> <p>Mean-squared error.</p> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>update</code> <p>Compute metric on new input and update current state.</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    self.device = torch.get_default_device() if device is None else device\n    self.is_nan = torch.tensor(False, device=self.device)\n    self.nan_count = torch.tensor(0, device=self.device)\n    self.min_value = torch.tensor(min_value, device=self.device)\n    self.max_value = torch.tensor(max_value, device=self.device)\n    self.n_observations = torch.tensor(n_observations, device=self.device)\n    self.n_images = torch.tensor(n_images, device=self.device)\n    self.value = torch.tensor(0, device=self.device)\n    self.aggregate = torch.tensor(0, device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/mse/#chuchichaestli.metrics.mse.MSE.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/mse.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    if self.n_observations == 0:\n        return None\n    mse = self.aggregate / self.n_observations\n    self.value = mse\n    return mse.item()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/mse/#chuchichaestli.metrics.mse.MSE.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = False,\n)\n</code></pre> <p>Compute metric on new input and update current state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>False</code> Source code in <code>src/chuchichaestli/metrics/mse.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = False\n):\n    \"\"\"Compute metric on new input and update current state.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    super().update(data, prediction, update_range=update_range)\n    square_error = torch.pow(data[~self.is_nan] - prediction[~self.is_nan], 2)\n    # sum square errors from batch\n    aggregate_batch = torch.sum(square_error)\n    self.aggregate = self.aggregate + aggregate_batch\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/psnr/","title":"chuchichaestli.metrics.psnr","text":""},{"location":"reference/chuchichaestli/metrics/psnr/#chuchichaestli.metrics.psnr","title":"chuchichaestli.metrics.psnr","text":"<p>Peak-Signal-to-Noise Ratio evaluation metric.</p> <p>Classes:</p> Name Description <code>PSNR</code> <p>Peak-signal-to-noise ratio.</p>"},{"location":"reference/chuchichaestli/metrics/psnr/#chuchichaestli.metrics.psnr.PSNR","title":"PSNR","text":"<pre><code>PSNR(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>MSE</code></p> <p>Peak-signal-to-noise ratio.</p> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>update</code> <p>Compute metric on new input and update current state.</p> Source code in <code>src/chuchichaestli/metrics/base.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device | None = None,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    self.device = torch.get_default_device() if device is None else device\n    self.is_nan = torch.tensor(False, device=self.device)\n    self.nan_count = torch.tensor(0, device=self.device)\n    self.min_value = torch.tensor(min_value, device=self.device)\n    self.max_value = torch.tensor(max_value, device=self.device)\n    self.n_observations = torch.tensor(n_observations, device=self.device)\n    self.n_images = torch.tensor(n_images, device=self.device)\n    self.value = torch.tensor(0, device=self.device)\n    self.aggregate = torch.tensor(0, device=self.device)\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/psnr/#chuchichaestli.metrics.psnr.PSNR.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/psnr.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    mse = super().compute()\n    if mse is None:\n        return None\n    elif mse == 0:\n        return torch.inf\n    psnr = 10 * torch.log10(torch.pow(self.data_range, 2) / mse)\n    self.value = psnr\n    return psnr.item()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/psnr/#chuchichaestli.metrics.psnr.PSNR.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = True,\n)\n</code></pre> <p>Compute metric on new input and update current state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>True</code> Source code in <code>src/chuchichaestli/metrics/psnr.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = True\n):\n    \"\"\"Compute metric on new input and update current state.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    super().update(data, prediction, update_range=update_range)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/ssim/","title":"chuchichaestli.metrics.ssim","text":""},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim","title":"chuchichaestli.metrics.ssim","text":"<p>Structural Similarity Index Measure evaluation metric and loss.</p> <p>Classes:</p> Name Description <code>SSIM</code> <p>Structural similarity index measure.</p> <code>SSIMLoss</code> <p>Structural similarity index measure loss function.</p>"},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim.SSIM","title":"SSIM","text":"<pre><code>SSIM(\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device = None,\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    kernel_type: Literal[\n        \"gaussian\", \"uniform\"\n    ] = \"gaussian\",\n    k1: float = 0.01,\n    k2: float = 0.03,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>EvalMetric</code></p> <p>Structural similarity index measure.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Minimum data value relative to metric computation.</p> <code>0</code> <code>max_value</code> <code>float</code> <p>Maximum data value relative to metric computation.</p> <code>1</code> <code>n_observations</code> <code>int</code> <p>Number of observations (pixels) seen by the internal state.</p> <code>0</code> <code>n_images</code> <code>int</code> <p>Number of images seen by the internal state.</p> <code>0</code> <code>device</code> <code>torch.device</code> <p>Tensor allocation/computation device.</p> <code>None</code> <code>kernel_size</code> <code>int | Sequence[int]</code> <p>Size of the moving window aka kernel.</p> <code>11</code> <code>kernel_sigma</code> <code>float | Sequence[float]</code> <p>Standard deviation for the Gaussian kernel.</p> <code>1.5</code> <code>kernel_type</code> <code>Literal['gaussian', 'uniform']</code> <p>Type of kernel; one of <code>['gaussian', 'uniform']</code>.</p> <code>'gaussian'</code> <code>k1</code> <code>float</code> <p>Algorithm parameter, K1 (small constant).</p> <code>0.01</code> <code>k2</code> <code>float</code> <p>Algorithm parameter, K1 (small constant).</p> <code>0.03</code> <code>kwargs</code> <p>Base class keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>compute</code> <p>Return current metric state total.</p> <code>reset</code> <p>Reset the current metrics state.</p> <code>update</code> <p>Compute metric, aggregate, and update internal state.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0,\n    max_value: float = 1,\n    n_observations: int = 0,\n    n_images: int = 0,\n    device: torch.device = None,\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    kernel_type: Literal[\"gaussian\", \"uniform\"] = \"gaussian\",\n    k1: float = 0.01,\n    k2: float = 0.03,\n    **kwargs,\n):\n    \"\"\"Constructor.\n\n    Args:\n        min_value: Minimum data value relative to metric computation.\n        max_value: Maximum data value relative to metric computation.\n        n_observations: Number of observations (pixels) seen by the internal state.\n        n_images: Number of images seen by the internal state.\n        device: Tensor allocation/computation device.\n        kernel_size: Size of the moving window aka kernel.\n        kernel_sigma: Standard deviation for the Gaussian kernel.\n        kernel_type: Type of kernel; one of `['gaussian', 'uniform']`.\n        k1: Algorithm parameter, K1 (small constant).\n        k2: Algorithm parameter, K1 (small constant).\n        kwargs: Base class keyword arguments.\n    \"\"\"\n    super().__init__(\n        min_value=min_value,\n        max_value=max_value,\n        n_observations=n_observations,\n        n_images=n_images,\n        device=device,\n    )\n    self.kernel_type = kernel_type\n    self.kernel_size = kernel_size\n    self.kernel_sigma = kernel_sigma\n    self.k1 = k1\n    self.k2 = k2\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim.SSIM.compute","title":"compute","text":"<pre><code>compute() -&gt; float\n</code></pre> <p>Return current metric state total.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>@torch.inference_mode()\ndef compute(self) -&gt; float:\n    \"\"\"Return current metric state total.\"\"\"\n    if self.n_images == 0:\n        return None\n    ssim = self.aggregate / self.n_images\n    self.value = ssim\n    return ssim.item()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim.SSIM.reset","title":"reset","text":"<pre><code>reset(**kwargs) -&gt; float\n</code></pre> <p>Reset the current metrics state.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def reset(self, **kwargs) -&gt; float:\n    \"\"\"Reset the current metrics state.\"\"\"\n    self.__init__(\n        min_value=self.min_value.item(),\n        max_value=self.max_value.item(),\n        device=self.device,\n        kernel_size=self.kernel_size,\n        kernel_sigma=self.kernel_sigma,\n        kernel_type=self.kernel_type,\n        k1=self.k1,\n        k2=self.k2,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim.SSIM.update","title":"update","text":"<pre><code>update(\n    data: torch.Tensor,\n    prediction: torch.Tensor,\n    update_range: bool = True,\n)\n</code></pre> <p>Compute metric, aggregate, and update internal state.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>update_range</code> <code>bool</code> <p>If True, ranges are automatically updated based on new observation.</p> <code>True</code> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>@torch.inference_mode()\ndef update(\n    self, data: torch.Tensor, prediction: torch.Tensor, update_range: bool = True\n):\n    \"\"\"Compute metric, aggregate, and update internal state.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        update_range: If True, ranges are automatically updated based on\n          new observation.\n    \"\"\"\n    super().update(data, prediction, update_range=update_range)\n    ssim_full, _ = self._compute_ssim_and_cs(\n        data,\n        prediction,\n        self.kernel_size,\n        self.kernel_sigma,\n        self.kernel_type,\n        self.data_range,\n        self.k1,\n        self.k2,\n    )\n    batch_size = ssim_full.shape[0]\n    aggregate_batch = ssim_full.view(batch_size, -1).nanmean(1, keepdim=True)\n    self.aggregate = self.aggregate + torch.sum(aggregate_batch)\n    return self\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim.SSIMLoss","title":"SSIMLoss","text":"<pre><code>SSIMLoss(\n    data_range: float = 1.0,\n    kernel_type: Literal[\n        \"gaussian\", \"uniform\"\n    ] = \"gaussian\",\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    k1: float = 0.01,\n    k2: float = 0.03,\n    reduction: Callable | None = torch.mean,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Structural similarity index measure loss function.</p> <p>Constructor.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Compute the SSIM loss.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 1.0,\n    kernel_type: Literal[\"gaussian\", \"uniform\"] = \"gaussian\",\n    kernel_size: int | Sequence[int] = 11,\n    kernel_sigma: float | Sequence[float] = 1.5,\n    k1: float = 0.01,\n    k2: float = 0.03,\n    reduction: Callable | None = torch.mean,\n):\n    \"\"\"Constructor.\"\"\"\n    super().__init__()\n    self.data_range = data_range\n    self.kernel_type = kernel_type\n    self.kernel_size = kernel_size\n    self.kernel_sigma = kernel_sigma\n    self.k1 = k1\n    self.k2 = k2\n    self.reduction = reduction if reduction is not None else torch.nn.Identity()\n</code></pre>"},{"location":"reference/chuchichaestli/metrics/ssim/#chuchichaestli.metrics.ssim.SSIMLoss.forward","title":"forward","text":"<pre><code>forward(\n    data: torch.Tensor, prediction: torch.Tensor, **kwargs\n) -&gt; torch.Tensor\n</code></pre> <p>Compute the SSIM loss.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>torch.Tensor</code> <p>Observed data.</p> required <code>prediction</code> <code>torch.Tensor</code> <p>Predicted data.</p> required <code>kwargs</code> <p>Additional keyword arguments, e.g., <code>reduction</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Loss value.</p> Source code in <code>src/chuchichaestli/metrics/ssim.py</code> <pre><code>def forward(\n    self, data: torch.Tensor, prediction: torch.Tensor, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Compute the SSIM loss.\n\n    Args:\n        data: Observed data.\n        prediction: Predicted data.\n        kwargs: Additional keyword arguments, e.g., `reduction`.\n\n    Returns:\n        Loss value.\n    \"\"\"\n    reduction = kwargs.get(\"reduction\", self.reduction)\n    if reduction is None:\n        reduction = torch.nn.Identity()\n    ssim_full, _ = SSIM._compute_ssim_and_cs(\n        data,\n        prediction,\n        kernel_size=self.kernel_size,\n        kernel_sigma=self.kernel_sigma,\n        kernel_type=self.kernel_type,\n        data_range=self.data_range,\n        k1=self.k1,\n        k2=self.k2,\n    )\n    batch_size = ssim_full.shape[0]\n    ssim_batch = ssim_full.view(batch_size, -1).nanmean(1, keepdim=True)\n    loss = 1 - ssim_batch\n    return reduction(loss)\n</code></pre>"},{"location":"reference/chuchichaestli/models/","title":"chuchichaestli.models","text":""},{"location":"reference/chuchichaestli/models/#chuchichaestli.models","title":"chuchichaestli.models","text":"<p>Models module of chuchichaestli.</p> <p>Modules:</p> Name Description <code>activations</code> <p>Activation functions for neural networks.</p> <code>adversarial</code> <p>Adversarial training module of chuchichaestli: models and utilities.</p> <code>attention</code> <p>Attention mechanism implementations.</p> <code>downsampling</code> <p>Downsampling modules for 1, 2, and 3D inputs.</p> <code>maps</code> <p>Dim-to-Layer maps for different spatial dimensions.</p> <code>norm</code> <p>Normalization modules for neural networks.</p> <code>resnet</code> <p>ResNet block implementation.</p> <code>unet</code> <p>UNet model implementation and utilities.</p> <code>upsampling</code> <p>Upsampling modules for 1, 2, and 3D inputs.</p>"},{"location":"reference/chuchichaestli/models/activations/","title":"chuchichaestli.models.activations","text":""},{"location":"reference/chuchichaestli/models/activations/#chuchichaestli.models.activations","title":"chuchichaestli.models.activations","text":"<p>Activation functions for neural networks.</p>"},{"location":"reference/chuchichaestli/models/adversarial/","title":"chuchichaestli.models.adversarial","text":""},{"location":"reference/chuchichaestli/models/adversarial/#chuchichaestli.models.adversarial","title":"chuchichaestli.models.adversarial","text":"<p>Adversarial training module of chuchichaestli: models and utilities.</p> <p>Modules:</p> Name Description <code>blocks</code> <p>Blocks for adversarial models.</p> <code>discriminator</code> <p>PatchGAN discriminator and variants.</p>"},{"location":"reference/chuchichaestli/models/adversarial/blocks/","title":"chuchichaestli.models.adversarial.blocks","text":""},{"location":"reference/chuchichaestli/models/adversarial/blocks/#chuchichaestli.models.adversarial.blocks","title":"chuchichaestli.models.adversarial.blocks","text":"<p>Blocks for adversarial models.</p> <p>Classes:</p> Name Description <code>ResidualBlock</code> <p>Residual convolutional block with skip connections.</p>"},{"location":"reference/chuchichaestli/models/adversarial/blocks/#chuchichaestli.models.adversarial.blocks.BaseConvBlock","title":"BaseConvBlock","text":"<pre><code>BaseConvBlock(\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    act: bool = True,\n    act_fn: str | None = None,\n    act_last: bool = False,\n    norm: bool = True,\n    norm_type: str | None = None,\n    num_groups: int = 16,\n    dropout: bool = True,\n    dropout_p: float | None = None,\n    kernel_size: int = 4,\n    stride: int = 2,\n    padding: int = 1,\n    attention: str = \"\",\n    attn_args: dict = {},\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Convolutional block with various components.</p> <p>Components include (in following order):       - normalization (optional)       - activation (optional)       - dropout (optional)       - attention (optional)       - convolution</p> <p>Initialize a convolutional block with various components.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>act</code> <code>bool</code> <p>Use an activation function in the block.</p> <code>True</code> <code>act_fn</code> <code>str | None</code> <p>Activation function.</p> <code>None</code> <code>act_last</code> <code>bool</code> <p>If <code>True</code>, activations is used last.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>Use normalization in the block.</p> <code>True</code> <code>norm_type</code> <code>str | None</code> <p>Normalization type for the conv blocks.</p> <code>None</code> <code>dropout</code> <code>bool</code> <p>Use dropout in the block.</p> <code>True</code> <code>dropout_p</code> <code>float | None</code> <p>Dropout probability of the conv blocks.</p> <code>None</code> <code>num_groups</code> <code>int</code> <p>Number of groups for the conv block normlization (if norm_type == 'group').</p> <code>16</code> <code>kernel_size</code> <code>int</code> <p>Kernel size for the conv block.</p> <code>4</code> <code>stride</code> <code>int</code> <p>Stride for the conv block.</p> <code>2</code> <code>padding</code> <code>int</code> <p>Padding size for the conv block.</p> <code>1</code> <code>attention</code> <code>str</code> <p>Attention descriptor from {\"self_attention\", \"attention_gate\"}; if None or unknown, no attention is used.</p> <code>''</code> <code>attn_args</code> <code>dict</code> <p>Keyword arguments for a <code>SelfAttention</code> module <code>from chuchichaestli.models.attention.self_attention</code></p> <code>{}</code> <code>kwargs</code> <p>Additional keyword arguments for the convolutional layer.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the convolutional downsampling block.</p> Source code in <code>src/chuchichaestli/models/adversarial/blocks.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    act: bool = True,\n    act_fn: str | None = None,\n    act_last: bool = False,\n    norm: bool = True,\n    norm_type: str | None = None,\n    num_groups: int = 16,\n    dropout: bool = True,\n    dropout_p: float | None = None,\n    kernel_size: int = 4,\n    stride: int = 2,\n    padding: int = 1,\n    attention: str = \"\",\n    attn_args: dict = {},\n    **kwargs,\n):\n    \"\"\"Initialize a convolutional block with various components.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      out_channels: Number of output channels.\n      act: Use an activation function in the block.\n      act_fn: Activation function.\n      act_last: If `True`, activations is used last.\n      norm: Use normalization in the block.\n      norm_type: Normalization type for the conv blocks.\n      dropout: Use dropout in the block.\n      dropout_p: Dropout probability of the conv blocks.\n      num_groups: Number of groups for the conv block normlization (if norm_type == 'group').\n      kernel_size: Kernel size for the conv block.\n      stride: Stride for the conv block.\n      padding: Padding size for the conv block.\n      attention: Attention descriptor from {\"self_attention\", \"attention_gate\"};\n        if None or unknown, no attention is used.\n      attn_args: Keyword arguments for a `SelfAttention` module\n        `from chuchichaestli.models.attention.self_attention`\n      kwargs: Additional keyword arguments for the convolutional layer.\n    \"\"\"\n    super().__init__()\n    self.norm: nn.Module | None = None\n    self.act: nn.Module | None = None\n    self.attn: nn.Module | None = None\n    self.dropout: nn.Module | None = None\n    self.act_last = act_last\n    if norm and norm_type is not None:\n        if norm_type == \"group\" and (\n            in_channels % num_groups != 0 or in_channels &lt; num_groups\n        ):\n            if in_channels % 2 == 0:\n                num_groups = in_channels // 2\n            else:\n                num_groups = gcd(in_channels, in_channels // 3)\n        self.norm = Norm(dimensions, norm_type, in_channels, num_groups)\n    if act and act_fn is not None:\n        self.act = ACTIVATION_FUNCTIONS[act_fn]()\n    if dropout and dropout_p is not None and dropout_p &gt; 0:\n        self.dropout = nn.Dropout(dropout_p)\n    match attention:\n        case \"self_attention\":\n            self.attn = ATTENTION_MAP[attention](in_channels, **attn_args)\n        case \"attention_gate\":\n            self.attn = ATTENTION_MAP[attention](\n                in_channels, out_channels, **attn_args\n            )\n        case _:\n            self.attn = None\n    self.conv = DIM_TO_CONV_MAP[dimensions](\n        in_channels,\n        out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/blocks/#chuchichaestli.models.adversarial.blocks.BaseConvBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, _h: torch.Tensor | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the convolutional downsampling block.</p> Source code in <code>src/chuchichaestli/models/adversarial/blocks.py</code> <pre><code>def forward(self, x: torch.Tensor, _h: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the convolutional downsampling block.\"\"\"\n    h = x\n    h = self.norm(h) if self.norm is not None else h\n    if not self.act_last:\n        h = self.act(h) if self.act is not None else h\n    h = self.dropout(h) if self.dropout is not None else h\n    h = self.attn(h, _h if _h is not None else h) if self.attn else h\n    h = self.conv(h)\n    if self.act_last:\n        h = self.act(h) if self.act is not None else h\n    return h\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/blocks/#chuchichaestli.models.adversarial.blocks.ResidualBlock","title":"ResidualBlock","text":"<pre><code>ResidualBlock(\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    act_fn: str = \"silu\",\n    norm_type: str = \"group\",\n    num_groups: int = 32,\n    dropout_p: float = 0.0,\n    kernel_size: int = 3,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>ResnetBlock</code></p> <p>Residual convolutional block with skip connections.</p> <p>Same implementation as chuchichaestli.models.resnet.ResidualBlock but with different argument keys and default values (analogous to BaseConvBlock). The time embedding is removed by default.</p> <p>Initialize the residual block.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>act_fn</code> <code>str</code> <p>Activation function.</p> <code>'silu'</code> <code>norm_type</code> <code>str</code> <p>Normalization type for the conv blocks.</p> <code>'group'</code> <code>dropout_p</code> <code>float</code> <p>Dropout probability of the conv blocks.</p> <code>0.0</code> <code>num_groups</code> <code>int</code> <p>Number of groups for the conv block normlization (if norm_type == 'group').</p> <code>32</code> <code>kernel_size</code> <code>int</code> <p>Kernel size for the conv block.</p> <code>3</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the residual block (time embedding optional).</p> Source code in <code>src/chuchichaestli/models/adversarial/blocks.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    act_fn: str = \"silu\",\n    norm_type: str = \"group\",\n    num_groups: int = 32,\n    dropout_p: float = 0.0,\n    kernel_size: int = 3,\n    **kwargs,\n):\n    \"\"\"Initialize the residual block.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      out_channels: Number of output channels.\n      act_fn: Activation function.\n      norm_type: Normalization type for the conv blocks.\n      dropout_p: Dropout probability of the conv blocks.\n      num_groups: Number of groups for the conv block normlization (if norm_type == 'group').\n      kernel_size: Kernel size for the conv block.\n      kwargs: Additional keyword arguments.\n    \"\"\"\n    kwargs[\"res_act_fn\"] = act_fn\n    kwargs[\"res_dropout\"] = dropout_p\n    kwargs[\"res_norm_type\"] = norm_type\n    kwargs[\"res_groups\"] = num_groups\n    kwargs[\"res_kernel_size\"] = kernel_size\n    for k in (\n        \"act\",\n        \"dropout\",\n        \"norm\",\n        \"stride\",\n        \"padding\",\n        \"attention\",\n        \"attn_args\",\n        \"bias\",\n    ):\n        if k in kwargs:\n            kwargs.pop(k)\n    super().__init__(dimensions, in_channels, out_channels, False, 0, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/blocks/#chuchichaestli.models.adversarial.blocks.ResidualBlock.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, _t: torch.Tensor = torch.empty(0))\n</code></pre> <p>Forward pass through the residual block (time embedding optional).</p> Source code in <code>src/chuchichaestli/models/adversarial/blocks.py</code> <pre><code>def forward(self, x: torch.Tensor, _t: torch.Tensor = torch.empty(0)):\n    \"\"\"Forward pass through the residual block (time embedding optional).\"\"\"\n    return super().forward(x, _t)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/","title":"chuchichaestli.models.adversarial.discriminator","text":""},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator","title":"chuchichaestli.models.adversarial.discriminator","text":"<p>PatchGAN discriminator and variants.</p> <p>Classes:</p> Name Description <code>AntialiasingDiscriminator</code> <p>Discriminator with separated feature mapping and downsampling block pairs.</p> <code>AntialiasingPatchDiscriminator</code> <p>PatchGAN discriminator with separated feature mapping and downsampling block pairs.</p> <code>AttnPatchDiscriminator</code> <p>PatchGAN discriminator with attention layers in the hidden blocks.</p> <code>BlockDiscriminator</code> <p>A base class for pixel and patch-based discriminators as in Pix2Pix.</p> <code>PatchDiscriminator</code> <p>PatchGAN discriminator.</p> <code>PixelDiscriminator</code> <p>PixelGAN or 1x1-PatchGAN discriminator.</p>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.AntialiasingDiscriminator","title":"AntialiasingDiscriminator","text":"<pre><code>AntialiasingDiscriminator(\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    patch_size: int = 32,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    channel_mults: tuple[int, ...] | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BlockDiscriminator</code></p> <p>Discriminator with separated feature mapping and downsampling block pairs.</p> <p>Constructs a discriminator of 5 antialiasing block pairs.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>patch_size</code> <code>int</code> <p>Size of the patch</p> <code>32</code> <code>n_hidden</code> <code>int</code> <p>Number of hidden block pairs in the model; only takes effect if block_types is None.</p> <code>3</code> <code>block_types</code> <code>tuple[str, ...] | None</code> <p>Types of down blocks.</p> <code>None</code> <code>channel_mults</code> <code>tuple[int, ...] | None</code> <p>Channel multipliers for each block.</p> <code>None</code> <code>out_channels</code> <p>Output channels (should generally be 1, i.e. true/fake).</p> required <code>attn_n_heads</code> <p>Number of attention heads.</p> required <code>attn_head_dim</code> <p>Dimension of the attention head.</p> required <code>attn_gate_inter_channels</code> <p>Number of intermediate channels for the attention gate.</p> required <code>kwargs</code> <p>Additional arguments for the blocks.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>n_hidden</code> <code>int</code> <p>Number of hidden block pairs in the model.</p> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    patch_size: int = 32,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    channel_mults: tuple[int, ...] | None = None,\n    **kwargs,\n):\n    \"\"\"Constructs a discriminator of 5 antialiasing block pairs.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      n_channels: Number of channels in the first layer.\n      patch_size: Size of the patch\n      n_hidden: Number of hidden block pairs in the model;\n        only takes effect if block_types is None.\n      block_types: Types of down blocks.\n      channel_mults: Channel multipliers for each block.\n      out_channels: Output channels (should generally be 1, i.e. true/fake).\n      attn_n_heads: Number of attention heads.\n      attn_head_dim: Dimension of the attention head.\n      attn_gate_inter_channels: Number of intermediate channels for the attention gate.\n      kwargs: Additional arguments for the blocks.\n    \"\"\"\n    self.patch_size = patch_size\n    if block_types is None:\n        block_types = (\n            (\"ConvBlock\", \"ActConvDownsampleBlock\", \"ConvBlock\")\n            + (\"NormActConvDownsampleBlock\", \"ConvBlock\") * (n_hidden - 1)\n            + (\"NormActConvBlock\",)\n        )\n        channel_mults = (\n            1,\n            2,\n        ) + (\n            1,\n            2,\n        ) * (n_hidden - 1)\n    kwargs[\"block_types\"] = block_types\n    kwargs[\"channel_mults\"] = channel_mults\n    super().__init__(dimensions, in_channels, n_channels, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.AntialiasingDiscriminator.n_hidden","title":"n_hidden  <code>property</code>","text":"<pre><code>n_hidden: int\n</code></pre> <p>Number of hidden block pairs in the model.</p>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.AntialiasingPatchDiscriminator","title":"AntialiasingPatchDiscriminator","text":"<pre><code>AntialiasingPatchDiscriminator(\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    patch_size: int = 32,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    channel_mults: tuple[int, ...] | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>AntialiasingDiscriminator</code></p> <p>PatchGAN discriminator with separated feature mapping and downsampling block pairs.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass.</p> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    patch_size: int = 32,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    channel_mults: tuple[int, ...] | None = None,\n    **kwargs,\n):\n    \"\"\"Constructs a discriminator of 5 antialiasing block pairs.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      n_channels: Number of channels in the first layer.\n      patch_size: Size of the patch\n      n_hidden: Number of hidden block pairs in the model;\n        only takes effect if block_types is None.\n      block_types: Types of down blocks.\n      channel_mults: Channel multipliers for each block.\n      out_channels: Output channels (should generally be 1, i.e. true/fake).\n      attn_n_heads: Number of attention heads.\n      attn_head_dim: Dimension of the attention head.\n      attn_gate_inter_channels: Number of intermediate channels for the attention gate.\n      kwargs: Additional arguments for the blocks.\n    \"\"\"\n    self.patch_size = patch_size\n    if block_types is None:\n        block_types = (\n            (\"ConvBlock\", \"ActConvDownsampleBlock\", \"ConvBlock\")\n            + (\"NormActConvDownsampleBlock\", \"ConvBlock\") * (n_hidden - 1)\n            + (\"NormActConvBlock\",)\n        )\n        channel_mults = (\n            1,\n            2,\n        ) + (\n            1,\n            2,\n        ) * (n_hidden - 1)\n    kwargs[\"block_types\"] = block_types\n    kwargs[\"channel_mults\"] = channel_mults\n    super().__init__(dimensions, in_channels, n_channels, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.AntialiasingPatchDiscriminator.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, patch_size: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass.</p> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def forward(self, x: torch.Tensor, patch_size: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\"\"\"\n    dim = len(x.shape)\n    B, C, W = x.size(0), x.size(1), x.size(2)\n    if patch_size is None:\n        patch_size = self.patch_size\n    if dim == 3:\n        X = W // patch_size\n        x = x.view(B, C, X, patch_size)\n        x = x.permute(0, 2, 1, 3).contiguous().view(B * X, C, patch_size)\n    elif dim == 4:\n        H, W = x.size(2), x.size(3)\n        X = H // patch_size\n        Y = W // patch_size\n        x = x.view(B, C, Y, patch_size, X, patch_size)\n        x = (\n            x.permute(0, 2, 4, 1, 3, 5)\n            .contiguous()\n            .view(B * Y * X, C, patch_size, patch_size)\n        )\n    elif dim == 5:\n        H, W, D = x.size(2), x.size(3), x.size(4)\n        X = H // patch_size\n        Y = W // patch_size\n        Z = D // patch_size\n        x = x.view(B, C, Z, patch_size, Y, patch_size, X, patch_size)\n        x = (\n            x.permute(0, 2, 4, 6, 1, 3, 5, 7)\n            .contiguous()\n            .view(B * Z * Y * X, C, patch_size, patch_size, patch_size)\n        )\n    return super().forward(x)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.AttnPatchDiscriminator","title":"AttnPatchDiscriminator","text":"<pre><code>AttnPatchDiscriminator(\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BlockDiscriminator</code></p> <p>PatchGAN discriminator with attention layers in the hidden blocks.</p> <p>Constructs a sequential model of 5 PatchGAN blocks.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>n_hidden</code> <code>int</code> <p>Number of hidden blocks in the model; only takes effect if block_types is None.</p> <code>3</code> <code>block_types</code> <code>tuple[str, ...] | None</code> <p>Types of down blocks.</p> <code>None</code> <code>channel_mults</code> <p>Channel multipliers for each block.</p> required <code>out_channels</code> <p>Output channels (should generally be 1, i.e. true/fake).</p> required <code>attn_n_heads</code> <p>Number of attention heads.</p> required <code>attn_head_dim</code> <p>Dimension of the attention head.</p> required <code>attn_gate_inter_channels</code> <p>Number of intermediate channels for the attention gate.</p> required <code>kwargs</code> <p>Additional arguments for the blocks.</p> <code>{}</code> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    **kwargs,\n):\n    \"\"\"Constructs a sequential model of 5 PatchGAN blocks.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      n_channels: Number of channels in the first layer.\n      n_hidden: Number of hidden blocks in the model;\n        only takes effect if block_types is None.\n      block_types: Types of down blocks.\n      channel_mults: Channel multipliers for each block.\n      out_channels: Output channels (should generally be 1, i.e. true/fake).\n      attn_n_heads: Number of attention heads.\n      attn_head_dim: Dimension of the attention head.\n      attn_gate_inter_channels: Number of intermediate channels for the attention gate.\n      kwargs: Additional arguments for the blocks.\n    \"\"\"\n    if block_types is None:\n        block_types = (\n            (\"ConvDownBlock\", \"ActConvDownBlock\")\n            + (\"NormActAttnConvDownBlock\",) * (n_hidden - 2)\n            + (\n                \"NormActAttnConvBlock\",\n                \"NormActConvBlock\",\n            )\n        )\n    kwargs[\"block_types\"] = block_types\n    super().__init__(dimensions, in_channels, n_channels, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.BlockDiscriminator","title":"BlockDiscriminator","text":"<pre><code>BlockDiscriminator(\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    block_types: tuple[str, ...] = (\n        \"ConvDownBlock\",\n        \"ActConvDownBlock\",\n        \"NormActConvDownBlock\",\n        \"NormActConvBlock\",\n        \"NormActConvBlock\",\n    ),\n    channel_mults: tuple[int, ...] | None = None,\n    out_channels: int = 1,\n    attn_n_heads: int = 1,\n    attn_head_dim: int = 16,\n    attn_gate_inter_channels: int = 32,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Sequential</code></p> <p>A base class for pixel and patch-based discriminators as in Pix2Pix.</p> <p>From the paper: https://arxiv.org/abs/1611.07004</p> <p>Construct a discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>block_types</code> <code>tuple[str, ...]</code> <p>Types of down blocks.</p> <code>('ConvDownBlock', 'ActConvDownBlock', 'NormActConvDownBlock', 'NormActConvBlock', 'NormActConvBlock')</code> <code>channel_mults</code> <code>tuple[int, ...] | None</code> <p>Channel multipliers for each block.</p> <code>None</code> <code>out_channels</code> <code>int</code> <p>Output channels (should generally be 1, i.e. true/fake).</p> <code>1</code> <code>attn_n_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>1</code> <code>attn_head_dim</code> <code>int</code> <p>Dimension of the attention head.</p> <code>16</code> <code>attn_gate_inter_channels</code> <code>int</code> <p>Number of intermediate channels for the attention gate.</p> <code>32</code> <code>kwargs</code> <p>Additional arguments for the blocks.</p> <code>{}</code> <p>Methods:</p> Name Description <code>receptive_field</code> <p>Calculate the receptive field size of the discriminator.</p> <p>Attributes:</p> Name Type Description <code>conv_layers</code> <code>list[nn.Module]</code> <p>List of convolutional and pooling layers.</p> <code>n_hidden</code> <code>int</code> <p>Number of hidden blocks in the model.</p> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    block_types: tuple[str, ...] = (\n        \"ConvDownBlock\",\n        \"ActConvDownBlock\",\n        \"NormActConvDownBlock\",\n        \"NormActConvBlock\",\n        \"NormActConvBlock\",\n    ),\n    channel_mults: tuple[int, ...] | None = None,\n    out_channels: int = 1,\n    attn_n_heads: int = 1,\n    attn_head_dim: int = 16,\n    attn_gate_inter_channels: int = 32,\n    **kwargs,\n):\n    \"\"\"Construct a discriminator.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      n_channels: Number of channels in the first layer.\n      block_types: Types of down blocks.\n      channel_mults: Channel multipliers for each block.\n      out_channels: Output channels (should generally be 1, i.e. true/fake).\n      attn_n_heads: Number of attention heads.\n      attn_head_dim: Dimension of the attention head.\n      attn_gate_inter_channels: Number of intermediate channels for the attention gate.\n      kwargs: Additional arguments for the blocks.\n    \"\"\"\n    if dimensions not in DIM_TO_CONV_MAP:\n        raise ValueError(\n            f\"Invalid number of dimensions ({dimensions}). \"\n            f\"Must be one of {list(DIM_TO_CONV_MAP.keys())}.\"\n        )\n\n    if any(block_type not in BLOCK_MAP for block_type in block_types):\n        raise ValueError(\n            f\"Invalid block types. Must be one of {list(BLOCK_MAP.keys())}.\"\n        )\n\n    n_blocks = len(block_types)\n    if n_blocks &lt; 2:\n        raise ValueError(\n            f\"At least two convolutional blocks are required ({block_types} was given).\"\n        )\n\n    if channel_mults is None:\n        channel_mults = (2,) * (len(block_types) - 2)\n    elif len(channel_mults) &lt; n_blocks - 2:\n        raise ValueError(\n            f\"Not enough channel multipliers. Must be at least len(block_types)-2 = {n_blocks-2}.\"\n        )\n\n    kwargs.setdefault(\n        \"attn_args\",\n        {\n            \"n_heads\": attn_n_heads,\n            \"head_dim\": attn_head_dim,\n            \"inter_channels\": attn_gate_inter_channels,\n        },\n    )\n\n    # input block\n    block_cls = BLOCK_MAP[block_types[0]]\n    blocks = [block_cls(dimensions, in_channels, n_channels, **kwargs)]\n    in_c = n_channels\n    for block_type, mult in zip(block_types[1:-1], channel_mults):\n        out_c = int(in_c * mult)\n        block_cls = BLOCK_MAP[block_type]\n        block = block_cls(dimensions, in_c, out_c, **(kwargs | {\"bias\": True}))\n        blocks.append(block)\n        in_c = out_c\n    # output block\n    block_cls = BLOCK_MAP[block_types[-1]]\n    blocks += [\n        block_cls(dimensions, in_c, out_channels, **(kwargs | {\"bias\": True}))\n    ]\n    super().__init__(*blocks)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.BlockDiscriminator.conv_layers","title":"conv_layers  <code>property</code>","text":"<pre><code>conv_layers: list[nn.Module]\n</code></pre> <p>List of convolutional and pooling layers.</p>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.BlockDiscriminator.n_hidden","title":"n_hidden  <code>property</code>","text":"<pre><code>n_hidden: int\n</code></pre> <p>Number of hidden blocks in the model.</p>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.BlockDiscriminator.receptive_field","title":"receptive_field","text":"<pre><code>receptive_field() -&gt; tuple[int, ...]\n</code></pre> <p>Calculate the receptive field size of the discriminator.</p> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def receptive_field(self) -&gt; tuple[int, ...]:\n    \"\"\"Calculate the receptive field size of the discriminator.\"\"\"\n    layers = self.conv_layers\n    n_dim = (\n        len(layers[0].kernel_size)\n        if isinstance(layers[0].kernel_size, tuple)\n        else 1\n    )\n    r = [1] * n_dim\n    for layer in reversed(layers):\n        for i in range(n_dim):\n            k = (\n                layer.kernel_size[i]\n                if isinstance(layer.kernel_size, tuple)\n                else layer.kernel_size\n            )\n            if hasattr(layer, \"dilation\"):\n                d = (\n                    layer.dilation[i]\n                    if isinstance(layer.dilation, tuple)\n                    else layer.dilation\n                )\n                k = d * (k - 1) + 1\n            s = layer.stride[i] if isinstance(layer.stride, tuple) else layer.stride\n            p = (\n                layer.padding[i]\n                if isinstance(layer.padding, tuple)\n                else layer.padding\n            )\n            if isinstance(p, str):\n                p = 0\n            r[i] = (r[i] - p) * s + k\n    return tuple(r)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.PatchDiscriminator","title":"PatchDiscriminator","text":"<pre><code>PatchDiscriminator(\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BlockDiscriminator</code></p> <p>PatchGAN discriminator.</p> <p>Constructs a sequential model of 5 PatchGAN blocks.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>n_hidden</code> <code>int</code> <p>Number of hidden blocks in the model; only takes effect if block_types is None.</p> <code>3</code> <code>block_types</code> <code>tuple[str, ...] | None</code> <p>Types of down blocks.</p> <code>None</code> <code>channel_mults</code> <p>Channel multipliers for each block.</p> required <code>out_channels</code> <p>Output channels (should generally be 1, i.e. true/fake).</p> required <code>attn_n_heads</code> <p>Number of attention heads.</p> required <code>attn_head_dim</code> <p>Dimension of the attention head.</p> required <code>attn_gate_inter_channels</code> <p>Number of intermediate channels for the attention gate.</p> required <code>kwargs</code> <p>Additional arguments for the blocks.</p> <code>{}</code> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    n_hidden: int = 3,\n    block_types: tuple[str, ...] | None = None,\n    **kwargs,\n):\n    \"\"\"Constructs a sequential model of 5 PatchGAN blocks.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      n_channels: Number of channels in the first layer.\n      n_hidden: Number of hidden blocks in the model;\n        only takes effect if block_types is None.\n      block_types: Types of down blocks.\n      channel_mults: Channel multipliers for each block.\n      out_channels: Output channels (should generally be 1, i.e. true/fake).\n      attn_n_heads: Number of attention heads.\n      attn_head_dim: Dimension of the attention head.\n      attn_gate_inter_channels: Number of intermediate channels for the attention gate.\n      kwargs: Additional arguments for the blocks.\n    \"\"\"\n    if block_types is None:\n        block_types = (\n            (\"ConvDownBlock\", \"ActConvDownBlock\")\n            + (\"NormActConvDownBlock\",) * (n_hidden - 2)\n            + (\n                \"NormActConvBlock\",\n                \"NormActConvBlock\",\n            )\n        )\n    kwargs[\"block_types\"] = block_types\n    super().__init__(dimensions, in_channels, n_channels, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/adversarial/discriminator/#chuchichaestli.models.adversarial.discriminator.PixelDiscriminator","title":"PixelDiscriminator","text":"<pre><code>PixelDiscriminator(\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    n_hidden: int = 1,\n    block_types: tuple[str, ...] | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BlockDiscriminator</code></p> <p>PixelGAN or 1x1-PatchGAN discriminator.</p> <p>Constructs a sequential model of 3 blocks, no downsampling, no padding.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of dimensions.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the first layer.</p> required <code>n_hidden</code> <code>int</code> <p>Number of hidden blocks in the model; only takes effect if block_types is None.</p> <code>1</code> <code>block_types</code> <code>tuple[str, ...] | None</code> <p>Types of down blocks.</p> <code>None</code> <code>channel_mults</code> <p>Channel multipliers for each block.</p> required <code>out_channels</code> <p>Output channels (should generally be 1, i.e. true/fake).</p> required <code>attn_n_heads</code> <p>Number of attention heads.</p> required <code>attn_head_dim</code> <p>Dimension of the attention head.</p> required <code>attn_gate_inter_channels</code> <p>Number of intermediate channels for the attention gate.</p> required <code>kwargs</code> <p>Additional arguments for the blocks.</p> <code>{}</code> Source code in <code>src/chuchichaestli/models/adversarial/discriminator.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    n_channels: int,\n    n_hidden: int = 1,\n    block_types: tuple[str, ...] | None = None,\n    **kwargs,\n):\n    \"\"\"Constructs a sequential model of 3 blocks, no downsampling, no padding.\n\n    Args:\n      dimensions: Number of dimensions.\n      in_channels: Number of input channels.\n      n_channels: Number of channels in the first layer.\n      n_hidden: Number of hidden blocks in the model;\n        only takes effect if block_types is None.\n      block_types: Types of down blocks.\n      channel_mults: Channel multipliers for each block.\n      out_channels: Output channels (should generally be 1, i.e. true/fake).\n      attn_n_heads: Number of attention heads.\n      attn_head_dim: Dimension of the attention head.\n      attn_gate_inter_channels: Number of intermediate channels for the attention gate.\n      kwargs: Additional arguments for the blocks.\n    \"\"\"\n    if block_types is None:\n        block_types = (\"ConvBlock\", \"ActConvBlock\") + (\n            \"NormActConvBlock\",\n        ) * n_hidden\n    kwargs[\"block_types\"] = block_types\n    kwargs[\"kernel_size\"] = 1\n    kwargs[\"stride\"] = 1\n    kwargs[\"padding\"] = 0\n    super().__init__(dimensions, in_channels, n_channels, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/attention/","title":"chuchichaestli.models.attention","text":""},{"location":"reference/chuchichaestli/models/attention/#chuchichaestli.models.attention","title":"chuchichaestli.models.attention","text":"<p>Attention mechanism implementations.</p> <p>Modules:</p> Name Description <code>attention_gate</code> <p>Implementation of the Attention Gate mechanism.</p> <code>conv_attention</code> <p>Conv-attention module.</p> <code>self_attention</code> <p>Self-attention module.</p>"},{"location":"reference/chuchichaestli/models/attention/attention_gate/","title":"chuchichaestli.models.attention.attention_gate","text":""},{"location":"reference/chuchichaestli/models/attention/attention_gate/#chuchichaestli.models.attention.attention_gate","title":"chuchichaestli.models.attention.attention_gate","text":"<p>Implementation of the Attention Gate mechanism.</p> <p>Classes:</p> Name Description <code>AttentionGate</code> <p>Attention Gate module.</p>"},{"location":"reference/chuchichaestli/models/attention/attention_gate/#chuchichaestli.models.attention.attention_gate.AttentionGate","title":"AttentionGate","text":"<pre><code>AttentionGate(\n    dimension: int = 2,\n    num_channels_x: int = 1,\n    num_channels_g: int = 1,\n    num_channels_inter: int = 1,\n    subsample_factor: int | tuple[int, ...] = 2,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Attention Gate module.</p> <p>As described in the paper: \"Attention U-Net: Learning Where to Look for the Pancreas\" by Oktay et al. (2018); see https://arxiv.org/abs/1804.03999.</p> <p>Initialize the AttentionGate.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass of the AttentionGate.</p> Source code in <code>src/chuchichaestli/models/attention/attention_gate.py</code> <pre><code>def __init__(\n    self,\n    dimension: int = 2,\n    num_channels_x: int = 1,\n    num_channels_g: int = 1,\n    num_channels_inter: int = 1,\n    subsample_factor: int | tuple[int, ...] = 2,\n    **kwargs,\n):\n    \"\"\"Initialize the AttentionGate.\"\"\"\n    super().__init__()\n\n    if dimension not in DIM_TO_CONV_MAP:\n        raise ValueError(f\"Invalid dimension: {dimension}\")\n    conv_cls = DIM_TO_CONV_MAP[dimension]\n    self.upsample_mode = UPSAMPLE_MODE[dimension]\n\n    # TODO: What about a \"multi-scale attention gate\"?\n\n    self.W_g = conv_cls(\n        num_channels_g,\n        num_channels_inter,\n        kernel_size=1,\n        stride=1,\n        padding=0,\n        bias=True,\n    )\n    self.W_x = conv_cls(\n        num_channels_x,\n        num_channels_inter,\n        kernel_size=subsample_factor,\n        stride=subsample_factor,\n        padding=0,\n        bias=False,\n    )  # Eq. 1 doesn't have bias for W_x\n    self.psi = conv_cls(\n        num_channels_inter, 1, kernel_size=1, stride=1, padding=0, bias=True\n    )\n\n    self.W_out = conv_cls(\n        num_channels_x,\n        num_channels_x,\n        kernel_size=1,\n        stride=1,\n        padding=0,\n        bias=True,\n    )\n\n    self.sigma1 = nn.ReLU()\n    self.sigma2 = nn.Sigmoid()\n</code></pre>"},{"location":"reference/chuchichaestli/models/attention/attention_gate/#chuchichaestli.models.attention.attention_gate.AttentionGate.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, g: torch.TensorType)\n</code></pre> <p>Forward pass of the AttentionGate.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>The input features.</p> required <code>g</code> <code>torch.TensorType</code> <p>The gating signal.</p> required <p>Returns:</p> Type Description <p>The attended features.</p> Source code in <code>src/chuchichaestli/models/attention/attention_gate.py</code> <pre><code>def forward(self, x: torch.Tensor, g: torch.TensorType):\n    \"\"\"Forward pass of the AttentionGate.\n\n    Args:\n        x: The input features.\n        g: The gating signal.\n\n    Returns:\n        The attended features.\n    \"\"\"\n    input_size = x.size()\n\n    theta_x = self.W_x(x)\n    phi_g = self.W_g(g)\n\n    phi_g = F.interpolate(phi_g, size=theta_x.size()[2:], mode=self.upsample_mode)\n    q = self.psi(self.sigma1(theta_x + phi_g))\n    alpha = F.interpolate(\n        self.sigma2(q), size=input_size[2:], mode=self.upsample_mode\n    )\n    x_hat = alpha.expand_as(x) * x\n    x_hat = self.W_out(x_hat)\n    return x_hat\n</code></pre>"},{"location":"reference/chuchichaestli/models/attention/conv_attention/","title":"chuchichaestli.models.attention.conv_attention","text":""},{"location":"reference/chuchichaestli/models/attention/conv_attention/#chuchichaestli.models.attention.conv_attention","title":"chuchichaestli.models.attention.conv_attention","text":"<p>Conv-attention module.</p> <p>Classes:</p> Name Description <code>ConvAttention</code> <p>Convolutional attention block implementation.</p>"},{"location":"reference/chuchichaestli/models/attention/conv_attention/#chuchichaestli.models.attention.conv_attention.ConvAttention","title":"ConvAttention","text":"<pre><code>ConvAttention(\n    dimensions: int,\n    n_channels: int,\n    norm_type: str = \"group\",\n    groups: int = 32,\n    kernel_size: int = 1,\n    dropout_p: float = 0.0,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Convolutional attention block implementation.</p> <p>Uses convolutions to compute query, key and value matrices.</p> <p>Convolutional attention block implementation.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass of the convolutional attention block.</p> Source code in <code>src/chuchichaestli/models/attention/conv_attention.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    n_channels: int,\n    norm_type: str = \"group\",\n    groups: int = 32,\n    kernel_size: int = 1,\n    dropout_p: float = 0.0,\n    **kwargs,\n):\n    \"\"\"Convolutional attention block implementation.\"\"\"\n    super().__init__()\n    self.norm = Norm(dimensions, norm_type, n_channels, groups)\n    self.dropout_p = dropout_p\n    conv_cls = DIM_TO_CONV_MAP[dimensions]\n    self.q = conv_cls(n_channels, n_channels, kernel_size=kernel_size)\n    self.k = conv_cls(n_channels, n_channels, kernel_size=kernel_size)\n    self.v = conv_cls(n_channels, n_channels, kernel_size=kernel_size)\n    self.proj_out = conv_cls(n_channels, n_channels, kernel_size=kernel_size)\n</code></pre>"},{"location":"reference/chuchichaestli/models/attention/conv_attention/#chuchichaestli.models.attention.conv_attention.ConvAttention.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, *args) -&gt; torch.Tensor\n</code></pre> <p>Forward pass of the convolutional attention block.</p> Source code in <code>src/chuchichaestli/models/attention/conv_attention.py</code> <pre><code>def forward(self, x: torch.Tensor, *args) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the convolutional attention block.\"\"\"\n    B, C = x.shape[:2]\n    h = x\n\n    h = self.norm(h)\n    q = self.q(h)\n    k = self.k(h)\n    v = self.v(h)\n\n    q = q.view(B, 1, C, -1).permute(0, 1, 3, 2).contiguous()\n    k = k.view(B, 1, C, -1).permute(0, 1, 3, 2).contiguous()\n    v = v.view(B, 1, C, -1).permute(0, 1, 3, 2).contiguous()\n    h = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout_p)\n    return h.permute(0, 3, 1, 2).reshape(x.shape)\n</code></pre>"},{"location":"reference/chuchichaestli/models/attention/self_attention/","title":"chuchichaestli.models.attention.self_attention","text":""},{"location":"reference/chuchichaestli/models/attention/self_attention/#chuchichaestli.models.attention.self_attention","title":"chuchichaestli.models.attention.self_attention","text":"<p>Self-attention module.</p> <p>Classes:</p> Name Description <code>SelfAttention</code> <p>Attention block implementation.</p>"},{"location":"reference/chuchichaestli/models/attention/self_attention/#chuchichaestli.models.attention.self_attention.SelfAttention","title":"SelfAttention","text":"<pre><code>SelfAttention(\n    n_channels: int,\n    n_heads: int = 1,\n    head_dim: int | None = None,\n    dropout_p: float = 0.0,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Attention block implementation.</p> <p>Attention block implementation.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass of the attention block.</p> Source code in <code>src/chuchichaestli/models/attention/self_attention.py</code> <pre><code>def __init__(\n    self,\n    n_channels: int,\n    n_heads: int = 1,\n    head_dim: int | None = None,\n    dropout_p: float = 0.0,\n    **kwargs,\n):\n    \"\"\"Attention block implementation.\"\"\"\n    super().__init__()\n\n    if head_dim is None:\n        head_dim = n_channels // n_heads\n\n    self.proj_in = nn.Linear(n_channels, n_heads * head_dim * 3)  # q, k, v\n    self.proj_out = nn.Linear(n_heads * head_dim, n_channels)\n\n    self.scale = head_dim**-0.5\n    self.n_heads = n_heads\n    self.head_dim = head_dim\n    self.dropout_p = dropout_p\n</code></pre>"},{"location":"reference/chuchichaestli/models/attention/self_attention/#chuchichaestli.models.attention.self_attention.SelfAttention.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, _h: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass of the attention block.</p> Source code in <code>src/chuchichaestli/models/attention/self_attention.py</code> <pre><code>def forward(self, x: torch.Tensor, _h: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the attention block.\"\"\"\n    in_shape = x.shape\n    x = x.view(in_shape[0], in_shape[1], -1).permute(0, 2, 1)\n\n    qkv = self.proj_in(x).view(in_shape[0], -1, self.n_heads, 3 * self.head_dim)\n    q, k, v = qkv.chunk(3, dim=-1)\n\n    out = F.scaled_dot_product_attention(\n        q, k, v, scale=self.scale, dropout_p=self.dropout_p\n    )\n    out = out.reshape(in_shape[0], -1, self.n_heads * self.head_dim)\n    out = self.proj_out(out).permute(0, 2, 1).reshape(in_shape)\n\n    return out\n</code></pre>"},{"location":"reference/chuchichaestli/models/downsampling/","title":"chuchichaestli.models.downsampling","text":""},{"location":"reference/chuchichaestli/models/downsampling/#chuchichaestli.models.downsampling","title":"chuchichaestli.models.downsampling","text":"<p>Downsampling modules for 1, 2, and 3D inputs.</p> <p>Classes:</p> Name Description <code>Downsample</code> <p>Downsampling layer for 1D, 2D, and 3D inputs.</p> <code>DownsampleInterpolate</code> <p>Downsampling layer for 1D, 2D, and 3D inputs implemented with interpolation.</p>"},{"location":"reference/chuchichaestli/models/downsampling/#chuchichaestli.models.downsampling.Downsample","title":"Downsample","text":"<pre><code>Downsample(dimensions: int, num_channels: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Downsampling layer for 1D, 2D, and 3D inputs.</p> <p>Initialize the downsampling layer.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the downsampling layer.</p> Source code in <code>src/chuchichaestli/models/downsampling.py</code> <pre><code>def __init__(self, dimensions: int, num_channels: int):\n    \"\"\"Initialize the downsampling layer.\"\"\"\n    super().__init__()\n    conv_cls = DIM_TO_CONV_MAP[dimensions]\n    self.conv = conv_cls(\n        num_channels, num_channels, kernel_size=3, stride=2, padding=1\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/downsampling/#chuchichaestli.models.downsampling.Downsample.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the downsampling layer.</p> Source code in <code>src/chuchichaestli/models/downsampling.py</code> <pre><code>def forward(self, x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the downsampling layer.\"\"\"\n    return self.conv(x)\n</code></pre>"},{"location":"reference/chuchichaestli/models/downsampling/#chuchichaestli.models.downsampling.DownsampleInterpolate","title":"DownsampleInterpolate","text":"<pre><code>DownsampleInterpolate(\n    dimensions: int,\n    num_channels: int | None = None,\n    factor: int | None = None,\n    antialias: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Downsampling layer for 1D, 2D, and 3D inputs implemented with interpolation.</p> <p>Note: In the U-Net architecture, downsampling by interpolation is not commonly used.</p> <p>Initialize the downsampling layer.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the downsampling layer.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>Literal['linear', 'bilinear', 'trilinear', 'nearest']</code> <p>Interpolation mode.</p> Source code in <code>src/chuchichaestli/models/downsampling.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    num_channels: int | None = None,\n    factor: int | None = None,\n    antialias: bool = False,\n):\n    \"\"\"Initialize the downsampling layer.\"\"\"\n    super().__init__()\n    self.dimensions = dimensions\n    self.num_channels = num_channels\n    self.factor = factor if factor is not None else 2\n    self.align_corners = False\n    self.antialias = antialias\n</code></pre>"},{"location":"reference/chuchichaestli/models/downsampling/#chuchichaestli.models.downsampling.DownsampleInterpolate.mode","title":"mode  <code>property</code>","text":"<pre><code>mode: Literal['linear', 'bilinear', 'trilinear', 'nearest']\n</code></pre> <p>Interpolation mode.</p>"},{"location":"reference/chuchichaestli/models/downsampling/#chuchichaestli.models.downsampling.DownsampleInterpolate.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the downsampling layer.</p> Source code in <code>src/chuchichaestli/models/downsampling.py</code> <pre><code>def forward(self, x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the downsampling layer.\"\"\"\n    spatial_dims = x.shape[2:]\n    output_dims = [s // self.factor for s in spatial_dims]\n    return F.interpolate(\n        x,\n        size=output_dims,\n        mode=self.mode,\n        align_corners=self.align_corners,\n        antialias=self.antialias,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/maps/","title":"chuchichaestli.models.maps","text":""},{"location":"reference/chuchichaestli/models/maps/#chuchichaestli.models.maps","title":"chuchichaestli.models.maps","text":"<p>Dim-to-Layer maps for different spatial dimensions.</p>"},{"location":"reference/chuchichaestli/models/norm/","title":"chuchichaestli.models.norm","text":""},{"location":"reference/chuchichaestli/models/norm/#chuchichaestli.models.norm","title":"chuchichaestli.models.norm","text":"<p>Normalization modules for neural networks.</p> <p>Classes:</p> Name Description <code>Norm</code> <p>Normalization layer implementation.</p>"},{"location":"reference/chuchichaestli/models/norm/#chuchichaestli.models.norm.AdaptiveBatchNorm","title":"AdaptiveBatchNorm","text":"<pre><code>AdaptiveBatchNorm(\n    dimensions: int,\n    channels: int,\n    eps: float = 1e-05,\n    momentum: float = 0.1,\n    affine: bool = True,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Adaptive BN implementation with two additional parameters.</p> <p>Constructor.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Adaptive BN with two additional parameters <code>a</code> and <code>b</code>.</p> Source code in <code>src/chuchichaestli/models/norm.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    channels: int,\n    eps: float = 1e-5,\n    momentum: float = 0.1,\n    affine: bool = True,\n):\n    \"\"\"Constructor.\"\"\"\n    super().__init__()\n    self.bn = Norm(\n        dimensions,\n        \"batch\",\n        channels,\n        0,\n        eps=eps,\n        momentum=momentum,\n        affine=affine,\n    )\n    self.a = nn.Parameter(torch.FloatTensor(1, 1, *((1,) * dimensions)))\n    self.b = nn.Parameter(torch.FloatTensor(1, 1, *((1,) * dimensions)))\n</code></pre>"},{"location":"reference/chuchichaestli/models/norm/#chuchichaestli.models.norm.AdaptiveBatchNorm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Adaptive BN with two additional parameters <code>a</code> and <code>b</code>.</p> Return <p>a * x + b * bn(x)</p> Source code in <code>src/chuchichaestli/models/norm.py</code> <pre><code>def forward(self, x):\n    \"\"\"Adaptive BN with two additional parameters `a` and `b`.\n\n    Return:\n      a * x + b * bn(x)\n    \"\"\"\n    return self.a * x + self.b * self.bn(x)\n</code></pre>"},{"location":"reference/chuchichaestli/models/norm/#chuchichaestli.models.norm.Norm","title":"Norm","text":"<pre><code>Norm(\n    dimensions: int,\n    norm_type: Literal[\n        \"group\", \"instance\", \"batch\", \"adabatch\"\n    ],\n    channels: int,\n    num_groups: int,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Normalization layer implementation.</p> <p>Initialize the normalization layer.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the normalization layer.</p> Source code in <code>src/chuchichaestli/models/norm.py</code> <pre><code>def __init__(\n    self, dimensions: int, norm_type: Literal[\"group\", \"instance\", \"batch\", \"adabatch\"], channels: int, num_groups: int, **kwargs\n):\n    \"\"\"Initialize the normalization layer.\"\"\"\n    super().__init__()\n    self.norm: nn.Module\n    match norm_type:\n        case \"group\":\n            self.norm = nn.GroupNorm(num_groups, channels)\n        case \"instance\" if dimensions == 1:\n            self.norm = nn.InstanceNorm1d(channels)\n        case \"instance\" if dimensions == 2:\n            self.norm = nn.InstanceNorm2d(channels)\n        case \"instance\" if dimensions == 3:\n            self.norm = nn.InstanceNorm3d(channels)\n        case \"batch\" if dimensions == 1:\n            self.norm = nn.BatchNorm1d(channels, **kwargs)\n        case \"batch\" if dimensions == 2:\n            self.norm = nn.BatchNorm2d(channels, **kwargs)\n        case \"batch\" if dimensions == 3:\n            self.norm = nn.BatchNorm3d(channels, **kwargs)\n        case \"adabatch\":\n            self.norm = AdaptiveBatchNorm(dimensions, channels, **kwargs)\n</code></pre>"},{"location":"reference/chuchichaestli/models/norm/#chuchichaestli.models.norm.Norm.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor)\n</code></pre> <p>Forward pass through the normalization layer.</p> Source code in <code>src/chuchichaestli/models/norm.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"Forward pass through the normalization layer.\"\"\"\n    return self.norm(x)\n</code></pre>"},{"location":"reference/chuchichaestli/models/resnet/","title":"chuchichaestli.models.resnet","text":""},{"location":"reference/chuchichaestli/models/resnet/#chuchichaestli.models.resnet","title":"chuchichaestli.models.resnet","text":"<p>ResNet block implementation.</p> <p>Classes:</p> Name Description <code>ResidualBlock</code> <p>Residual block implementation.</p>"},{"location":"reference/chuchichaestli/models/resnet/#chuchichaestli.models.resnet.ResidualBlock","title":"ResidualBlock","text":"<pre><code>ResidualBlock(\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    time_embedding: bool,\n    time_channels: int,\n    res_groups: int = 32,\n    res_act_fn: str = \"silu\",\n    res_dropout: float = 0.1,\n    res_norm_type: str = \"group\",\n    res_kernel_size: int = 3,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Residual block implementation.</p> <p>Initialize the residual block.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the residual block.</p> Source code in <code>src/chuchichaestli/models/resnet.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    time_embedding: bool,\n    time_channels: int,\n    res_groups: int = 32,\n    res_act_fn: str = \"silu\",\n    res_dropout: float = 0.1,\n    res_norm_type: str = \"group\",\n    res_kernel_size: int = 3,\n):\n    \"\"\"Initialize the residual block.\"\"\"\n    super().__init__()\n    act_cls = ACTIVATION_FUNCTIONS[res_act_fn]\n    conv_cls = DIM_TO_CONV_MAP[dimensions]\n\n    self.dimensions = dimensions\n\n    self.norm1 = Norm(dimensions, res_norm_type, in_channels, res_groups)\n    self.act1 = act_cls()\n    self.conv1 = conv_cls(\n        in_channels, out_channels, kernel_size=res_kernel_size, padding=\"same\"\n    )\n\n    self.norm2 = Norm(dimensions, res_norm_type, out_channels, res_groups)\n    self.act2 = act_cls()\n    self.conv2 = conv_cls(\n        out_channels, out_channels, kernel_size=res_kernel_size, padding=\"same\"\n    )\n\n    self.shortcut = (\n        conv_cls(in_channels, out_channels, kernel_size=1)\n        if in_channels != out_channels\n        else nn.Identity()\n    )\n\n    self.time_embedding = time_embedding\n    if time_embedding:\n        self.time_proj = nn.Linear(time_channels, out_channels)\n        self.time_act = act_cls()\n\n    self.dropout = nn.Dropout(res_dropout)\n</code></pre>"},{"location":"reference/chuchichaestli/models/resnet/#chuchichaestli.models.resnet.ResidualBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, t: torch.Tensor | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the residual block.</p> Source code in <code>src/chuchichaestli/models/resnet.py</code> <pre><code>def forward(self, x: torch.Tensor, t: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the residual block.\"\"\"\n    hh = self.conv1(self.act1(self.norm1(x)))\n    idx = [slice(None), slice(None)] + [None] * self.dimensions\n    if self.time_embedding:\n        hh += self.time_proj(self.time_act(t))[idx]\n    hh = self.conv2(self.dropout(self.act2(self.norm2(hh))))\n\n    return hh + self.shortcut(x)\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/","title":"chuchichaestli.models.unet","text":""},{"location":"reference/chuchichaestli/models/unet/#chuchichaestli.models.unet","title":"chuchichaestli.models.unet","text":"<p>UNet model implementation and utilities.</p> <p>Modules:</p> Name Description <code>blocks</code> <p>UNet building blocks.</p> <code>time_embeddings</code> <p>Time embeddings for the U-Net model.</p> <code>unet</code> <p>A highly customizable U-Net model implementation.</p>"},{"location":"reference/chuchichaestli/models/unet/blocks/","title":"chuchichaestli.models.unet.blocks","text":""},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks","title":"chuchichaestli.models.unet.blocks","text":"<p>UNet building blocks.</p> <p>Classes:</p> Name Description <code>DownBlock</code> <p>Down block for UNet.</p> <code>GaussianNoiseBlock</code> <p>Gaussian noise regularizer.</p> <code>MidBlock</code> <p>Mid block for UNet.</p> <code>UpBlock</code> <p>Up block for UNet.</p>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.DownBlock","title":"DownBlock","text":"<pre><code>DownBlock(\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    time_embedding: bool = False,\n    time_channels: int = 32,\n    res_args: dict = {},\n    attention: Literal[\"self_attention\", \"conv_attention\"]\n    | None = None,\n    attn_args: dict = {},\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Down block for UNet.</p> <p>Initialize the down block.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the down block.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    time_embedding: bool = False,\n    time_channels: int = 32,\n    res_args: dict = {},\n    attention: Literal[\"self_attention\", \"conv_attention\"] | None = None,\n    attn_args: dict = {},\n):\n    \"\"\"Initialize the down block.\"\"\"\n    super().__init__()\n    self.res_block = ResidualBlock(\n        dimensions,\n        in_channels,\n        out_channels,\n        time_embedding,\n        time_channels,\n        **res_args,\n    )\n\n    match attention:\n        case \"self_attention\":\n            self.attn = ATTENTION_MAP[attention](in_channels, **attn_args)\n        case \"conv_attention\":\n            self.attn = ATTENTION_MAP[attention](\n                dimensions, in_channels, **attn_args\n            )\n        case _:\n            self.attn = None\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.DownBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, t: torch.Tensor | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the down block.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def forward(self, x: torch.Tensor, t: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the down block.\"\"\"\n    x = self.attn(x, None) if self.attn else x\n    x = self.res_block(x, t)\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.GaussianNoiseBlock","title":"GaussianNoiseBlock","text":"<pre><code>GaussianNoiseBlock(\n    sigma: float = 0.1,\n    mu: float = 0.0,\n    detached: bool = True,\n    device: torch.device | str | None = None,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Gaussian noise regularizer.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Relative (to the magnitude of the input) standard deviation for noise generation.</p> <code>0.1</code> <code>mu</code> <code>float</code> <p>Mean for the noise generation.</p> <code>0.0</code> <code>detached</code> <code>bool</code> <p>If True, the input is detached for the noise generation.</p> <code>True</code> <code>device</code> <code>torch.device | str | None</code> <p>Compute device where to pass the noise.</p> <code>None</code> If detached=False, the network sees the noise as a trainable parameter <p>(no reparametrization trick) and introduce a bias to generate vectors closer to the noise level.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass using the reparametrization trick.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def __init__(\n    self,\n    sigma: float = 0.1,\n    mu: float = 0.0,\n    detached: bool = True,\n    device: torch.device | str | None = None,\n):\n    \"\"\"Constructor.\n\n    Args:\n      sigma: Relative (to the magnitude of the input) standard deviation for noise generation.\n      mu: Mean for the noise generation.\n      detached: If True, the input is detached for the noise generation.\n      device: Compute device where to pass the noise.\n\n    Note: If detached=False, the network sees the noise as a trainable parameter\n      (no reparametrization trick) and introduce a bias to generate vectors closer\n      to the noise level.\n    \"\"\"\n    super().__init__()\n    self.sigma = nn.Parameter(torch.tensor(sigma))\n    self.detached = detached\n    self.noise = torch.tensor(mu)\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.GaussianNoiseBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, *args, noise_at_inference: bool = False\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass using the reparametrization trick.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, *args, noise_at_inference: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass using the reparametrization trick.\"\"\"\n    if (self.training or noise_at_inference) and self.sigma != 0:\n        scale = self.sigma * x.detach() if self.detached else self.sigma * x\n        sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n        x = x + sampled_noise\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.MidBlock","title":"MidBlock","text":"<pre><code>MidBlock(\n    dimensions: int,\n    channels: int,\n    time_embedding: bool = False,\n    time_channels: int = 32,\n    res_args: dict = {},\n    attention: Literal[\"self_attention\", \"conv_attention\"]\n    | None = None,\n    attn_args: dict = {},\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Mid block for UNet.</p> <p>Initialize the mid block.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the mid block.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    channels: int,\n    time_embedding: bool = False,\n    time_channels: int = 32,\n    res_args: dict = {},\n    attention: Literal[\"self_attention\", \"conv_attention\"] | None = None,\n    attn_args: dict = {},\n):\n    \"\"\"Initialize the mid block.\"\"\"\n    super().__init__()\n    self.res_block = ResidualBlock(\n        dimensions,\n        channels,\n        channels,\n        time_embedding,\n        time_channels,\n        **res_args,\n    )\n    match attention:\n        case \"self_attention\":\n            self.attn = ATTENTION_MAP[attention](channels, **attn_args)\n        case \"conv_attention\":\n            self.attn = ATTENTION_MAP[attention](dimensions, channels, **attn_args)\n        case _:\n            self.attn = None\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.MidBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, t: torch.Tensor | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the mid block.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def forward(self, x: torch.Tensor, t: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the mid block.\"\"\"\n    x = self.attn(x, None) if self.attn else x\n    x = self.res_block(x, t)\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.UpBlock","title":"UpBlock","text":"<pre><code>UpBlock(\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    time_embedding: bool = False,\n    time_channels: int = 32,\n    res_args: dict = {},\n    attention: Literal[\n        \"self_attention\", \"conv_attention\", \"attention_gate\"\n    ]\n    | None = None,\n    attn_args: dict = {},\n    skip_connection_action: Literal[\"concat\", \"avg\", \"add\"]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Up block for UNet.</p> <p>Initialize the up block.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the up block.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    in_channels: int,\n    out_channels: int,\n    time_embedding: bool = False,\n    time_channels: int = 32,\n    res_args: dict = {},\n    attention: Literal[\"self_attention\", \"conv_attention\", \"attention_gate\"]\n    | None = None,\n    attn_args: dict = {},\n    skip_connection_action: Literal[\"concat\", \"avg\", \"add\"] | None = None,\n):\n    \"\"\"Initialize the up block.\"\"\"\n    super().__init__()\n    self.skip_connection_action = skip_connection_action\n    if skip_connection_action == \"concat\":\n        self.res_block = ResidualBlock(\n            dimensions,\n            in_channels + in_channels,\n            out_channels,\n            time_embedding,\n            time_channels,\n            **res_args,\n        )\n    elif skip_connection_action in [\"avg\", \"add\", None]:\n        self.res_block = ResidualBlock(\n            dimensions,\n            in_channels,\n            out_channels,\n            time_embedding,\n            time_channels,\n            **res_args,\n        )\n    else:\n        raise ValueError(\n            f\"Invalid skip connection action: {skip_connection_action}\"\n        )\n\n    match attention:\n        case \"self_attention\":\n            self.attn = ATTENTION_MAP[attention](in_channels, **attn_args)\n        case \"conv_attention\":\n            self.attn = ATTENTION_MAP[attention](\n                dimensions, in_channels, **attn_args\n            )\n        case \"attention_gate\":\n            self.attn = ATTENTION_MAP[attention](\n                dimensions, in_channels, in_channels, **attn_args\n            )\n        case _:\n            self.attn = None\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/blocks/#chuchichaestli.models.unet.blocks.UpBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor,\n    h: torch.Tensor,\n    t: torch.Tensor | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the up block.</p> Source code in <code>src/chuchichaestli/models/unet/blocks.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, h: torch.Tensor, t: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the up block.\"\"\"\n    x = self.attn(x, h) if self.attn else x\n    if self.skip_connection_action == \"avg\":\n        replication_factor = x.shape[1] // h.shape[1]\n        h = h.repeat(\n            (1, replication_factor) + (1,) * len(x.shape[2:])\n        )  # Repeat channels\n        xh = (x + h) / 2\n    elif self.skip_connection_action == \"add\":\n        replication_factor = x.shape[1] // h.shape[1]\n        h = h.repeat(\n            (1, replication_factor) + (1,) * len(x.shape[2:])\n        )  # Repeat channels\n        xh = x + h\n    elif self.skip_connection_action == \"concat\":\n        xh = torch.cat([x, h], dim=1)\n    else:\n        xh = x\n    x = self.res_block(xh, t)\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/","title":"chuchichaestli.models.unet.time_embeddings","text":""},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings","title":"chuchichaestli.models.unet.time_embeddings","text":"<p>Time embeddings for the U-Net model.</p> <p>Adapted from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py. Original code is licensed under the Apache License, Version 2.0. Modifications made by CAIIVS are licensed under the GNU General Public License v3.0.</p> <p>Classes:</p> Name Description <code>DeepSinusoidalTimeEmbedding</code> <p>Deep sinusoidal time embeddings, i.e. sinusoidal embedding and MLP).</p> <code>GaussianFourierProjection</code> <p>Gaussian Fourier embeddings for noise levels.</p> <code>SinusoidalTimeEmbedding</code> <p>Sinusoidal time embeddings as described in Denoising Diffusion Probabilistic Models.</p>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.DeepSinusoidalTimeEmbedding","title":"DeepSinusoidalTimeEmbedding","text":"<pre><code>DeepSinusoidalTimeEmbedding(\n    num_channels: int,\n    embedding_dim: int | None = None,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1.0,\n    activation: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    post_activation: bool = False,\n    condition_dim: int = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Deep sinusoidal time embeddings, i.e. sinusoidal embedding and MLP).</p> <p>Deep sinusoidal time embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>num_channels</code> <code>int</code> <p>The number of channels.</p> required <code>embedding_dim</code> <code>int | None</code> <p>The dimension for the deep embedding.</p> <code>None</code> <code>flip_sin_to_cos</code> <code>bool</code> <p>Whether to flip the sin to cos.</p> <code>False</code> <code>downscale_freq_shift</code> <code>float</code> <p>The downscale frequency shift.</p> <code>1.0</code> <code>activation</code> <code>Literal['silu', 'swish', 'mish', 'gelu', 'relu', 'prelu', 'leakyrelu', 'leakyrelu,0.1', 'leakyrelu,0.2', 'softplus']</code> <p>The activation function. Defaults to \"silu\".</p> <code>'silu'</code> <code>post_activation</code> <code>bool</code> <p>Whether to use an activation function at the end.</p> <code>False</code> <code>condition_dim</code> <code>int</code> <p>The condition dimension. Defaults to None. If set, will condition the input on the condition tensor.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments for compatibility (have no effect).</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward step.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def __init__(\n    self,\n    num_channels: int,\n    embedding_dim: int | None = None,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1.0,\n    activation: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    post_activation: bool = False,\n    condition_dim: int = None,\n    **kwargs,\n):\n    \"\"\"Deep sinusoidal time embeddings.\n\n    Args:\n        num_channels: The number of channels.\n        embedding_dim: The dimension for the deep embedding.\n        flip_sin_to_cos: Whether to flip the sin to cos.\n        downscale_freq_shift: The downscale frequency shift.\n        activation: The activation function. Defaults to \"silu\".\n        post_activation: Whether to use an activation function at the end.\n        condition_dim: The condition dimension. Defaults to None.\n            If set, will condition the input on the condition tensor.\n        kwargs: Additional keyword arguments for compatibility (have no effect).\n    \"\"\"\n    super().__init__()\n    self.sinusoidal_embedding = SinusoidalTimeEmbedding(\n        num_channels=num_channels,\n        flip_sin_to_cos=flip_sin_to_cos,\n        downscale_freq_shift=downscale_freq_shift,\n    )\n    embedding_dim = embedding_dim if embedding_dim is not None else num_channels\n    self.mlp = TimestepEmbedding(\n        num_channels,\n        embedding_dim,\n        out_dim=num_channels,\n        activation=activation,\n        post_activation=post_activation,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.DeepSinusoidalTimeEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    timesteps: torch.Tensor,\n    scale: float = 1.0,\n    max_period: float = 10000.0,\n    condition: torch.Tensor | None = None,\n)\n</code></pre> <p>Forward step.</p> <p>Parameters:</p> Name Type Description Default <code>timesteps</code> <code>torch.Tensor</code> <p>a 1-D Tensor of N indices, one per batch element. These may be fractional.</p> required <code>scale</code> <code>float</code> <p>The scale of the embeddings. Defaults to 1.0.</p> <code>1.0</code> <code>max_period</code> <code>float</code> <p>controls the minimum frequency of the embeddings.</p> <code>10000.0</code> <code>condition</code> <code>torch.Tensor | None</code> <p>An optional tensor to condition the input.</p> <code>None</code> <p>Returns:</p> Type Description <p>[N x dim] Tensor of positional embeddings.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def forward(\n    self,\n    timesteps: torch.Tensor,\n    scale: float = 1.0,\n    max_period: float = 1e4,\n    condition: torch.Tensor | None = None,\n):\n    \"\"\"Forward step.\n\n    Args:\n        timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n        scale: The scale of the embeddings. Defaults to 1.0.\n        max_period: controls the minimum frequency of the embeddings.\n        condition: An optional tensor to condition the input.\n\n    Returns:\n        [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    emb = self.sinusoidal_embedding(timesteps, scale=scale, max_period=max_period)\n    emb = self.mlp(emb, condition=condition)\n    return emb\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.GaussianFourierProjection","title":"GaussianFourierProjection","text":"<pre><code>GaussianFourierProjection(\n    embedding_size: int = 256,\n    scale: float = 1.0,\n    log: bool = True,\n    flip_sin_to_cos: bool = False,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Gaussian Fourier embeddings for noise levels.</p> <p>Gaussian Fourier embeddings for noise levels.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_size</code> <code>int</code> <p>The size of the embedding. Defaults to 256.</p> <code>256</code> <code>scale</code> <code>float</code> <p>The scale of the embedding. Defaults to 1.0.</p> <code>1.0</code> <code>log</code> <code>bool</code> <p>Whether to take the log of the input. Defaults to True.</p> <code>True</code> <code>flip_sin_to_cos</code> <code>bool</code> <p>Whether to flip the sin to cos. Defaults to False.</p> <code>False</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def __init__(\n    self,\n    embedding_size: int = 256,\n    scale: float = 1.0,\n    log: bool = True,\n    flip_sin_to_cos: bool = False,\n):\n    \"\"\"Gaussian Fourier embeddings for noise levels.\n\n    Args:\n        embedding_size: The size of the embedding. Defaults to 256.\n        scale: The scale of the embedding. Defaults to 1.0.\n        log: Whether to take the log of the input. Defaults to True.\n        flip_sin_to_cos: Whether to flip the sin to cos. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.weight = nn.Parameter(\n        torch.randn(embedding_size) * scale, requires_grad=False\n    )\n    self.log = log\n    self.flip_sin_to_cos = flip_sin_to_cos\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.GaussianFourierProjection.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor)\n</code></pre> <p>Forward pass.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"Forward pass.\"\"\"\n    if self.log:\n        x = torch.log(x)\n\n    x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi\n\n    if self.flip_sin_to_cos:\n        out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)\n    else:\n        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n    return out\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.SinusoidalTimeEmbedding","title":"SinusoidalTimeEmbedding","text":"<pre><code>SinusoidalTimeEmbedding(\n    num_channels: int,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1.0,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Sinusoidal time embeddings as described in Denoising Diffusion Probabilistic Models.</p> <p>Sinusoidal time embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>num_channels</code> <code>int</code> <p>The number of channels.</p> required <code>flip_sin_to_cos</code> <code>bool</code> <p>Whether to flip the sin to cos.</p> <code>False</code> <code>downscale_freq_shift</code> <code>float</code> <p>The downscale frequency shift.</p> <code>1.0</code> <code>kwargs</code> <p>Additional keyword arguments for compatibility (have no effect).</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward step.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def __init__(\n    self,\n    num_channels: int,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1.0,\n    **kwargs,\n):\n    \"\"\"Sinusoidal time embeddings.\n\n    Args:\n        num_channels: The number of channels.\n        flip_sin_to_cos: Whether to flip the sin to cos.\n        downscale_freq_shift: The downscale frequency shift.\n        kwargs: Additional keyword arguments for compatibility (have no effect).\n    \"\"\"\n    super().__init__()\n    self.num_channels = num_channels\n    self.flip_sin_to_cos = flip_sin_to_cos\n    self.downscale_freq_shift = downscale_freq_shift\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.SinusoidalTimeEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    timesteps: torch.Tensor,\n    scale: float = 1.0,\n    max_period: float = 10000.0,\n)\n</code></pre> <p>Forward step.</p> <p>Parameters:</p> Name Type Description Default <code>timesteps</code> <code>torch.Tensor</code> <p>a 1-D Tensor of N indices, one per batch element. These may be fractional.</p> required <code>scale</code> <code>float</code> <p>The scale of the embeddings. Defaults to 1.0.</p> <code>1.0</code> <code>max_period</code> <code>float</code> <p>controls the minimum frequency of the embeddings.</p> <code>10000.0</code> <p>Returns:</p> Type Description <p>[N x dim] Tensor of positional embeddings.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def forward(\n    self, timesteps: torch.Tensor, scale: float = 1.0, max_period: float = 1e4\n):\n    \"\"\"Forward step.\n\n    Args:\n        timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n        scale: The scale of the embeddings. Defaults to 1.0.\n        max_period: controls the minimum frequency of the embeddings.\n\n    Returns:\n        [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half_dim = self.num_channels // 2\n    exponent = -math.log(max_period) * torch.arange(\n        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n    )\n    exponent = exponent / (half_dim - self.downscale_freq_shift)\n\n    emb = torch.exp(exponent)\n    emb = timesteps[:, None].float() * emb[None, :]\n\n    # scale embeddings\n    emb = scale * emb\n\n    # concat sine and cosine embeddings\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n    # flip sine and cosine embeddings\n    if self.flip_sin_to_cos:\n        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n\n    # zero pad\n    if self.num_channels % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.TimestepEmbedding","title":"TimestepEmbedding","text":"<pre><code>TimestepEmbedding(\n    input_dim: int,\n    embedding_dim: int,\n    out_dim: int = None,\n    activation: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    post_activation: bool = False,\n    condition_dim: int | None = None,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Linear timestep embedding with optional conditioning.</p> <p>Typicall used to further process sinusoidal embedding representations.</p> <p>Linear timestep embedding.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The input dimension.</p> required <code>embedding_dim</code> <code>int</code> <p>The embedding dimension.</p> required <code>out_dim</code> <code>int</code> <p>The output dimension. If not set, will use embedding_dim.</p> <code>None</code> <code>activation</code> <code>Literal['silu', 'swish', 'mish', 'gelu', 'relu', 'prelu', 'leakyrelu', 'leakyrelu,0.1', 'leakyrelu,0.2', 'softplus']</code> <p>The activation function. Defaults to \"silu\".</p> <code>'silu'</code> <code>post_activation</code> <code>bool</code> <p>Whether to use an activation function at the end.</p> <code>False</code> <code>condition_dim</code> <code>int | None</code> <p>The condition dimension. Defaults to None. If set, will condition the input on the condition tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass.</p> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    embedding_dim: int,\n    out_dim: int = None,\n    activation: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    post_activation: bool = False,\n    condition_dim: int | None = None,\n):\n    \"\"\"Linear timestep embedding.\n\n    Args:\n        input_dim: The input dimension.\n        embedding_dim: The embedding dimension.\n        out_dim: The output dimension. If not set, will use embedding_dim.\n        activation: The activation function. Defaults to \"silu\".\n        post_activation: Whether to use an activation function at the end.\n        condition_dim: The condition dimension. Defaults to None.\n            If set, will condition the input on the condition tensor.\n    \"\"\"\n    super().__init__()\n    self.linear_1 = nn.Linear(input_dim, embedding_dim, bias=True)\n    self.activation = ACTIVATION_FUNCTIONS.get(activation, \"silu\")()\n    self.linear_2 = nn.Linear(\n        embedding_dim, out_dim if out_dim is not None else embedding_dim, bias=True\n    )\n    self.post_activation = (\n        ACTIVATION_FUNCTIONS.get(activation, \"silu\")()\n        if post_activation\n        else nn.Identity()\n    )\n    self.proj_cond = (\n        nn.Linear(condition_dim, input_dim, bias=True)\n        if condition_dim is not None\n        else None\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/time_embeddings/#chuchichaestli.models.unet.time_embeddings.TimestepEmbedding.forward","title":"forward","text":"<pre><code>forward(\n    sample: torch.Tensor,\n    condition: torch.Tensor | None = None,\n)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>torch.Tensor</code> <p>The input tensor.</p> required <code>condition</code> <code>torch.Tensor | None</code> <p>An optional tensor to condition the input.</p> <code>None</code> Source code in <code>src/chuchichaestli/models/unet/time_embeddings.py</code> <pre><code>def forward(self, sample: torch.Tensor, condition: torch.Tensor | None = None):\n    \"\"\"Forward pass.\n\n    Args:\n        sample: The input tensor.\n        condition: An optional tensor to condition the input.\n    \"\"\"\n    if condition is not None and self.proj_cond is not None:\n        sample += self.proj_cond(condition)\n    sample = self.linear_1(sample)\n    sample = self.activation(sample)\n    sample = self.linear_2(sample)\n    sample = self.post_activation(sample)\n    return sample\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/unet/","title":"chuchichaestli.models.unet.unet","text":""},{"location":"reference/chuchichaestli/models/unet/unet/#chuchichaestli.models.unet.unet","title":"chuchichaestli.models.unet.unet","text":"<p>A highly customizable U-Net model implementation.</p> <p>Classes:</p> Name Description <code>UNet</code> <p>Highly customizable U-Net model implementation.</p>"},{"location":"reference/chuchichaestli/models/unet/unet/#chuchichaestli.models.unet.unet.UNet","title":"UNet","text":"<pre><code>UNet(\n    dimensions: int = 2,\n    in_channels: int = 1,\n    n_channels: int = 32,\n    out_channels: int = 1,\n    down_block_types: Sequence[\n        Literal[\n            \"DownBlock\",\n            \"AttnDownBlock\",\n            \"ConvAttnDownBlock\",\n        ]\n    ] = (\n        \"DownBlock\",\n        \"DownBlock\",\n        \"AttnDownBlock\",\n        \"AttnDownBlock\",\n    ),\n    mid_block_type: Literal[\n        \"MidBlock\", \"AttnMidBlock\", \"ConvAttnMidBlock\"\n    ] = \"MidBlock\",\n    up_block_types: Sequence[\n        Literal[\n            \"UpBlock\",\n            \"AttnUpBlock\",\n            \"ConvAttnUpBlock\",\n            \"AttnGateUpBlock\",\n        ]\n    ] = (\n        \"UpBlock\",\n        \"UpBlock\",\n        \"AttnUpBlock\",\n        \"AttnUpBlock\",\n    ),\n    block_out_channel_mults: Sequence[int] = (1, 2, 2, 4),\n    num_blocks_per_level: int = 1,\n    upsample_type: Literal[\n        \"Upsample\", \"UpsampleInterpolate\"\n    ] = \"Upsample\",\n    downsample_type: Literal[\n        \"Downsample\", \"DownsampleInterpolate\"\n    ] = \"Downsample\",\n    act: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    groups: int = 8,\n    in_kernel_size: int = 3,\n    out_kernel_size: int = 3,\n    time_embedding: Literal[\n        \"SinusoidalTimeEmbedding\",\n        \"DeepSinusoidalTimeEmbedding\",\n    ]\n    | bool\n    | None = None,\n    time_channels: int = 32,\n    t_emb_dim: int = 32,\n    t_emb_flip: bool = False,\n    t_emb_shift: float = 1.0,\n    t_emb_act_fn: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    t_emb_post_act: bool = False,\n    t_emb_condition_dim: int | None = None,\n    res_act_fn: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    res_dropout: float = 0.1,\n    res_norm_type: Literal[\n        \"group\", \"instance\", \"batch\", \"adabatch\"\n    ] = \"group\",\n    res_groups: int = 32,\n    res_kernel_size: int = 3,\n    attn_head_dim: int = 32,\n    attn_n_heads: int = 1,\n    attn_dropout_p: float = 0.0,\n    attn_norm_type: Literal[\n        \"group\", \"instance\", \"batch\", \"adabatch\"\n    ] = \"group\",\n    attn_groups: int = 32,\n    attn_kernel_size: int = 1,\n    attn_gate_inter_channels: int = 32,\n    skip_connection_action: Literal[\"concat\", \"avg\", \"add\"]\n    | None = \"concat\",\n    skip_connection_to_all_blocks: bool | None = None,\n    add_noise: Literal[\"up\", \"down\"] | None = None,\n    noise_sigma: float = 0.1,\n    noise_detached: bool = True,\n)\n</code></pre> <p>               Bases: <code>nn.Module</code></p> <p>Highly customizable U-Net model implementation.</p> <p>The architecture consists of an encoder-decoder structure with skip connections. The encoder chains several convolutional (residual) and downsampling blocks. Each downsampling block separates the encoder into spatially hierarchical levels. The decoder is built symmetrically to the encoder with (residual) transposed convolutional and upsampling blocks, each level linked via skip connections which ensure spatial information is passed through the network.</p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>int</code> <p>Number of (spatial) dimensions.</p> <code>2</code> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>1</code> <code>n_channels</code> <code>int</code> <p>Number of channels in the first block.</p> <code>32</code> <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> <code>1</code> <code>down_block_types</code> <code>Sequence[Literal['DownBlock', 'AttnDownBlock', 'ConvAttnDownBlock']]</code> <p>Types of down blocks as a list, starting at the first block (in the highest level).</p> <code>('DownBlock', 'DownBlock', 'AttnDownBlock', 'AttnDownBlock')</code> <code>mid_block_type</code> <code>Literal['MidBlock', 'AttnMidBlock', 'ConvAttnMidBlock']</code> <p>Type of mid block.</p> <code>'MidBlock'</code> <code>up_block_types</code> <code>Sequence[Literal['UpBlock', 'AttnUpBlock', 'ConvAttnUpBlock', 'AttnGateUpBlock']]</code> <p>Types of up blocks as a list, starting with the last block (lowest level).</p> <code>('UpBlock', 'UpBlock', 'AttnUpBlock', 'AttnUpBlock')</code> <code>block_out_channel_mults</code> <code>Sequence[int]</code> <p>Output channel multipliers for each block.</p> <code>(1, 2, 2, 4)</code> <code>num_blocks_per_level</code> <code>int</code> <p>Number of blocks per level (blocks are repeated if <code>&gt;1</code>).</p> <code>1</code> <code>upsample_type</code> <code>Literal['Upsample', 'UpsampleInterpolate']</code> <p>Type of upsampling block (see <code>chuchichaestli.models.upsampling</code> for details).</p> <code>'Upsample'</code> <code>downsample_type</code> <code>Literal['Downsample', 'DownsampleInterpolate']</code> <p>Type of downsampling block (see <code>chuchichaestli.models.downsampling</code> for details).</p> <code>'Downsample'</code> <code>act</code> <code>Literal['silu', 'swish', 'mish', 'gelu', 'relu', 'prelu', 'leakyrelu', 'leakyrelu,0.1', 'leakyrelu,0.2', 'softplus']</code> <p>Activation function for the output layer (see <code>chuchichaestli.models.activations</code> for details).</p> <code>'silu'</code> <code>groups</code> <code>int</code> <p>Number of groups for group normalization in the output layer.</p> <code>8</code> <code>in_kernel_size</code> <code>int</code> <p>Kernel size for the input convolution.</p> <code>3</code> <code>out_kernel_size</code> <code>int</code> <p>Kernel size for the output convolution.</p> <code>3</code> <code>time_embedding</code> <code>Literal['SinusoidalTimeEmbedding', 'DeepSinusoidalTimeEmbedding'] | bool | None</code> <p>Whether to use a time embedding.</p> <code>None</code> <code>time_channels</code> <code>int</code> <p>Number of time channels.</p> <code>32</code> <code>t_emb_dim</code> <code>int</code> <p>The dimension for the deep embedding (takes only effect if <code>time_embedding='DeepSinusoidalTimeEmbedding'</code>).</p> <code>32</code> <code>t_emb_flip</code> <code>bool</code> <p>Whether to flip the sine to cosine in the time embedding.</p> <code>False</code> <code>t_emb_shift</code> <code>float</code> <p>The downscale frequency shift for the time embedding.</p> <code>1.0</code> <code>t_emb_act_fn</code> <code>Literal['silu', 'swish', 'mish', 'gelu', 'relu', 'prelu', 'leakyrelu', 'leakyrelu,0.1', 'leakyrelu,0.2', 'softplus']</code> <p>Activation function for the time embedding.</p> <code>'silu'</code> <code>t_emb_post_act</code> <code>bool</code> <p>Whether to use an activation function at the end of the time embedding.</p> <code>False</code> <code>t_emb_condition_dim</code> <code>int | None</code> <p>The condition dimension for the time embedding.</p> <code>None</code> <code>res_act_fn</code> <code>Literal['silu', 'swish', 'mish', 'gelu', 'relu', 'prelu', 'leakyrelu', 'leakyrelu,0.1', 'leakyrelu,0.2', 'softplus']</code> <p>Activation function for the residual blocks (see <code>chuchichaestli.models.activations</code> for details).</p> <code>'silu'</code> <code>res_dropout</code> <code>float</code> <p>Dropout rate for the residual blocks.</p> <code>0.1</code> <code>res_norm_type</code> <code>Literal['group', 'instance', 'batch', 'adabatch']</code> <p>Normalization type for the residual block (see <code>chuchichaestli.models.norm</code> for details).</p> <code>'group'</code> <code>res_groups</code> <code>int</code> <p>Number of groups for the residual block normalization (if group norm).</p> <code>32</code> <code>res_kernel_size</code> <code>int</code> <p>Kernel size for the residual blocks.</p> <code>3</code> <code>attn_head_dim</code> <code>int</code> <p>Dimension of the attention heads.</p> <code>32</code> <code>attn_n_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>1</code> <code>attn_dropout_p</code> <code>float</code> <p>Dropout probability of the scaled dot product attention.</p> <code>0.0</code> <code>attn_norm_type</code> <code>Literal['group', 'instance', 'batch', 'adabatch']</code> <p>Normalization type for the convolutional attention block (see <code>chuchichaestli.models.norm</code> for details).</p> <code>'group'</code> <code>attn_groups</code> <code>int</code> <p>Number of groups for the convolutional attention block normalization (if <code>attn_norm_type</code> is <code>\"group\"</code>).</p> <code>32</code> <code>attn_kernel_size</code> <code>int</code> <p>Kernel size for the convolutional attention block.</p> <code>1</code> <code>attn_gate_inter_channels</code> <code>int</code> <p>Number of intermediate channels for the attention gate (if <code>up_block_types</code> contains <code>\"AttnGateUpBlock\"</code>).</p> <code>32</code> <code>skip_connection_action</code> <code>Literal['concat', 'avg', 'add'] | None</code> <p>Action to take for the skip connection. If <code>None</code>, no skip connections are used.</p> <code>'concat'</code> <code>skip_connection_to_all_blocks</code> <code>bool | None</code> <p>If <code>True</code>, the U-Net builds skip connections to all blocks in a level, otherwise only to the first block in a level.</p> <code>None</code> <code>add_noise</code> <code>Literal['up', 'down'] | None</code> <p>Add a Gaussian noise regularizer block in the bottleneck (before or after). Can be \"up\" (after the bottleneck) or \"down\" (before the bottleneck).</p> <code>None</code> <code>noise_sigma</code> <code>float</code> <p>Std. relative (to the magnitude of the input) for the noise generation.</p> <code>0.1</code> <code>noise_detached</code> <code>bool</code> <p>If True, the input is detached for the noise generation. Note, this should generally be <code>True</code>, otherwise the noise is learnable.</p> <code>True</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass - optimized but maintains exact original logic.</p> Source code in <code>src/chuchichaestli/models/unet/unet.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int = 2,\n    in_channels: int = 1,\n    n_channels: int = 32,\n    out_channels: int = 1,\n    down_block_types: Sequence[\n        Literal[\"DownBlock\", \"AttnDownBlock\", \"ConvAttnDownBlock\"]\n    ] = (\n        \"DownBlock\",\n        \"DownBlock\",\n        \"AttnDownBlock\",\n        \"AttnDownBlock\",\n    ),\n    mid_block_type: Literal[\n        \"MidBlock\", \"AttnMidBlock\", \"ConvAttnMidBlock\"\n    ] = \"MidBlock\",\n    up_block_types: Sequence[\n        Literal[\"UpBlock\", \"AttnUpBlock\", \"ConvAttnUpBlock\", \"AttnGateUpBlock\"]\n    ] = (\n        \"UpBlock\",\n        \"UpBlock\",\n        \"AttnUpBlock\",\n        \"AttnUpBlock\",\n    ),\n    block_out_channel_mults: Sequence[int] = (1, 2, 2, 4),\n    num_blocks_per_level: int = 1,\n    upsample_type: Literal[\"Upsample\", \"UpsampleInterpolate\"] = \"Upsample\",\n    downsample_type: Literal[\"Downsample\", \"DownsampleInterpolate\"] = \"Downsample\",\n    act: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    groups: int = 8,\n    in_kernel_size: int = 3,\n    out_kernel_size: int = 3,\n    time_embedding: Literal[\n        \"SinusoidalTimeEmbedding\", \"DeepSinusoidalTimeEmbedding\"\n    ]\n    | bool\n    | None = None,\n    time_channels: int = 32,\n    t_emb_dim: int = 32,\n    t_emb_flip: bool = False,\n    t_emb_shift: float = 1.0,\n    t_emb_act_fn: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    t_emb_post_act: bool = False,\n    t_emb_condition_dim: int | None = None,\n    res_act_fn: Literal[\n        \"silu\",\n        \"swish\",\n        \"mish\",\n        \"gelu\",\n        \"relu\",\n        \"prelu\",\n        \"leakyrelu\",\n        \"leakyrelu,0.1\",\n        \"leakyrelu,0.2\",\n        \"softplus\",\n    ] = \"silu\",\n    res_dropout: float = 0.1,\n    res_norm_type: Literal[\"group\", \"instance\", \"batch\", \"adabatch\"] = \"group\",\n    res_groups: int = 32,\n    res_kernel_size: int = 3,\n    attn_head_dim: int = 32,\n    attn_n_heads: int = 1,\n    attn_dropout_p: float = 0.0,\n    attn_norm_type: Literal[\"group\", \"instance\", \"batch\", \"adabatch\"] = \"group\",\n    attn_groups: int = 32,\n    attn_kernel_size: int = 1,\n    attn_gate_inter_channels: int = 32,\n    skip_connection_action: Literal[\"concat\", \"avg\", \"add\"] | None = \"concat\",\n    skip_connection_to_all_blocks: bool | None = None,\n    add_noise: Literal[\"up\", \"down\"] | None = None,\n    noise_sigma: float = 0.1,\n    noise_detached: bool = True,\n):\n    \"\"\"Constructor.\n\n    Args:\n        dimensions: Number of (spatial) dimensions.\n        in_channels: Number of input channels.\n        n_channels: Number of channels in the first block.\n        out_channels: Number of output channels.\n        down_block_types: Types of down blocks as a list, starting at the\n          first block (in the highest level).\n        mid_block_type: Type of mid block.\n        up_block_types: Types of up blocks as a list, starting with the last\n          block (lowest level).\n        block_out_channel_mults: Output channel multipliers for each block.\n        num_blocks_per_level: Number of blocks per level (blocks are repeated if `&gt;1`).\n        upsample_type: Type of upsampling block (see `chuchichaestli.models.upsampling` for details).\n        downsample_type: Type of downsampling block (see `chuchichaestli.models.downsampling` for details).\n        act: Activation function for the output layer (see\n          `chuchichaestli.models.activations` for details).\n        groups: Number of groups for group normalization in the output layer.\n        in_kernel_size: Kernel size for the input convolution.\n        out_kernel_size: Kernel size for the output convolution.\n        time_embedding: Whether to use a time embedding.\n        time_channels: Number of time channels.\n        t_emb_dim: The dimension for the deep embedding (takes only\n          effect if `time_embedding='DeepSinusoidalTimeEmbedding'`).\n        t_emb_flip: Whether to flip the sine to cosine in the time embedding.\n        t_emb_shift: The downscale frequency shift for the time embedding.\n        t_emb_act_fn: Activation function for the time embedding.\n        t_emb_post_act: Whether to use an activation function\n          at the end of the time embedding.\n        t_emb_condition_dim: The condition dimension for the time embedding.\n        res_act_fn: Activation function for the residual blocks\n          (see `chuchichaestli.models.activations` for details).\n        res_dropout: Dropout rate for the residual blocks.\n        res_norm_type: Normalization type for the residual block\n          (see `chuchichaestli.models.norm` for details).\n        res_groups: Number of groups for the residual block normalization (if group norm).\n        res_kernel_size: Kernel size for the residual blocks.\n        attn_head_dim: Dimension of the attention heads.\n        attn_n_heads: Number of attention heads.\n        attn_dropout_p: Dropout probability of the scaled dot product attention.\n        attn_norm_type: Normalization type for the convolutional attention block\n          (see `chuchichaestli.models.norm` for details).\n        attn_groups: Number of groups for the convolutional attention block normalization\n          (if `attn_norm_type` is `\"group\"`).\n        attn_kernel_size: Kernel size for the convolutional attention block.\n        attn_gate_inter_channels: Number of intermediate channels for the attention gate\n          (if `up_block_types` contains `\"AttnGateUpBlock\"`).\n        skip_connection_action: Action to take for the skip connection.\n          If `None`, no skip connections are used.\n        skip_connection_to_all_blocks: If `True`, the U-Net builds skip connections\n          to all blocks in a level, otherwise only to the first block in a level.\n        add_noise: Add a Gaussian noise regularizer block in the bottleneck (before or after).\n          Can be \"up\" (after the bottleneck) or \"down\" (before the bottleneck).\n        noise_sigma: Std. relative (to the magnitude of the input) for the noise generation.\n        noise_detached: If True, the input is detached for the noise generation.\n          Note, this should generally be `True`, otherwise the noise is learnable.\n    \"\"\"\n    super().__init__()\n\n    self._validate_inputs(\n        dimensions, down_block_types, up_block_types, block_out_channel_mults\n    )\n\n    # Cache commonly used values\n    conv_cls = DIM_TO_CONV_MAP[dimensions]\n    upsample_cls = UPSAMPLE_FUNCTIONS[upsample_type]\n    downsample_cls = DOWNSAMPLE_FUNCTIONS[downsample_type]\n    n_mults = len(block_out_channel_mults)\n    self.num_blocks_per_level = num_blocks_per_level\n    self.skip_connection_to_all_blocks = skip_connection_to_all_blocks\n\n    # Group normalization configuration\n    if res_norm_type == \"group\" and n_channels % res_groups != 0:\n        warnings.warn(\n            f\"Number of channels ({n_channels}) is not divisible by the number of groups ({res_groups}). Setting number of groups to n_channels.\"\n        )\n        res_groups = n_channels\n        groups = min(groups, n_channels)\n\n    # Pre-compute argument dictionaries to avoid repeated dict creation\n    res_args = {\n        \"res_groups\": res_groups,\n        \"res_act_fn\": res_act_fn,\n        \"res_dropout\": res_dropout,\n        \"res_norm_type\": res_norm_type,\n        \"res_kernel_size\": res_kernel_size,\n    }\n\n    attn_args = {\n        \"n_heads\": attn_n_heads,\n        \"head_dim\": attn_head_dim,\n        \"dropout_p\": attn_dropout_p,\n        \"norm_type\": attn_norm_type,\n        \"groups\": attn_groups,\n        \"kernel_size\": attn_kernel_size,\n        \"inter_channels\": attn_gate_inter_channels,\n    }\n\n    # Input layer\n    self.conv_in = conv_cls(\n        in_channels, n_channels, kernel_size=in_kernel_size, padding=\"same\"\n    )\n\n    self.time_channels = time_channels\n    self.time_emb = (\n        TIME_EMBEDDING_MAP[time_embedding](\n            num_channels=time_channels,\n            embedding_dim=t_emb_dim,\n            flip_sin_to_cos=t_emb_flip,\n            downscale_freq_shift=t_emb_shift,\n            activation=t_emb_act_fn,\n            post_activation=t_emb_post_act,\n            condition_dim=t_emb_condition_dim,\n        )\n        if time_embedding\n        else None\n    )\n\n    # Build encoder\n    self.down_blocks = nn.ModuleList([])\n    ins = n_channels\n    for i in range(n_mults):\n        outs = ins * block_out_channel_mults[i]\n\n        for _ in range(num_blocks_per_level):\n            down_block = BLOCK_MAP[down_block_types[i]](\n                dimensions=dimensions,\n                in_channels=ins,\n                out_channels=outs,\n                time_embedding=self.time_emb is not None,\n                time_channels=time_channels,\n                res_args=res_args,\n                attn_args=attn_args,\n            )\n            self.down_blocks.append(down_block)\n            ins = outs\n\n        if i &lt; n_mults - 1:\n            self.down_blocks.append(downsample_cls(dimensions, ins))\n\n    # Build middle block\n    self.mid_block = BLOCK_MAP[mid_block_type](\n        dimensions=dimensions,\n        channels=outs,\n        time_embedding=self.time_emb is not None,\n        time_channels=time_channels,\n        res_args=res_args,\n        attn_args=attn_args,\n    )\n\n    # Build decoder\n    self.up_blocks = nn.ModuleList([])\n\n    for i in reversed(range(n_mults)):\n        ins = outs\n        outs = ins // block_out_channel_mults[i]\n\n        for j in range(num_blocks_per_level):\n            up_block = BLOCK_MAP[up_block_types[i]](\n                dimensions=dimensions,\n                in_channels=ins if j == 0 else outs,\n                out_channels=outs,\n                time_embedding=self.time_emb is not None,\n                time_channels=time_channels,\n                res_args=res_args,\n                attn_args=attn_args,\n                skip_connection_action=(\n                    skip_connection_action\n                    if j == 0 or skip_connection_to_all_blocks\n                    else None\n                ),\n            )\n            self.up_blocks.append(up_block)\n\n        ins = outs\n        if i &gt; 0:\n            self.up_blocks.append(upsample_cls(dimensions, outs))\n\n    match add_noise:\n        case \"up\":\n            self.up_blocks.insert(\n                0, GaussianNoiseBlock(sigma=noise_sigma, detached=noise_detached)\n            )\n        case \"down\":\n            self.down_blocks.append(\n                GaussianNoiseBlock(sigma=noise_sigma, detached=noise_detached)\n            )\n\n    # Output layer\n    self.norm = Norm(dimensions, res_norm_type, outs, groups)\n    self.act = ACTIVATION_FUNCTIONS[act]()\n    self.conv_out = conv_cls(\n        outs, out_channels, kernel_size=out_kernel_size, padding=\"same\"\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/unet/unet/#chuchichaestli.models.unet.unet.UNet.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor,\n    t: int | float | torch.Tensor | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass - optimized but maintains exact original logic.</p> Source code in <code>src/chuchichaestli/models/unet/unet.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, t: int | float | torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass - optimized but maintains exact original logic.\"\"\"\n    t_emb = None\n    if t is not None:\n        if not torch.is_tensor(t):\n            t = torch.tensor(t, dtype=torch.long, device=x.device)\n        t = t.expand(x.shape[0])\n        t_emb = self.time_emb(t) if self.time_emb is not None else None\n\n    x = self.conv_in(x)\n\n    hh = []\n    for i, down_block in enumerate(self.down_blocks):\n        x = down_block(x, t_emb)\n        if isinstance(\n            down_block, Downsample | DownsampleInterpolate | GaussianNoiseBlock\n        ):\n            continue\n        # Append skip connection for the last down_block in each layer\n        if (i + 1) % self.num_blocks_per_level == 0:\n            hh.append(x)\n\n    x = self.mid_block(x, t_emb)\n\n    no_count_block = 0\n    for i, up_block in enumerate(self.up_blocks):\n        if isinstance(\n            up_block, Upsample | UpsampleInterpolate | GaussianNoiseBlock\n        ):\n            x = up_block(x, t_emb)\n            no_count_block += 1\n            continue\n        # concat skip connection for the first upblock of each layer\n        if (i - no_count_block) % self.num_blocks_per_level == 0:\n            hs = hh.pop()\n            x = up_block(x, hs, t_emb)\n        elif self.skip_connection_to_all_blocks:\n            hs = hh[-1]\n            x = up_block(x, hs, t_emb)\n        else:\n            x = up_block(x=x, h=None, t=t_emb)\n    x = self.conv_out(self.act(self.norm(x)))\n    return x\n</code></pre>"},{"location":"reference/chuchichaestli/models/upsampling/","title":"chuchichaestli.models.upsampling","text":""},{"location":"reference/chuchichaestli/models/upsampling/#chuchichaestli.models.upsampling","title":"chuchichaestli.models.upsampling","text":"<p>Upsampling modules for 1, 2, and 3D inputs.</p> <p>Classes:</p> Name Description <code>Upsample</code> <p>Upsampling layer for 1D, 2D, and 3D inputs.</p> <code>UpsampleInterpolate</code> <p>Upsampling layer for 1D, 2D, and 3D inputs implemented with interpolation.</p>"},{"location":"reference/chuchichaestli/models/upsampling/#chuchichaestli.models.upsampling.Upsample","title":"Upsample","text":"<pre><code>Upsample(dimensions: int, num_channels: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Upsampling layer for 1D, 2D, and 3D inputs.</p> <p>Initialize the upsampling layer.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the upsampling layer.</p> Source code in <code>src/chuchichaestli/models/upsampling.py</code> <pre><code>def __init__(self, dimensions: int, num_channels: int):\n    \"\"\"Initialize the upsampling layer.\"\"\"\n    super().__init__()\n    conv_cls = DIM_TO_CONVT_MAP[dimensions]\n    self.conv = conv_cls(\n        num_channels, num_channels, kernel_size=4, stride=2, padding=1\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/models/upsampling/#chuchichaestli.models.upsampling.Upsample.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the upsampling layer.</p> Source code in <code>src/chuchichaestli/models/upsampling.py</code> <pre><code>def forward(self, x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the upsampling layer.\"\"\"\n    return self.conv(x)\n</code></pre>"},{"location":"reference/chuchichaestli/models/upsampling/#chuchichaestli.models.upsampling.UpsampleInterpolate","title":"UpsampleInterpolate","text":"<pre><code>UpsampleInterpolate(\n    dimensions: int,\n    num_channels: int | None = None,\n    factor: int | None = None,\n    antialias: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Upsampling layer for 1D, 2D, and 3D inputs implemented with interpolation.</p> <p>Initialize the upsampling layer.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the upsampling layer.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>Literal['linear', 'bilinear', 'trilinear', 'nearest']</code> <p>Interpolation mode.</p> Source code in <code>src/chuchichaestli/models/upsampling.py</code> <pre><code>def __init__(\n    self,\n    dimensions: int,\n    num_channels: int | None = None,\n    factor: int | None = None,\n    antialias: bool = False,\n):\n    \"\"\"Initialize the upsampling layer.\"\"\"\n    self.dimensions = dimensions\n    self.num_channels = num_channels\n    self.factor = factor if factor is not None else 2\n    self.align_corners = False\n    self.antialias = antialias\n</code></pre>"},{"location":"reference/chuchichaestli/models/upsampling/#chuchichaestli.models.upsampling.UpsampleInterpolate.mode","title":"mode  <code>property</code>","text":"<pre><code>mode: Literal['linear', 'bilinear', 'trilinear', 'nearest']\n</code></pre> <p>Interpolation mode.</p>"},{"location":"reference/chuchichaestli/models/upsampling/#chuchichaestli.models.upsampling.UpsampleInterpolate.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Forward pass through the upsampling layer.</p> Source code in <code>src/chuchichaestli/models/upsampling.py</code> <pre><code>def forward(self, x: torch.Tensor, _t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the upsampling layer.\"\"\"\n    spatial_dims = x.shape[2:]\n    output_dims = [s * self.factor for s in spatial_dims]\n    return F.interpolate(\n        x,\n        size=output_dims,\n        mode=self.mode,\n        align_corners=self.align_corners,\n        antialias=self.antialias,\n    )\n</code></pre>"},{"location":"reference/chuchichaestli/utils/","title":"chuchichaestli.utils","text":""},{"location":"reference/chuchichaestli/utils/#chuchichaestli.utils","title":"chuchichaestli.utils","text":"<p>Various utility functions for chuchichaestli.</p> <p>Functions:</p> Name Description <code>partialclass</code> <p>Partial for init class constructors.</p>"},{"location":"reference/chuchichaestli/utils/#chuchichaestli.utils.partialclass","title":"partialclass","text":"<pre><code>partialclass(name: str, cls: type[object], *args, **kwargs)\n</code></pre> <p>Partial for init class constructors.</p> Source code in <code>src/chuchichaestli/utils.py</code> <pre><code>def partialclass(name: str, cls: type[object], *args, **kwargs):\n    \"\"\"Partial for __init__ class constructors.\"\"\"\n    part_cls = type(\n        name, (cls,), {\"__init__\": partialmethod(cls.__init__, *args, **kwargs)}\n    )\n    try:\n        part_cls.__module__ = sys._getframe(1).f_globals.get(\"__name__\", \"__main__\")\n    except (AttributeError, ValueError):\n        pass\n    return part_cls\n</code></pre>"}]}